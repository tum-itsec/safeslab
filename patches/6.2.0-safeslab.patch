diff --git a/Makefile b/Makefile
index 3f6628780eb2..bcb265b9ab09 100644
--- a/Makefile
+++ b/Makefile
@@ -1,4 +1,4 @@
-# SPDX-License-Identifier: GPL-2.0
+# SPDX-License-Identifier: GPL-3.0
 VERSION = 6
 PATCHLEVEL = 2
 SUBLEVEL = 0
diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 6c2826417b33..f00c05007c4d 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -34,6 +34,7 @@
 #include <asm/io_bitmap.h>
 #include <asm/syscall.h>
 #include <asm/irq_stack.h>
+#include <asm/delay.h>
 
 #ifdef CONFIG_X86_64
 
@@ -70,8 +71,22 @@ static __always_inline bool do_syscall_x32(struct pt_regs *regs, int nr)
 	return false;
 }
 
+#ifdef CONFIG_DEBUG_KMEMLEAK
+extern bool world_is_stopped;
+
+static inline void stw_block(void)
+{
+	while (world_is_stopped)
+		schedule();
+}
+#else
+static inline void stw_block(void) {}
+#endif
+
 __visible noinstr void do_syscall_64(struct pt_regs *regs, int nr)
 {
+	stw_block();
+	
 	add_random_kstack_offset();
 	nr = syscall_enter_from_user_mode(regs, nr);
 
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 15739a2c0983..306a820fbd45 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -628,6 +628,12 @@ SYM_INNER_LABEL(swapgs_restore_regs_and_return_to_usermode, SYM_L_GLOBAL)
 	ALTERNATIVE "", "jmp xenpv_restore_regs_and_return_to_usermode", X86_FEATURE_XENPV
 #endif
 
+#ifdef CONFIG_SAFESLAB_PK
+	xor 	%ecx, %ecx
+	rdpkru
+	movq	%rax, PER_CPU_VAR(safeslab_kernel_pkru)
+#endif
+
 	POP_REGS pop_rdi=0
 
 	/*
@@ -1065,6 +1071,14 @@ SYM_CODE_START(error_entry)
 	IBRS_ENTER
 	UNTRAIN_RET_FROM_CALL
 
+#ifdef CONFIG_SAFESLAB_PK
+	rdpkru
+	cmpq PER_CPU_VAR(safeslab_kernel_pkru), %rax
+	jz .Lsafeslab_pkru_ok
+	ud2
+.Lsafeslab_pkru_ok:
+#endif
+
 	leaq	8(%rsp), %rdi			/* arg0 = pt_regs pointer */
 	/* Put us onto the real thread stack. */
 	jmp	sync_regs
diff --git a/arch/x86/include/asm/idtentry.h b/arch/x86/include/asm/idtentry.h
index 72184b0b2219..897f27ddccb7 100644
--- a/arch/x86/include/asm/idtentry.h
+++ b/arch/x86/include/asm/idtentry.h
@@ -656,6 +656,7 @@ DECLARE_IDTENTRY_SYSVEC(IRQ_MOVE_CLEANUP_VECTOR,	sysvec_irq_move_cleanup);
 DECLARE_IDTENTRY_SYSVEC(REBOOT_VECTOR,			sysvec_reboot);
 DECLARE_IDTENTRY_SYSVEC(CALL_FUNCTION_SINGLE_VECTOR,	sysvec_call_function_single);
 DECLARE_IDTENTRY_SYSVEC(CALL_FUNCTION_VECTOR,		sysvec_call_function);
+DECLARE_IDTENTRY_SYSVEC(SAFESLAB_CHANGE_PKRU_VECTOR, 	sysvec_safeslab_change_pkru);
 #endif
 
 #ifdef CONFIG_X86_LOCAL_APIC
diff --git a/arch/x86/include/asm/irq_vectors.h b/arch/x86/include/asm/irq_vectors.h
index 43dcb9284208..19ef7e00e1ba 100644
--- a/arch/x86/include/asm/irq_vectors.h
+++ b/arch/x86/include/asm/irq_vectors.h
@@ -84,6 +84,13 @@
  */
 #define IRQ_WORK_VECTOR			0xf6
 
+/*
+ * SAFESLAB: we have got our own interrupt, I guess... 
+ * the CALL FUNCTION STUFF DOES NOT SEEM TO LIKE BEING USED IN INTERRUPTS... 
+ * see comments in smp_call_function() et al.
+ */
+#define SAFESLAB_CHANGE_PKRU_VECTOR 	0xf5
+
 /* 0xf5 - unused, was UV_BAU_MESSAGE */
 #define DEFERRED_ERROR_VECTOR		0xf4
 
diff --git a/arch/x86/include/asm/page.h b/arch/x86/include/asm/page.h
index 9cc82f305f4b..305df2f55600 100644
--- a/arch/x86/include/asm/page.h
+++ b/arch/x86/include/asm/page.h
@@ -59,6 +59,8 @@ static inline void copy_user_page(void *to, void *from, unsigned long vaddr,
 #define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
 #endif
 
+#define __va_in_dm(x, dm_idx)	((void *)((unsigned long)(x)+PAGE_OFFSET+(dm_idx*DM_SIZE)))
+
 #define __boot_va(x)		__va(x)
 #define __boot_pa(x)		__pa(x)
 
diff --git a/arch/x86/include/asm/page_64.h b/arch/x86/include/asm/page_64.h
index 198e03e59ca1..243589685f38 100644
--- a/arch/x86/include/asm/page_64.h
+++ b/arch/x86/include/asm/page_64.h
@@ -18,12 +18,20 @@ extern unsigned long page_offset_base;
 extern unsigned long vmalloc_base;
 extern unsigned long vmemmap_base;
 
+// fine to declare this here for now, as this file will be included in page.h
+#define DM_SIZE 0x4000000000 /* 256 GB */
+
 static __always_inline unsigned long __phys_addr_nodebug(unsigned long x)
 {
 	unsigned long y = x - __START_KERNEL_map;
 
 	/* use the carry flag to determine if x was < __START_KERNEL_map */
-	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
+	//x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
+	
+	// for the false cond, first subtract the DM base, which will yield
+	// a DM_SIZE (eg. 256GB) multiple, then remove the DM_SIZE multiple 
+	// to just get the offset in RAM; let's hope this works!
+	x = (x > y) ? (y + phys_base) : ((x - PAGE_OFFSET) & (DM_SIZE - 1));
 
 	return x;
 }
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 0564edd24ffb..f2cc2482a8bd 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -289,6 +289,12 @@ static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
 	return native_make_pte(v & ~clear);
 }
 
+//SAFESLAB: this moves the PTE into domain with index dm_idx 
+static inline pte_t pte_set_pkey(pte_t pte, int dm_idx)
+{
+	return pte_set_flags(pte_clear_flags(pte,(pteval_t)_PAGE_PKEY_MASK), (pteval_t)((pteval_t)dm_idx<<_PAGE_BIT_PKEY_BIT0));
+}
+
 #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
 static inline int pte_uffd_wp(pte_t pte)
 {
@@ -401,6 +407,12 @@ static inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)
 	return native_make_pmd(v & ~clear);
 }
 
+//SAFESLAB: this moves the PMDE into domain with index dm_idx 
+static inline pmd_t pmd_set_pkey(pmd_t pmd, int dm_idx)
+{
+	return pmd_set_flags(pmd_clear_flags(pmd,(pmdval_t)_PAGE_PKEY_MASK), (pmdval_t)((pmdval_t)dm_idx<<_PAGE_BIT_PKEY_BIT0));
+}
+
 #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
 static inline int pmd_uffd_wp(pmd_t pmd)
 {
@@ -988,7 +1000,8 @@ extern void memblock_find_dma_reserve(void);
 void __init poking_init(void);
 unsigned long init_memory_mapping(unsigned long start,
 				  unsigned long end, pgprot_t prot);
-
+unsigned long safeslab_clone_memory_mapping(unsigned long start,
+				  unsigned long end, pgprot_t prot);
 #ifdef CONFIG_X86_64
 extern pgd_t trampoline_pgd_entry;
 #endif
diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
index 447d4bee25c4..3be3c7814af4 100644
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@ -187,6 +187,8 @@ enum page_cache_mode {
 #define PAGE_READONLY_EXEC   __pg(__PP|   0|_USR|___A|   0|   0|   0|   0)
 
 #define __PAGE_KERNEL		 (__PP|__RW|   0|___A|__NX|___D|   0|___G)
+//SAFESLAB: added this set of flags, in order for the page to use userspace protection keys
+#define __PAGE_KERNEL_PKEY      (__PP|__RW|_USR|___A|__NX|___D|   0|___G)
 #define __PAGE_KERNEL_EXEC	 (__PP|__RW|   0|___A|   0|___D|   0|___G)
 #define _KERNPG_TABLE_NOENC	 (__PP|__RW|   0|___A|   0|___D|   0|   0)
 #define _KERNPG_TABLE		 (__PP|__RW|   0|___A|   0|___D|   0|   0| _ENC)
@@ -215,6 +217,8 @@ enum page_cache_mode {
 #define __pgprot_mask(x)	__pgprot((x) & __default_kernel_pte_mask)
 
 #define PAGE_KERNEL		__pgprot_mask(__PAGE_KERNEL            | _ENC)
+//SAFESLAB: define the macro for the stuff that got defined above
+#define PAGE_KERNEL_PKEY       __pgprot_mask(__PAGE_KERNEL_PKEY       | _ENC)
 #define PAGE_KERNEL_NOENC	__pgprot_mask(__PAGE_KERNEL            |    0)
 #define PAGE_KERNEL_RO		__pgprot_mask(__PAGE_KERNEL_RO         | _ENC)
 #define PAGE_KERNEL_EXEC	__pgprot_mask(__PAGE_KERNEL_EXEC       | _ENC)
diff --git a/arch/x86/include/asm/pkru.h b/arch/x86/include/asm/pkru.h
index 74f0a2d34ffd..407f699047c4 100644
--- a/arch/x86/include/asm/pkru.h
+++ b/arch/x86/include/asm/pkru.h
@@ -50,13 +50,4 @@ static inline void write_pkru(u32 pkru)
 	if (pkru != rdpkru())
 		wrpkru(pkru);
 }
-
-static inline void pkru_write_default(void)
-{
-	if (!cpu_feature_enabled(X86_FEATURE_OSPKE))
-		return;
-
-	wrpkru(pkru_get_init_value());
-}
-
 #endif
diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h
index b4dbb20dab1a..27cdd88b2af9 100644
--- a/arch/x86/include/asm/smp.h
+++ b/arch/x86/include/asm/smp.h
@@ -136,6 +136,8 @@ void native_send_call_func_ipi(const struct cpumask *mask);
 void native_send_call_func_single_ipi(int cpu);
 void x86_idle_thread_init(unsigned int cpu, struct task_struct *idle);
 
+void safeslab_send_update_pkru(void);
+
 void smp_store_boot_cpu_info(void);
 void smp_store_cpu_info(int id);
 
diff --git a/arch/x86/kernel/apic/ipi.c b/arch/x86/kernel/apic/ipi.c
index 2a6509e8c840..afe098910111 100644
--- a/arch/x86/kernel/apic/ipi.c
+++ b/arch/x86/kernel/apic/ipi.c
@@ -71,6 +71,12 @@ void native_smp_send_reschedule(int cpu)
 	apic->send_IPI(cpu, RESCHEDULE_VECTOR);
 }
 
+// SAFESLAB: this is used to update the pkru register on all remote cpus
+void safeslab_send_update_pkru(void)
+{
+	apic->send_IPI_allbutself(SAFESLAB_CHANGE_PKRU_VECTOR);
+}
+
 void native_send_call_func_single_ipi(int cpu)
 {
 	apic->send_IPI(cpu, CALL_FUNCTION_SINGLE_VECTOR);
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f3cc7699e1e1..e390a2a7a9b6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -65,6 +65,8 @@
 
 #include "cpu.h"
 
+#include <linux/safebuddy.h>
+
 u32 elf_hwcap2 __read_mostly;
 
 /* all of these masks are initialized in setup_cpu_local_masks() */
@@ -379,7 +381,14 @@ static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 
 static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 {
-	unsigned long eflags = native_save_fl();
+	unsigned long eflags;
+
+#ifdef CONFIG_SAFESLAB_PK	
+	// SAFESLAB: we disable SMAP to be able to use US pages in kernel mode
+	return;
+#endif
+	
+	eflags = native_save_fl();
 
 	/* This should have been cleared long ago */
 	BUG_ON(eflags & X86_EFLAGS_AC);
@@ -544,8 +553,11 @@ static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 	}
 
 	cr4_set_bits(X86_CR4_PKE);
+
+#ifdef CONFIG_SAFESLAB_PK
 	/* Load the default PKRU value */
 	pkru_write_default();
+#endif
 }
 
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
@@ -1970,6 +1982,7 @@ void __init identify_boot_cpu(void)
 void identify_secondary_cpu(struct cpuinfo_x86 *c)
 {
 	BUG_ON(c == &boot_cpu_data);
+	safebuddy_init_secondary();
 	identify_cpu(c);
 #ifdef CONFIG_X86_32
 	enable_sep_cpu();
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 9baa89a8877d..322bdf4a1a8e 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -668,7 +668,7 @@ static inline void restore_fpregs_from_init_fpstate(u64 features_mask)
 	else
 		frstor(&init_fpstate.regs.fsave);
 
-	pkru_write_default();
+	//pkru_write_default();
 }
 
 /*
diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
index a58c6bc1cd68..6893c1514b12 100644
--- a/arch/x86/kernel/idt.c
+++ b/arch/x86/kernel/idt.c
@@ -133,6 +133,7 @@ static const __initconst struct idt_data apic_idts[] = {
 	INTG(CALL_FUNCTION_SINGLE_VECTOR,	asm_sysvec_call_function_single),
 	INTG(IRQ_MOVE_CLEANUP_VECTOR,		asm_sysvec_irq_move_cleanup),
 	INTG(REBOOT_VECTOR,			asm_sysvec_reboot),
+	INTG(SAFESLAB_CHANGE_PKRU_VECTOR, 	asm_sysvec_safeslab_change_pkru),
 #endif
 
 #ifdef CONFIG_X86_THERMAL_VECTOR
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 40d156a31676..54f9669f34f6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -228,7 +228,7 @@ static void pkru_flush_thread(void)
 	 * If PKRU is enabled the default PKRU value has to be loaded into
 	 * the hardware right here (similar to context switch).
 	 */
-	pkru_write_default();
+	//pkru_write_default();
 }
 
 void flush_thread(void)
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4e34b3b68ebd..e5fef9fd40ca 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -349,6 +349,8 @@ static __always_inline void load_seg_legacy(unsigned short prev_index,
 static __always_inline void x86_pkru_load(struct thread_struct *prev,
 					  struct thread_struct *next)
 {
+	return;
+
 	if (!cpu_feature_enabled(X86_FEATURE_OSPKE))
 		return;
 
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index 88188549647c..567afd2a2af8 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -1168,6 +1168,7 @@ void __init setup_arch(char **cmdline_p)
 	 */
 	x86_platform.realmode_reserve();
 
+	// setup direct mapping for kernel
 	init_mem_mapping();
 
 	idt_setup_early_pf();
@@ -1224,6 +1225,7 @@ void __init setup_arch(char **cmdline_p)
 
 	early_acpi_boot_init();
 
+	// essentially initializes numa control structures
 	initmem_init();
 	dma_contiguous_reserve(max_pfn_mapped << PAGE_SHIFT);
 
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 06db901fabe8..0c93115b4793 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -34,6 +34,8 @@
 #include <asm/kexec.h>
 #include <asm/virtext.h>
 
+#include <linux/safebuddy.h>
+
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -240,6 +242,13 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
 }
 
+DEFINE_IDTENTRY_SYSVEC(sysvec_safeslab_change_pkru)
+{
+  ack_APIC_irq();
+  
+  safebuddy_switch_domain();
+}
+
 DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)
 {
 	ack_APIC_irq();
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d317dc3d06a3..dd359ce56ff9 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -75,6 +75,8 @@
 
 #include <asm/proto.h>
 
+#include <linux/safebuddy.h>
+
 DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
 static inline void cond_local_irq_enable(struct pt_regs *regs)
@@ -415,6 +417,8 @@ __visible void __noreturn handle_stack_overflow(struct pt_regs *regs,
 }
 #endif
 
+extern void do_kernel_pkey_fault(unsigned long address);
+
 /*
  * Runs on an IST stack for x86_64 and on a special task stack for x86_32.
  *
@@ -443,6 +447,11 @@ DEFINE_IDTENTRY_DF(exc_double_fault)
 	unsigned long address = read_cr2();
 	struct stack_info info;
 #endif
+	
+	//wrpkru(0);
+	//do_kernel_pkey_fault(address);
+	//pkru_write_default();
+	//return;
 
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 7b0d4ab894c8..02710501d7cb 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -34,9 +34,16 @@
 #include <asm/vdso.h>			/* fixup_vdso_exception()	*/
 #include <asm/irq_stack.h>
 
+#include <trace/events/safeslab_trace.h>
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
+#include <asm/pgalloc.h>
+#include <../../../mm/safeslab.h>
+#include <../../../mm/slab.h>
+
+#include <linux/safebuddy.h>
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1169,6 +1176,146 @@ bool fault_in_kernel_space(unsigned long address)
 	return address >= TASK_SIZE_MAX;
 }
 
+#ifdef CONFIG_SAFESLAB_PK
+static void fault_populate_pte(pte_t *ptep, unsigned long address, unsigned int pkey)
+{
+	struct page *page =  virt_to_page(address);
+	unsigned long offset, pfn = page_to_pfn(page);
+	for(offset=0; offset<PTRS_PER_PTE;offset++, pfn++) {
+		set_pte(ptep,pfn_pte(pfn,__pgprot(__PAGE_KERNEL_PKEY)));
+		set_pte(ptep, pte_set_pkey(*ptep,pkey));
+		ptep++;
+	}
+}
+
+// The definiton of KAI_FAULTDEBUG results in printing debug output
+// in fault handler for protection key related faults
+#define safeslab_fault_printk(...)
+//#define safeslab_fault_printk(fmt, ...) printk("[SAFESLAB] %s " #fmt, __FUNCTION__, ##__VA_ARGS__)
+
+static void fault_split_huge_page(pmd_t *pmdp)
+{
+	pte_t *new_ptep;
+	unsigned long address;
+	unsigned int  pkey;
+
+	if (!(new_ptep = pte_alloc_one_kernel(&init_mm))) {
+		panic("[SAFESLAB] failed to split: oom\n");
+		return;
+	}
+
+	safeslab_fault_printk("Splitting hugepage!\n");
+
+#ifdef CONFIG_SAFESLAB_TRACE
+	trace_split_huge_page(_RET_IP_);
+#endif
+
+	address = pmd_page_vaddr(*pmdp);
+	pkey = pte_flags_pkey(pmd_flags(*pmdp));
+	fault_populate_pte(new_ptep, address, pkey);
+	
+	smp_wmb();
+	set_pmd(pmdp,__pmd(__pa(new_ptep) | _KERNPG_TABLE | _PAGE_NX));
+}
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+extern unsigned int set_sid;
+extern unsigned int check_sid;
+#endif
+
+/*
+ * KAI: this function is called when a Protection Key Error happens while in Kernel Mode
+ * This should only happen, when something triggered an access of a page which resides
+ * in an inactive domain that contains a long living object
+ * OR
+ * if a page formerly contained a long living object and thus the protection keys of that
+ * domain where modified to another domain
+ */
+void do_kernel_pkey_fault(unsigned long address)
+{
+	pgd_t* pgdp;
+	p4d_t* p4dp;
+	pud_t* pudp;
+	pmd_t* pmdp;
+	pte_t* ptep, pte;
+	unsigned long flags;
+	struct safeslab_slab *slab;
+	unsigned long active_domain, addr_dom;
+
+#ifdef CONFIG_SAFESLAB_TRACE
+	trace_pkey_fault(_RET_IP_);
+#endif
+
+	safeslab_fault_printk("PF on address %llx virt_to_slab %llx\n", address, virt_to_slab(address));
+
+	slab = (struct safeslab_slab *) virt_to_slab((void *)address);
+	if (!PageSafeslab((struct page *)slab))
+		panic("Faulting addr %lx not on valid safeslab\n!", address);
+
+	// it's enough to test if the faulting address points to a legitmate slab
+	// doesn't matter if the object is allocated or not
+	addr_dom = (address - page_offset_base) / DM_SIZE;
+	if (addr_dom != slab->dm_idx)
+		panic("Faulting addr %lx not in same domain as slab\n", address);
+
+	// assume entries are present (otherwise how would it work?)
+	pgdp = pgd_offset_k(address);
+	
+	safeslab_fault_printk("pgd");
+	
+	p4dp = p4d_offset(pgdp,address);
+
+	safeslab_fault_printk("p4d");
+	
+	pudp = pud_offset(p4dp,address);
+	
+	safeslab_fault_printk("pud");
+	
+	pmdp = pmd_offset(pudp,address);
+
+	safeslab_fault_printk("pmd");
+	
+	/* We have got everything we need at this point
+	 * - The pte/pmdp
+	 * - The address
+	 * - The current domain
+	 */
+	active_domain = this_cpu_read(safeslab_active_domain);
+	
+	spin_lock_irqsave(&init_mm.page_table_lock, flags);
+	
+	if(pmd_large(*pmdp)) {
+		//Huge pages (the 2M ones only) are being handled here
+		fault_split_huge_page(pmdp);
+		//We should have a regular pmd now - Oh please just work as expected
+		ptep = pte_offset_kernel(pmdp, address);
+		pte =  *ptep;
+		set_pte(ptep, pte_set_pkey(pte, active_domain));
+	} else {
+		//Regular 4K Pages (the good ol' pages) are being handled here
+		ptep = pte_offset_kernel(pmdp,address);
+		pte =  *ptep;
+		set_pte(ptep, pte_set_pkey(pte, active_domain));
+	}
+
+	//This should flush the current cpu only (-> or prolly sometimes all the cpus if supported, see the impl...)
+	__flush_tlb_all();
+	spin_unlock_irqrestore(&init_mm.page_table_lock, flags);
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+	// we're moving the slab into a new domain, so mark it as unsafe for that domain
+	slab = (struct safeslab_slab *)pfn_to_page(page_to_pfn((struct page *)slab) & (~((1 << SAFEBUDDY_ORDER_EXTEND) - 1)));
+	
+	//if (slab->sid < check_sid)
+	//	slab->unsafe_dms = 0;
+	slab->pg_md->unsafe_sid = set_sid;
+#endif
+
+	return;
+}
+EXPORT_SYMBOL(do_kernel_pkey_fault);
+#endif
+
 /*
  * Called for all faults where 'address' is part of the kernel address
  * space.  Might get called for faults that originate from *code* that
@@ -1182,8 +1329,17 @@ do_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,
 	 * Protection keys exceptions only happen on user pages.  We
 	 * have no user pages in the kernel portion of the address
 	 * space, so do not expect them here.
+	 * 
+	 * KAI: I FEEL GUILTY - NOW THEY DO
 	 */
+#ifdef CONFIG_SAFESLAB_PK
+	if (hw_error_code & X86_PF_PK) {
+		do_kernel_pkey_fault(address);
+		return;
+	}
+#else
 	WARN_ON_ONCE(hw_error_code & X86_PF_PK);
+#endif
 
 #ifdef CONFIG_X86_32
 	/*
@@ -1514,8 +1670,16 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 
 	/* Was the fault on kernel-controlled part of the address space? */
 	if (unlikely(fault_in_kernel_space(address))) {
+#ifdef CONFIG_SAFESLAB_PK
+		wrpkru(0);
+#endif
 		do_kern_addr_fault(regs, error_code, address);
+#ifdef CONFIG_SAFESLAB_PK
+		pkru_write_default();
+#endif
 	} else {
+		prefetchw(&current->mm->mmap_lock);
+
 		do_user_addr_fault(regs, error_code, address);
 		/*
 		 * User address page fault handling might have reenabled
@@ -1533,7 +1697,7 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	unsigned long address = read_cr2();
 	irqentry_state_t state;
 
-	prefetchw(&current->mm->mmap_lock);
+	//prefetchw(&current->mm->mmap_lock);
 
 	/*
 	 * KVM uses #PF vector to deliver 'page not present' events to guests
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index cb258f58fdc8..3e910bf0fa27 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -5,7 +5,6 @@
 #include <linux/memblock.h>
 #include <linux/swapfile.h>
 #include <linux/swapops.h>
-#include <linux/kmemleak.h>
 #include <linux/sched/task.h>
 
 #include <asm/set_memory.h>
@@ -540,6 +539,28 @@ unsigned long __ref init_memory_mapping(unsigned long start,
 	return ret >> PAGE_SHIFT;
 }
 
+unsigned long __ref safeslab_clone_memory_mapping(unsigned long start,
+					unsigned long end, pgprot_t prot)
+{
+	struct map_range mr[NR_RANGE_MR];
+	unsigned long ret = 0;
+	int nr_range, i;
+
+	pr_debug("clone_memory_mapping: [mem %#010lx-%#010lx]\n",
+	       start, end - 1);
+
+	memset(mr, 0, sizeof(mr));
+	nr_range = split_mem_range(mr, 0, start, end);
+	
+	//SAFESLAB TODO: can prot be extended with the appropriate protection key here?
+	for (i = 0; i < nr_range; i++)
+		ret = safeslab_clone_kernel_physical_mapping(mr[i].start, mr[i].end,
+						   mr[i].page_size_mask,
+						   prot);
+
+	return ret >> PAGE_SHIFT;
+}
+
 /*
  * We need to iterate through the E820 memory map and create direct mappings
  * for only E820_TYPE_RAM and E820_KERN_RESERVED regions. We cannot simply
@@ -901,7 +922,6 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 		 * Inform kmemleak about the hole in the memory since the
 		 * corresponding pages will be unmapped.
 		 */
-		kmemleak_free_part((void *)begin, end - begin);
 		set_memory_np(begin, (end - begin) >> PAGE_SHIFT);
 	} else {
 		/*
@@ -913,7 +933,15 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 		set_memory_rw(begin, (end - begin) >> PAGE_SHIFT);
 
 		free_reserved_area((void *)begin, (void *)end,
-				   POISON_FREE_INITMEM, what);
+			POISON_FREE_INITMEM, what);
+
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_PK
+		safeslab_clone_memory_mapping(begin, end, PAGE_KERNEL_PKEY);
+#else
+		safeslab_clone_memory_mapping(begin, end, PAGE_KERNEL);
+#endif
+#endif
 	}
 }
 
diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c
index a190aae8ceaf..8a8ac0023b8d 100644
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@ -452,7 +452,7 @@ void __init cleanup_highmap(void)
  */
 static unsigned long __meminit
 phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
-	      pgprot_t prot, bool init)
+	      pgprot_t prot, bool init, int dm_idx)
 {
 	unsigned long pages = 0, paddr_next;
 	unsigned long paddr_last = paddr_end;
@@ -465,12 +465,18 @@ phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
 	for (; i < PTRS_PER_PTE; i++, paddr = paddr_next, pte++) {
 		paddr_next = (paddr & PAGE_MASK) + PAGE_SIZE;
 		if (paddr >= paddr_end) {
-			if (!after_bootmem &&
-			    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,
-					     E820_TYPE_RAM) &&
-			    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,
-					     E820_TYPE_RESERVED_KERN))
-				set_pte_init(pte, __pte(0), init);
+			// these lines initialize entires outside the range we
+			// currently want to map; doing so they might clear entries
+			// that we already mapped, so let's just skip them for now;
+			// should cause no harm coz pt pages are already cleared when
+			// allocated
+			
+			//if (!after_bootmem &&
+			//    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,
+			//		     E820_TYPE_RAM) &&
+			//    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,
+			//		     E820_TYPE_RESERVED_KERN))
+			//	set_pte_init(pte, __pte(0), init);
 			continue;
 		}
 
@@ -481,6 +487,9 @@ phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
 		 * these mappings are more intelligent.
 		 */
 		if (!pte_none(*pte)) {
+#ifdef CONFIG_SAFESLAB_PK
+			set_pte(pte, pte_set_pkey(*pte, dm_idx));
+#endif
 			if (!after_bootmem)
 				pages++;
 			continue;
@@ -491,6 +500,10 @@ phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
 				pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL).pte);
 		pages++;
 		set_pte_init(pte, pfn_pte(paddr >> PAGE_SHIFT, prot), init);
+		// SAFESLAB TODO: this one can prolly be merged with the one above
+#ifdef CONFIG_SAFESLAB_PK
+		set_pte(pte, pte_set_pkey(*pte, dm_idx));
+#endif
 		paddr_last = (paddr & PAGE_MASK) + PAGE_SIZE;
 	}
 
@@ -506,12 +519,15 @@ phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
  */
 static unsigned long __meminit
 phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
-	      unsigned long page_size_mask, pgprot_t prot, bool init)
+	      unsigned long page_size_mask, pgprot_t prot, bool init, int dm_idx)
 {
 	unsigned long pages = 0, paddr_next;
 	unsigned long paddr_last = paddr_end;
 
 	int i = pmd_index(paddr);
+	
+	//pr_info("phys_pmd_init: [pmem %#010lx-%#010lx)\n",
+	//       paddr, paddr_end);
 
 	for (; i < PTRS_PER_PMD; i++, paddr = paddr_next) {
 		pmd_t *pmd = pmd_page + pmd_index(paddr);
@@ -520,12 +536,18 @@ phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
 
 		paddr_next = (paddr & PMD_MASK) + PMD_SIZE;
 		if (paddr >= paddr_end) {
-			if (!after_bootmem &&
-			    !e820__mapped_any(paddr & PMD_MASK, paddr_next,
-					     E820_TYPE_RAM) &&
-			    !e820__mapped_any(paddr & PMD_MASK, paddr_next,
-					     E820_TYPE_RESERVED_KERN))
-				set_pmd_init(pmd, __pmd(0), init);
+			// these lines initialize entires outside the range we
+			// currently want to map; doing so they might clear entries
+			// that we already mapped, so let's just skip them for now;
+			// should cause no harm coz pt pages are already cleared when
+			// allocated
+			
+			//if (!after_bootmem &&
+			//    !e820__mapped_any(paddr & PMD_MASK, paddr_next,
+			//		     E820_TYPE_RAM) &&
+			//    !e820__mapped_any(paddr & PMD_MASK, paddr_next,
+			//		     E820_TYPE_RESERVED_KERN))
+			//	set_pmd_init(pmd, __pmd(0), init);
 			continue;
 		}
 
@@ -535,7 +557,7 @@ phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
 				pte = (pte_t *)pmd_page_vaddr(*pmd);
 				paddr_last = phys_pte_init(pte, paddr,
 							   paddr_end, prot,
-							   init);
+							   init, dm_idx);
 				spin_unlock(&init_mm.page_table_lock);
 				continue;
 			}
@@ -552,6 +574,9 @@ phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
 			 * attributes.
 			 */
 			if (page_size_mask & (1 << PG_LEVEL_2M)) {
+#ifdef CONFIG_SAFESLAB_PK
+				set_pmd(pmd, pmd_set_pkey(*pmd, dm_idx));
+#endif
 				if (!after_bootmem)
 					pages++;
 				paddr_last = paddr_next;
@@ -566,13 +591,16 @@ phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
 			set_pmd_init(pmd,
 				     pfn_pmd(paddr >> PAGE_SHIFT, prot_sethuge(prot)),
 				     init);
+#ifdef CONFIG_SAFESLAB_PK
+			set_pmd(pmd, pmd_set_pkey(*pmd, dm_idx));
+#endif
 			spin_unlock(&init_mm.page_table_lock);
 			paddr_last = paddr_next;
 			continue;
 		}
 
 		pte = alloc_low_page();
-		paddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot, init);
+		paddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot, init, dm_idx);
 
 		spin_lock(&init_mm.page_table_lock);
 		pmd_populate_kernel_init(&init_mm, pmd, pte, init);
@@ -590,29 +618,42 @@ phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
  */
 static unsigned long __meminit
 phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,
-	      unsigned long page_size_mask, pgprot_t _prot, bool init)
+	      unsigned long page_size_mask, pgprot_t _prot, bool init, uint8_t dm_idx)
 {
 	unsigned long pages = 0, paddr_next;
 	unsigned long paddr_last = paddr_end;
-	unsigned long vaddr = (unsigned long)__va(paddr);
+	unsigned long vaddr = (unsigned long)__va_in_dm(paddr, dm_idx);
 	int i = pud_index(vaddr);
 
+	//pr_info("phys_pud_init: [pmem %#010lx-%#010lx) --> [vmem %#010llx)\n",
+	//       paddr, paddr_end, vaddr);
+
 	for (; i < PTRS_PER_PUD; i++, paddr = paddr_next) {
 		pud_t *pud;
 		pmd_t *pmd;
 		pgprot_t prot = _prot;
 
-		vaddr = (unsigned long)__va(paddr);
+		vaddr = (unsigned long)__va_in_dm(paddr, dm_idx);
 		pud = pud_page + pud_index(vaddr);
 		paddr_next = (paddr & PUD_MASK) + PUD_SIZE;
-
+		
 		if (paddr >= paddr_end) {
-			if (!after_bootmem &&
-			    !e820__mapped_any(paddr & PUD_MASK, paddr_next,
-					     E820_TYPE_RAM) &&
-			    !e820__mapped_any(paddr & PUD_MASK, paddr_next,
-					     E820_TYPE_RESERVED_KERN))
-				set_pud_init(pud, __pud(0), init);
+			// these lines initialize entires outside the range we
+			// currently want to map; doing so they might clear entries
+			// that we already mapped, so let's just skip them for now;
+			// should cause no harm coz pt pages are already cleared when
+			// allocated
+			
+			//if (!after_bootmem &&
+			//    !e820__mapped_any(paddr & PUD_MASK, paddr_next,
+			//		     E820_TYPE_RAM) &&
+			//    !e820__mapped_any(paddr & PUD_MASK, paddr_next,
+			//		     E820_TYPE_RESERVED_KERN)) {
+			//	if (pud == 0xffff888003406820)
+			//		printk("found it paddr %llx paddr_end %llx vaddr %llx dm_idx %d", paddr, paddr_end, vaddr, dm_idx);
+			//	set_pud_init(pud, __pud(0), init);
+			//
+			//}
 			continue;
 		}
 
@@ -622,7 +663,7 @@ phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,
 				paddr_last = phys_pmd_init(pmd, paddr,
 							   paddr_end,
 							   page_size_mask,
-							   prot, init);
+							   prot, init, dm_idx);
 				continue;
 			}
 			/*
@@ -658,8 +699,9 @@ phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,
 		}
 
 		pmd = alloc_low_page();
+
 		paddr_last = phys_pmd_init(pmd, paddr, paddr_end,
-					   page_size_mask, prot, init);
+					   page_size_mask, prot, init, dm_idx);
 
 		spin_lock(&init_mm.page_table_lock);
 		pud_populate_init(&init_mm, pud, pmd, init);
@@ -673,18 +715,20 @@ phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,
 
 static unsigned long __meminit
 phys_p4d_init(p4d_t *p4d_page, unsigned long paddr, unsigned long paddr_end,
-	      unsigned long page_size_mask, pgprot_t prot, bool init)
+	      unsigned long page_size_mask, pgprot_t prot, bool init, uint8_t dm_idx)
 {
 	unsigned long vaddr, vaddr_end, vaddr_next, paddr_next, paddr_last;
 
 	paddr_last = paddr_end;
-	vaddr = (unsigned long)__va(paddr);
-	vaddr_end = (unsigned long)__va(paddr_end);
+	vaddr = (unsigned long)__va_in_dm(paddr, dm_idx);
+	vaddr_end = (unsigned long)__va_in_dm(paddr_end, dm_idx);
 
 	if (!pgtable_l5_enabled())
 		return phys_pud_init((pud_t *) p4d_page, paddr, paddr_end,
-				     page_size_mask, prot, init);
+				     page_size_mask, prot, init, dm_idx);
 
+	BUG_ON(true);
+	// TODO Marius: we just ignore from here on, as we don't support 5-level paging yet
 	for (; vaddr < vaddr_end; vaddr = vaddr_next) {
 		p4d_t *p4d = p4d_page + p4d_index(vaddr);
 		pud_t *pud;
@@ -706,13 +750,13 @@ phys_p4d_init(p4d_t *p4d_page, unsigned long paddr, unsigned long paddr_end,
 		if (!p4d_none(*p4d)) {
 			pud = pud_offset(p4d, 0);
 			paddr_last = phys_pud_init(pud, paddr, __pa(vaddr_end),
-					page_size_mask, prot, init);
+					page_size_mask, prot, init, dm_idx);
 			continue;
 		}
 
 		pud = alloc_low_page();
 		paddr_last = phys_pud_init(pud, paddr, __pa(vaddr_end),
-					   page_size_mask, prot, init);
+					   page_size_mask, prot, init, dm_idx);
 
 		spin_lock(&init_mm.page_table_lock);
 		p4d_populate_init(&init_mm, p4d, pud, init);
@@ -726,15 +770,18 @@ static unsigned long __meminit
 __kernel_physical_mapping_init(unsigned long paddr_start,
 			       unsigned long paddr_end,
 			       unsigned long page_size_mask,
-			       pgprot_t prot, bool init)
+			       pgprot_t prot, bool init, uint8_t dm_idx)
 {
 	bool pgd_changed = false;
 	unsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;
 
 	paddr_last = paddr_end;
-	vaddr = (unsigned long)__va(paddr_start);
-	vaddr_end = (unsigned long)__va(paddr_end);
+	vaddr = (unsigned long)__va_in_dm(paddr_start, dm_idx);
+	vaddr_end = (unsigned long)__va_in_dm(paddr_end, dm_idx);
 	vaddr_start = vaddr;
+	
+	//pr_info("[%d] __kernel_physical_mapping_init: [pmem %#010lx-%#010lx) --> [vmem %lx-%lx)\n",
+	  //     dm_idx, paddr_start, paddr_end, vaddr_start, vaddr_end);
 
 	for (; vaddr < vaddr_end; vaddr = vaddr_next) {
 		pgd_t *pgd = pgd_offset_k(vaddr);
@@ -747,17 +794,18 @@ __kernel_physical_mapping_init(unsigned long paddr_start,
 			paddr_last = phys_p4d_init(p4d, __pa(vaddr),
 						   __pa(vaddr_end),
 						   page_size_mask,
-						   prot, init);
+						   prot, init, dm_idx);
 			continue;
 		}
 
 		p4d = alloc_low_page();
 		paddr_last = phys_p4d_init(p4d, __pa(vaddr), __pa(vaddr_end),
-					   page_size_mask, prot, init);
+					   page_size_mask, prot, init, dm_idx);
 
 		spin_lock(&init_mm.page_table_lock);
 		if (pgtable_l5_enabled())
-			pgd_populate_init(&init_mm, pgd, p4d, init);
+			// TODO Marius: we just ignore this since we don't support 5-level paging yet
+			pgd_populate_init(&init_mm, pgd, p4d, init); 
 		else
 			p4d_populate_init(&init_mm, p4d_offset(pgd, vaddr),
 					  (pud_t *) p4d, init);
@@ -766,8 +814,10 @@ __kernel_physical_mapping_init(unsigned long paddr_start,
 		pgd_changed = true;
 	}
 
-	if (pgd_changed)
+	if (pgd_changed) {
+		//printk("need to sync pgds \n");
 		sync_global_pgds(vaddr_start, vaddr_end - 1);
+	}
 
 	return paddr_last;
 }
@@ -784,8 +834,46 @@ kernel_physical_mapping_init(unsigned long paddr_start,
 			     unsigned long paddr_end,
 			     unsigned long page_size_mask, pgprot_t prot)
 {
+	// the true boolean arg is just to avoid tlb flushes, as they're
+	// not needed at this early stage
 	return __kernel_physical_mapping_init(paddr_start, paddr_end,
-					      page_size_mask, prot, true);
+					      page_size_mask, prot, true, 0);
+}
+
+/*
+ * Create page table mapping for the physical memory for specific physical
+ * addresses. Note that it can only be used to populate non-present entries.
+ * The virtual and physical addresses have to be aligned on PMD level
+ * down. It returns the last physical address mapped.
+ */
+#define NR_DMS 16
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+unsigned long long amount_pages_cloned = 0;
+EXPORT_SYMBOL(amount_pages_cloned);
+#endif
+
+unsigned long __meminit
+safeslab_clone_kernel_physical_mapping(unsigned long paddr_start,
+			     unsigned long paddr_end,
+			     unsigned long page_size_mask, pgprot_t prot)
+{
+	unsigned long ret;
+
+	printk("%s cloning paddr [%lx --> %lx)", __FUNCTION__, paddr_start, paddr_end);
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+	amount_pages_cloned += (paddr_end >> 12) - (paddr_start >> 12);
+#endif
+	
+	for (int i = 1; i < NR_DMS; i++) {
+		// the true boolean arg is just to avoid tlb flushes, as they're
+		// not needed at this early stage
+		ret =__kernel_physical_mapping_init(paddr_start, paddr_end,
+				page_size_mask, prot, true, i);
+	}
+
+	return ret;				      
 }
 
 /*
@@ -801,7 +889,7 @@ kernel_physical_mapping_change(unsigned long paddr_start,
 {
 	return __kernel_physical_mapping_init(paddr_start, paddr_end,
 					      page_size_mask, PAGE_KERNEL,
-					      false);
+					      false, 0);
 }
 
 #ifndef CONFIG_NUMA
diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c
index 557f0fe25dff..73275ff1eb7c 100644
--- a/arch/x86/mm/kaslr.c
+++ b/arch/x86/mm/kaslr.c
@@ -85,6 +85,11 @@ void __init kernel_randomize_memory(void)
 	if (!kaslr_memory_enabled())
 		return;
 
+	/* Marius: interestingly, when kaslr is enabled, the kernel VA regions
+	 * sa. the DM, vmalloc area, and vmemmap don't start at the addresses
+	 * defined in the virtual memory map.
+	 */
+
 	kaslr_regions[0].size_tb = 1 << (MAX_PHYSMEM_BITS - TB_SHIFT);
 	kaslr_regions[1].size_tb = VMALLOC_SIZE_TB;
 
diff --git a/arch/x86/mm/mm_internal.h b/arch/x86/mm/mm_internal.h
index 3f37b5c80bb3..6e14921a79bc 100644
--- a/arch/x86/mm/mm_internal.h
+++ b/arch/x86/mm/mm_internal.h
@@ -14,6 +14,10 @@ unsigned long kernel_physical_mapping_init(unsigned long start,
 					     unsigned long end,
 					     unsigned long page_size_mask,
 					     pgprot_t prot);
+unsigned long safeslab_clone_kernel_physical_mapping(unsigned long start,
+					     unsigned long end,
+					     unsigned long page_size_mask,
+					     pgprot_t prot);
 unsigned long kernel_physical_mapping_change(unsigned long start,
 					     unsigned long end,
 					     unsigned long page_size_mask);
diff --git a/arch/x86/mm/physaddr.c b/arch/x86/mm/physaddr.c
index fc3f3d3e2ef2..d3dad07b5c77 100644
--- a/arch/x86/mm/physaddr.c
+++ b/arch/x86/mm/physaddr.c
@@ -56,6 +56,7 @@ bool __virt_addr_valid(unsigned long x)
 			return false;
 	} else {
 		x = y + (__START_KERNEL_map - PAGE_OFFSET);
+		x = x & (DM_SIZE - 1);
 
 		/* carry flag will be set if starting x was >= PAGE_OFFSET */
 		if ((x > y) || !phys_addr_valid(x))
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 9c8dc70020bc..840cac120340 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -11,7 +11,6 @@
 #include <linux/bio.h>
 #include <linux/blkdev.h>
 #include <linux/blk-integrity.h>
-#include <linux/kmemleak.h>
 #include <linux/mm.h>
 #include <linux/init.h>
 #include <linux/slab.h>
@@ -3254,11 +3253,6 @@ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 	while (!list_empty(&tags->page_list)) {
 		page = list_first_entry(&tags->page_list, struct page, lru);
 		list_del_init(&page->lru);
-		/*
-		 * Remove kmemleak object previously allocated in
-		 * blk_mq_alloc_rqs().
-		 */
-		kmemleak_free(page_address(page));
 		__free_pages(page, page->private);
 	}
 }
@@ -3401,11 +3395,6 @@ static int blk_mq_alloc_rqs(struct blk_mq_tag_set *set,
 		list_add_tail(&page->lru, &tags->page_list);
 
 		p = page_address(page);
-		/*
-		 * Allow kmemleak to scan these pages as they contain pointers
-		 * to additional allocations like via ops->init_request().
-		 */
-		kmemleak_alloc(p, order_to_size(this_order), 1, GFP_NOIO);
 		entries_per_page = order_to_size(this_order) / rq_size;
 		to_do = min(entries_per_page, depth - i);
 		left -= to_do * rq_size;
diff --git a/drivers/acpi/acpica/utobject.c b/drivers/acpi/acpica/utobject.c
index d3667bfff401..78bf119d4381 100644
--- a/drivers/acpi/acpica/utobject.c
+++ b/drivers/acpi/acpica/utobject.c
@@ -8,7 +8,6 @@
  *****************************************************************************/
 
 #include <acpi/acpi.h>
-#include <linux/kmemleak.h>
 #include "accommon.h"
 #include "acnamesp.h"
 
@@ -71,7 +70,6 @@ union acpi_operand_object *acpi_ut_create_internal_object_dbg(const char
 	if (!object) {
 		return_PTR(NULL);
 	}
-	kmemleak_not_leak(object);
 
 	switch (type) {
 	case ACPI_TYPE_REGION:
diff --git a/drivers/acpi/tables.c b/drivers/acpi/tables.c
index 5fbc32b802d0..f75435f04d47 100644
--- a/drivers/acpi/tables.c
+++ b/drivers/acpi/tables.c
@@ -21,7 +21,6 @@
 #include <linux/earlycpio.h>
 #include <linux/initrd.h>
 #include <linux/security.h>
-#include <linux/kmemleak.h>
 #include "internal.h"
 
 #ifdef CONFIG_ACPI_CUSTOM_DSDT
@@ -657,7 +656,6 @@ void __init acpi_table_upgrade(void)
 	 */
 	arch_reserve_mem_area(acpi_tables_addr, all_tables_size);
 
-	kmemleak_ignore_phys(acpi_tables_addr);
 
 	/*
 	 * early_ioremap only can remap 256k one time. If we map all
diff --git a/drivers/gpu/drm/drm_buddy.c b/drivers/gpu/drm/drm_buddy.c
index 3d1f50f481cf..794b9631416d 100644
--- a/drivers/gpu/drm/drm_buddy.c
+++ b/drivers/gpu/drm/drm_buddy.c
@@ -723,7 +723,6 @@ int drm_buddy_alloc_blocks(struct drm_buddy *mm,
 
 		mark_allocated(block);
 		mm->avail -= drm_buddy_block_size(mm, block);
-		kmemleak_update_trace(block);
 		list_add_tail(&block->link, &allocated);
 
 		pages -= BIT(order);
diff --git a/drivers/iommu/amd/init.c b/drivers/iommu/amd/init.c
index 467b194975b3..d4dd76c292e0 100644
--- a/drivers/iommu/amd/init.c
+++ b/drivers/iommu/amd/init.c
@@ -19,7 +19,6 @@
 #include <linux/irq.h>
 #include <linux/amd-iommu.h>
 #include <linux/export.h>
-#include <linux/kmemleak.h>
 #include <linux/cc_platform.h>
 #include <linux/iopoll.h>
 #include <asm/pci-direct.h>
@@ -687,8 +686,6 @@ static inline int __init alloc_irq_lookup_table(struct amd_iommu_pci_seg *pci_se
 	pci_seg->irq_lookup_table = (void *)__get_free_pages(
 					     GFP_KERNEL | __GFP_ZERO,
 					     get_order(pci_seg->rlookup_table_size));
-	kmemleak_alloc(pci_seg->irq_lookup_table,
-		       pci_seg->rlookup_table_size, 1, GFP_KERNEL);
 	if (pci_seg->irq_lookup_table == NULL)
 		return -ENOMEM;
 
@@ -697,7 +694,6 @@ static inline int __init alloc_irq_lookup_table(struct amd_iommu_pci_seg *pci_se
 
 static inline void free_irq_lookup_table(struct amd_iommu_pci_seg *pci_seg)
 {
-	kmemleak_free(pci_seg->irq_lookup_table);
 	free_pages((unsigned long)pci_seg->irq_lookup_table,
 		   get_order(pci_seg->rlookup_table_size));
 	pci_seg->irq_lookup_table = NULL;
diff --git a/drivers/staging/rtl8712/xmit_linux.c b/drivers/staging/rtl8712/xmit_linux.c
index 132afbf49dde..4dc43a4cd697 100644
--- a/drivers/staging/rtl8712/xmit_linux.c
+++ b/drivers/staging/rtl8712/xmit_linux.c
@@ -115,7 +115,6 @@ int r8712_xmit_resource_alloc(struct _adapter *padapter,
 			netdev_err(padapter->pnetdev, "pxmitbuf->pxmit_urb[i] == NULL\n");
 			return -ENOMEM;
 		}
-		kmemleak_not_leak(pxmitbuf->pxmit_urb[i]);
 	}
 	return 0;
 }
diff --git a/fs/efivarfs/inode.c b/fs/efivarfs/inode.c
index 617f3ad2485e..b79bda06a9a6 100644
--- a/fs/efivarfs/inode.c
+++ b/fs/efivarfs/inode.c
@@ -112,7 +112,6 @@ static int efivarfs_create(struct user_namespace *mnt_userns, struct inode *dir,
 	var->var.VariableName[i] = '\0';
 
 	inode->i_private = var;
-	kmemleak_ignore(var);
 
 	err = efivar_entry_add(var, &efivarfs_list);
 	if (err)
diff --git a/fs/file_table.c b/fs/file_table.c
index dd88701e54a9..c5e2b6ec8c16 100644
--- a/fs/file_table.c
+++ b/fs/file_table.c
@@ -27,7 +27,6 @@
 #include <linux/task_work.h>
 #include <linux/ima.h>
 #include <linux/swap.h>
-#include <linux/kmemleak.h>
 
 #include <linux/atomic.h>
 
@@ -123,7 +122,6 @@ static int __init init_fs_stat_sysctls(void)
 	if (IS_ENABLED(CONFIG_BINFMT_MISC)) {
 		struct ctl_table_header *hdr;
 		hdr = register_sysctl_mount_point("fs/binfmt_misc");
-		kmemleak_not_leak(hdr);
 	}
 	return 0;
 }
diff --git a/fs/nfs/dir.c b/fs/nfs/dir.c
index f7e4a88d5d92..5872476eebdf 100644
--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@ -286,8 +286,6 @@ static const char *nfs_readdir_copy_name(const char *name, unsigned int len)
 	 * Avoid a kmemleak false positive. The pointer to the name is stored
 	 * in a page cache page which kmemleak does not scan.
 	 */
-	if (ret != NULL)
-		kmemleak_not_leak(ret);
 	return ret;
 }
 
diff --git a/fs/proc/proc_sysctl.c b/fs/proc/proc_sysctl.c
index 48f2d60bd78a..770ed8992944 100644
--- a/fs/proc/proc_sysctl.c
+++ b/fs/proc/proc_sysctl.c
@@ -16,7 +16,6 @@
 #include <linux/module.h>
 #include <linux/bpf-cgroup.h>
 #include <linux/mount.h>
-#include <linux/kmemleak.h>
 #include "internal.h"
 
 #define list_for_each_table_entry(entry, table) \
@@ -1432,7 +1431,6 @@ void __init __register_sysctl_init(const char *path, struct ctl_table *table,
 		pr_err("failed when register_sysctl %s to %s\n", table_name, path);
 		return;
 	}
-	kmemleak_not_leak(hdr);
 }
 
 static char *append_path(const char *path, char *pos, const char *name)
@@ -1655,7 +1653,6 @@ int __register_sysctl_base(struct ctl_table *base_table)
 	struct ctl_table_header *hdr;
 
 	hdr = register_sysctl_table(base_table);
-	kmemleak_not_leak(hdr);
 	return 0;
 }
 
diff --git a/fs/seq_file.c b/fs/seq_file.c
index f5fdaf3b1572..69b48fb116d4 100644
--- a/fs/seq_file.c
+++ b/fs/seq_file.c
@@ -1146,5 +1146,5 @@ EXPORT_SYMBOL(seq_hlist_next_percpu);
 
 void __init seq_file_init(void)
 {
-	seq_file_cache = KMEM_CACHE(seq_file, SLAB_ACCOUNT|SLAB_PANIC);
+	seq_file_cache = KMEM_CACHE(seq_file, SLAB_ACCOUNT|SLAB_PANIC|SLAB_SAFE_CACHE);
 }
diff --git a/include/linux/exactfit.h b/include/linux/exactfit.h
new file mode 100644
index 000000000000..8a73ea04df54
--- /dev/null
+++ b/include/linux/exactfit.h
@@ -0,0 +1,14 @@
+#ifndef EXACTFIT_H
+#define EXACTFIT_H
+
+#ifdef CONFIG_SAFESLAB
+void __init exactfit_allocator_init(void);
+#else
+void __init exactfit_allocator_init(void) {}
+#endif
+
+inline void *exactfit_alloc(unsigned int size, gfp_t gfp_flags);
+
+inline void exactfit_free(void *address, unsigned int size);
+
+#endif
\ No newline at end of file
diff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h
index d88c46ca82e1..064cb9289956 100644
--- a/include/linux/gfp_types.h
+++ b/include/linux/gfp_types.h
@@ -60,6 +60,7 @@ typedef unsigned int __bitwise gfp_t;
 #else
 #define ___GFP_NOLOCKDEP	0
 #endif
+#define ___GFP_SAFESLAB_SKIP_ZERO 0x10000000u
 /* If the above are modified, __GFP_BITS_SHIFT may need updating */
 
 /*
@@ -248,6 +249,7 @@ typedef unsigned int __bitwise gfp_t;
 #define __GFP_COMP	((__force gfp_t)___GFP_COMP)
 #define __GFP_ZERO	((__force gfp_t)___GFP_ZERO)
 #define __GFP_ZEROTAGS	((__force gfp_t)___GFP_ZEROTAGS)
+#define __GFP_SAFESLAB_SKIP_ZERO ((__force gfp_t)___GFP_SAFESLAB_SKIP_ZERO)
 #define __GFP_SKIP_ZERO ((__force gfp_t)___GFP_SKIP_ZERO)
 #define __GFP_SKIP_KASAN_UNPOISON ((__force gfp_t)___GFP_SKIP_KASAN_UNPOISON)
 #define __GFP_SKIP_KASAN_POISON   ((__force gfp_t)___GFP_SKIP_KASAN_POISON)
@@ -256,7 +258,7 @@ typedef unsigned int __bitwise gfp_t;
 #define __GFP_NOLOCKDEP ((__force gfp_t)___GFP_NOLOCKDEP)
 
 /* Room for N __GFP_FOO bits */
-#define __GFP_BITS_SHIFT (27 + IS_ENABLED(CONFIG_LOCKDEP))
+#define __GFP_BITS_SHIFT (29)
 #define __GFP_BITS_MASK ((__force gfp_t)((1 << __GFP_BITS_SHIFT) - 1))
 
 /**
diff --git a/include/linux/kmemleak.h b/include/linux/kmemleak.h
index 6a3cd1bf4680..d66cb43ca331 100644
--- a/include/linux/kmemleak.h
+++ b/include/linux/kmemleak.h
@@ -12,110 +12,8 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
-#ifdef CONFIG_DEBUG_KMEMLEAK
+void kmemleak_init(void) __init;
 
-extern void kmemleak_init(void) __init;
-extern void kmemleak_alloc(const void *ptr, size_t size, int min_count,
-			   gfp_t gfp) __ref;
-extern void kmemleak_alloc_percpu(const void __percpu *ptr, size_t size,
-				  gfp_t gfp) __ref;
-extern void kmemleak_vmalloc(const struct vm_struct *area, size_t size,
-			     gfp_t gfp) __ref;
-extern void kmemleak_free(const void *ptr) __ref;
-extern void kmemleak_free_part(const void *ptr, size_t size) __ref;
-extern void kmemleak_free_percpu(const void __percpu *ptr) __ref;
-extern void kmemleak_update_trace(const void *ptr) __ref;
-extern void kmemleak_not_leak(const void *ptr) __ref;
-extern void kmemleak_ignore(const void *ptr) __ref;
-extern void kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp) __ref;
-extern void kmemleak_no_scan(const void *ptr) __ref;
-extern void kmemleak_alloc_phys(phys_addr_t phys, size_t size,
-				gfp_t gfp) __ref;
-extern void kmemleak_free_part_phys(phys_addr_t phys, size_t size) __ref;
-extern void kmemleak_ignore_phys(phys_addr_t phys) __ref;
-
-static inline void kmemleak_alloc_recursive(const void *ptr, size_t size,
-					    int min_count, slab_flags_t flags,
-					    gfp_t gfp)
-{
-	if (!(flags & SLAB_NOLEAKTRACE))
-		kmemleak_alloc(ptr, size, min_count, gfp);
-}
-
-static inline void kmemleak_free_recursive(const void *ptr, slab_flags_t flags)
-{
-	if (!(flags & SLAB_NOLEAKTRACE))
-		kmemleak_free(ptr);
-}
-
-static inline void kmemleak_erase(void **ptr)
-{
-	*ptr = NULL;
-}
-
-#else
-
-static inline void kmemleak_init(void)
-{
-}
-static inline void kmemleak_alloc(const void *ptr, size_t size, int min_count,
-				  gfp_t gfp)
-{
-}
-static inline void kmemleak_alloc_recursive(const void *ptr, size_t size,
-					    int min_count, slab_flags_t flags,
-					    gfp_t gfp)
-{
-}
-static inline void kmemleak_alloc_percpu(const void __percpu *ptr, size_t size,
-					 gfp_t gfp)
-{
-}
-static inline void kmemleak_vmalloc(const struct vm_struct *area, size_t size,
-				    gfp_t gfp)
-{
-}
-static inline void kmemleak_free(const void *ptr)
-{
-}
-static inline void kmemleak_free_part(const void *ptr, size_t size)
-{
-}
-static inline void kmemleak_free_recursive(const void *ptr, slab_flags_t flags)
-{
-}
-static inline void kmemleak_free_percpu(const void __percpu *ptr)
-{
-}
-static inline void kmemleak_update_trace(const void *ptr)
-{
-}
-static inline void kmemleak_not_leak(const void *ptr)
-{
-}
-static inline void kmemleak_ignore(const void *ptr)
-{
-}
-static inline void kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp)
-{
-}
-static inline void kmemleak_erase(void **ptr)
-{
-}
-static inline void kmemleak_no_scan(const void *ptr)
-{
-}
-static inline void kmemleak_alloc_phys(phys_addr_t phys, size_t size,
-				       gfp_t gfp)
-{
-}
-static inline void kmemleak_free_part_phys(phys_addr_t phys, size_t size)
-{
-}
-static inline void kmemleak_ignore_phys(phys_addr_t phys)
-{
-}
-
-#endif	/* CONFIG_DEBUG_KMEMLEAK */
+void kmemleak_unpark(void);
 
 #endif	/* __KMEMLEAK_H */
diff --git a/include/linux/ksmap.h b/include/linux/ksmap.h
new file mode 100644
index 000000000000..479cd5a576d0
--- /dev/null
+++ b/include/linux/ksmap.h
@@ -0,0 +1,54 @@
+#ifndef _LINUX_KSMAP_H
+#define _LINUX_KSMAP_H
+
+#define ksmap__ASM_CLAC	".byte 0x0f,0x01,0xca"
+#define ksmap__ASM_STAC	".byte 0x0f,0x01,0xcb"
+
+static __always_inline void ksmap_stac(void)
+{
+	__asm__("stac");
+}
+
+static __always_inline void ksmap_clac(void)
+{
+	__asm__("clac");
+}
+
+static __always_inline void ksmap_nesting_warning(void)
+{
+	//unsigned long eflags = native_save_fl();
+
+	///* This should be cleared */
+	//WARN_ON(eflags & X86_EFLAGS_AC);
+}
+
+/*
+ * The following two functions _have_ to be always inlined, otherwise objtool
+ * complains about UACCESS being disabled after function return.
+ * 
+ */
+static __always_inline void ksmap_disable_smap(void)
+{
+	//unsigned long eflags = native_save_fl();
+
+	///* This should be cleared */
+	//WARN_ON(eflags & X86_EFLAGS_AC);
+	
+	// no need to worry about spec execution here
+	// as ksmap is entirely used to access addresses
+	// that are not user-controlled
+	ksmap_stac();
+}
+
+static __always_inline void ksmap_enable_smap(void)
+{
+	ksmap_clac();
+}
+
+void __init ksmap_init(void);
+
+struct page *ksmap_get_isolated_pages(unsigned int order);
+
+void ksmap_free_isolated_pages(struct page *page, unsigned int order);
+
+#endif /* _LINUX_KSMAP_H */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index f13f20258ce9..d2ab4d92c2c0 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -114,6 +114,7 @@ extern int mmap_rnd_compat_bits __read_mostly;
 #ifndef page_to_virt
 #define page_to_virt(x)	__va(PFN_PHYS(page_to_pfn(x)))
 #endif
+#define page_to_virt_dm(x, dm_idx)	__va_in_dm(PFN_PHYS(page_to_pfn(x)), dm_idx)
 
 #ifndef lm_alias
 #define lm_alias(x)	__va(__pa_symbol(x))
@@ -1866,6 +1867,11 @@ static __always_inline void *lowmem_page_address(const struct page *page)
 	return page_to_virt(page);
 }
 
+static __always_inline void *lowmem_page_address_dm(const struct page *page, uint8_t dm_idx)
+{
+	return page_to_virt_dm(page, dm_idx);
+}
+
 #if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)
 #define HASHED_PAGE_VIRTUAL
 #endif
@@ -1890,6 +1896,7 @@ void page_address_init(void);
 
 #if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)
 #define page_address(page) lowmem_page_address(page)
+#define page_address_dm(page, dm_idx) lowmem_page_address_dm(page, dm_idx)
 #define set_page_address(page, address)  do { } while(0)
 #define page_address_init()  do { } while(0)
 #endif
@@ -1899,6 +1906,11 @@ static inline void *folio_address(const struct folio *folio)
 	return page_address(&folio->page);
 }
 
+static inline void *folio_address_dm(const struct folio *folio, uint8_t dm_idx)
+{
+	return page_address_dm(&folio->page, dm_idx);
+}
+
 extern void *page_rmapping(struct page *page);
 extern pgoff_t __page_file_index(struct page *page);
 
@@ -2665,10 +2677,10 @@ static inline void mark_page_reserved(struct page *page)
  */
 static inline unsigned long free_initmem_default(int poison)
 {
-	extern char __init_begin[], __init_end[];
-
-	return free_reserved_area(&__init_begin, &__init_end,
-				  poison, "unused kernel image (initmem)");
+	//extern char __init_begin[], __init_end[];
+return 0;
+	/*return free_reserved_area(&__init_begin, &__init_end,
+				  poison, "unused kernel image (initmem)");*/
 }
 
 static inline unsigned long get_num_physpages(void)
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 9757067c3053..a92b75ff40e1 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -218,6 +218,14 @@ struct page {
 	/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
 	atomic_t _refcount;
 
+	//struct { // 8 bytes
+	//	unsigned short unmapped; // 2 bytes
+	//	unsigned char unsafe; // 1 byte
+	//	unsigned char sid; // 1 byte
+	//	unsigned int tmd_orders; // 4 bytes
+	//} safeslab;
+	struct safeslab_pg_md *safeslab;
+
 #ifdef CONFIG_MEMCG
 	unsigned long memcg_data;
 #endif
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 69e93a0c1277..ab2db876ab27 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -139,6 +139,7 @@ enum pageflags {
 #ifdef CONFIG_KASAN_HW_TAGS
 	PG_skip_kasan_poison,
 #endif
+	PG_safeslab,
 	__NR_PAGEFLAGS,
 
 	PG_readahead = PG_reclaim,
@@ -371,6 +372,8 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)
 #define FOLIO_PF_NO_COMPOUND	0
 #define FOLIO_PF_SECOND		1
 
+//#define PG_safeslab		0x04000000
+
 /*
  * Macros to create function definitions for page flags
  */
@@ -483,6 +486,7 @@ PAGEFLAG(Active, active, PF_HEAD) __CLEARPAGEFLAG(Active, active, PF_HEAD)
 PAGEFLAG(Workingset, workingset, PF_HEAD)
 	TESTCLEARFLAG(Workingset, workingset, PF_HEAD)
 __PAGEFLAG(Slab, slab, PF_NO_TAIL)
+__PAGEFLAG(Safeslab, safeslab, PF_NO_TAIL)
 __PAGEFLAG(SlobFree, slob_free, PF_NO_TAIL)
 PAGEFLAG(Checked, checked, PF_NO_COMPOUND)	   /* Used by some filesystems */
 
@@ -917,10 +921,11 @@ static inline bool is_page_hwpoison(struct page *page)
 #define PAGE_TYPE_BASE	0xf0000000
 /* Reserve		0x0000007f to catch underflows of page_mapcount */
 #define PAGE_MAPCOUNT_RESERVE	-128
-#define PG_buddy	0x00000080
-#define PG_offline	0x00000100
-#define PG_table	0x00000200
-#define PG_guard	0x00000400
+#define PG_buddy		0x00000080
+#define PG_offline		0x00000100
+#define PG_table		0x00000200
+#define PG_guard		0x00000400
+#define PG_safebuddy		0x08000000
 
 #define PageType(page, flag)						\
 	((page->page_type & (PAGE_TYPE_BASE | flag)) == PAGE_TYPE_BASE)
@@ -952,6 +957,14 @@ static __always_inline void __ClearPage##uname(struct page *page)	\
  */
 PAGE_TYPE_OPS(Buddy, buddy)
 
+/*
+ * PageSafebuddy() indicates that the page is usable (not in quarantine)
+ * in the Safebuddy subsystem
+ */
+PAGE_TYPE_OPS(Safebuddy, safebuddy)
+
+//PAGE_TYPE_OPS(Safeslab, safeslab)
+
 /*
  * PageOffline() indicates that the page is logically offline although the
  * containing section is online. (e.g. inflated in a balloon driver or
diff --git a/include/linux/safebuddy.h b/include/linux/safebuddy.h
new file mode 100644
index 000000000000..a11cccb4a1b1
--- /dev/null
+++ b/include/linux/safebuddy.h
@@ -0,0 +1,47 @@
+#ifndef _LINUX_SAFEBUDDY_H
+#define _LINUX_SAFEBUDDY_H
+
+#include <asm/special_insns.h>
+#include "../../mm/safeslab.h"
+
+#define SAFEBUDDY_ORDER_EXTEND 2
+
+#ifdef CONFIG_SAFESLAB_PK
+#define pkey_to_ad_bit(pkey) (PKRU_AD_BIT << (pkey * PKRU_BITS_PER_PKEY))
+#define pkey_to_wd_bit(pkey) (PKRU_WD_BIT << (pkey * PKRU_BITS_PER_PKEY))
+
+DECLARE_PER_CPU(unsigned long, safeslab_kernel_pkru);
+
+static inline void pkru_write_domain(int dm_idx)
+{
+	uint32_t pkru_val = ~(3 | pkey_to_ad_bit(dm_idx) | pkey_to_wd_bit(dm_idx));
+	
+	this_cpu_write(safeslab_kernel_pkru, pkru_val);
+
+	wrpkru(pkru_val);
+}
+
+DECLARE_PER_CPU(unsigned long, safeslab_active_domain);
+
+static inline void pkru_write_default(void)
+{
+	unsigned long curr_dom = this_cpu_read(safeslab_active_domain);
+	uint32_t pkru_val = ~(3 | pkey_to_ad_bit(curr_dom) | pkey_to_wd_bit(curr_dom));
+	
+	this_cpu_write(safeslab_kernel_pkru, pkru_val);
+	
+	wrpkru(pkru_val);
+}
+#endif
+
+void __init safebuddy_init(void);
+
+void safebuddy_init_secondary(void);
+
+struct page *safebuddy_alloc_pages(gfp_t flags, unsigned int order, int node);
+
+void safebuddy_quar_pages(struct safeslab_slab *page, unsigned int order, bool was_safe);
+
+void safebuddy_switch_domain(void);
+
+#endif /* _LINUX_SAFEBUDDY_H */
diff --git a/include/linux/safeslab.h b/include/linux/safeslab.h
new file mode 100644
index 000000000000..6f6b22dd9f16
--- /dev/null
+++ b/include/linux/safeslab.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef SAFESLAB_H
+#define SAFESLAB_H
+
+/*
+Things we expect to be off:
+CONFIG_KFENCE
+CONFIG_MEMCG maybe
+CONFIG_KASAN_GENERIC
+
+Not supported configs:
+SLAB_SUPPORTS_SYSFS
+*/
+
+DECLARE_PER_CPU(unsigned long, safeslab_kernel_pkru);
+
+#ifdef CONFIG_SAFESLAB
+void __init safeslab_init(void);
+void __init kmem_cache_init_late_safeslab(void);
+#else
+void __init safeslab_init(void) {}
+void __init kmem_cache_init_late_safeslab(void) {}
+#endif
+
+void safeslab_test_active_domain(void);
+
+#endif /* SAFESLAB_H */
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 45af70315a94..4707fe8d0241 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -138,6 +138,13 @@
 #define SLAB_SKIP_KFENCE	0
 #endif
 
+// We use kfence flag. kfence is a lying piece of allocator and is never used anyway!
+#ifdef CONFIG_SAFESLAB
+#define SLAB_SAFE_CACHE		((slab_flags_t __force)0x20000000U)
+#else
+#define SLAB_SAFE_CACHE		0
+#endif
+
 /* The following flags affect the page allocator grouping pages by mobility */
 /* Objects are reclaimable */
 #ifndef CONFIG_SLUB_TINY
@@ -162,6 +169,8 @@
 
 #include <linux/kasan.h>
 
+
+void slub_test_active_domain(void);
 struct list_lru;
 struct mem_cgroup;
 /*
@@ -487,13 +496,20 @@ void kmem_cache_free(struct kmem_cache *s, void *objp);
 void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p);
 int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size, void **p);
 
+// TODO: fwd declaration, needed but exposes safeslab internal
+void kmem_cache_free_bulk_safeslab(struct kmem_cache *s, size_t size, void **p);
+
 /*
  * Caller must not use kfree_bulk() on memory not originally allocated
  * by kmalloc(), because the SLOB allocator cannot handle this.
  */
 static __always_inline void kfree_bulk(size_t size, void **p)
 {
+#ifdef CONFIG_SAFESLAB
+	kmem_cache_free_bulk_safeslab(NULL, size, p);
+#else
 	kmem_cache_free_bulk(NULL, size, p);
+#endif
 }
 
 void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment
diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index aa0ee1678d29..955ff693a910 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -47,7 +47,16 @@ enum stat_item {
  * with this_cpu_cmpxchg_double() alignment requirements.
  */
 struct kmem_cache_cpu {
-	void **freelist;	/* Pointer to next available object */
+	union {
+		void **freelist;	/* Pointer to next available object */
+		union {
+			unsigned long counters;
+			struct {
+				unsigned inuse:16;
+				unsigned objects: 15;
+			};
+		};
+	};
 	unsigned long tid;	/* Globally unique transaction id */
 	struct slab *slab;	/* The slab from which we are allocating */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h
index 412b5a46374c..127792fbf7b1 100644
--- a/include/trace/events/mmflags.h
+++ b/include/trace/events/mmflags.h
@@ -51,7 +51,8 @@
 	gfpflag_string(__GFP_RECLAIM),		\
 	gfpflag_string(__GFP_DIRECT_RECLAIM),	\
 	gfpflag_string(__GFP_KSWAPD_RECLAIM),	\
-	gfpflag_string(__GFP_ZEROTAGS)
+	gfpflag_string(__GFP_ZEROTAGS),		\
+	gfpflag_string(__GFP_SAFESLAB_SKIP_ZERO)
 
 #ifdef CONFIG_KASAN_HW_TAGS
 #define __def_gfpflag_names_kasan ,			\
@@ -124,7 +125,8 @@
 	{1UL << PG_mappedtodisk,	"mappedtodisk"	},		\
 	{1UL << PG_reclaim,		"reclaim"	},		\
 	{1UL << PG_swapbacked,		"swapbacked"	},		\
-	{1UL << PG_unevictable,		"unevictable"	}		\
+	{1UL << PG_unevictable,		"unevictable"	},		\
+	{1UL << PG_safeslab,		"safeslab"	}		\
 IF_HAVE_PG_MLOCK(PG_mlocked,		"mlocked"	)		\
 IF_HAVE_PG_UNCACHED(PG_uncached,	"uncached"	)		\
 IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
@@ -132,7 +134,7 @@ IF_HAVE_PG_IDLE(PG_young,		"young"		)		\
 IF_HAVE_PG_IDLE(PG_idle,		"idle"		)		\
 IF_HAVE_PG_ARCH_X(PG_arch_2,		"arch_2"	)		\
 IF_HAVE_PG_ARCH_X(PG_arch_3,		"arch_3"	)		\
-IF_HAVE_PG_SKIP_KASAN_POISON(PG_skip_kasan_poison, "skip_kasan_poison")
+IF_HAVE_PG_SKIP_KASAN_POISON(PG_skip_kasan_poison, "skip_kasan_poison") 
 
 #define show_page_flags(flags)						\
 	(flags) ? __print_flags(flags, "|",				\
diff --git a/include/trace/events/safeslab_trace.h b/include/trace/events/safeslab_trace.h
new file mode 100644
index 000000000000..5c94417358f7
--- /dev/null
+++ b/include/trace/events/safeslab_trace.h
@@ -0,0 +1,117 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM safeslab_trace
+
+#if !defined(_TRACE_SFSLAB) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_SFSLAB
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(pkey_fault,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(split_huge_page,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(switch_domain,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(kmemleak_scan,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(alloc_slab_page,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(free_slab,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(alloc_slab_page_pagecount,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(free_slab_pagecount,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+
+TRACE_EVENT(free_slab_pagecount_rcu,
+	TP_PROTO(unsigned long call_site),
+	TP_ARGS(call_site),
+	TP_STRUCT__entry(
+		__field(unsigned long, call_site)
+	),
+	TP_fast_assign(
+		__entry->call_site = call_site;
+	),
+	TP_printk("call_site=%pS", (void *)__entry->call_site)
+);
+#endif
+
+#include<trace/define_trace.h>
diff --git a/init/main.c b/init/main.c
index e1c3911d7c70..921af99c55f7 100644
--- a/init/main.c
+++ b/init/main.c
@@ -102,6 +102,9 @@
 #include <linux/stackdepot.h>
 #include <linux/randomize_kstack.h>
 #include <net/net_namespace.h>
+#include <linux/safebuddy.h>
+#include <linux/safeslab.h>
+#include <linux/exactfit.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -845,13 +848,18 @@ static void __init mm_init(void)
 	stack_depot_early_init();
 	mem_init();
 	mem_init_print_info();
+	exactfit_allocator_init();
 	kmem_cache_init();
+	safeslab_init();
+	
 	/*
 	 * page_owner must be initialized after buddy is ready, and also after
 	 * slab is ready so that stack_depot_init() works properly
 	 */
 	page_ext_init_flatmem_late();
+#ifdef CONFIG_DEBUG_KMEMLEAK	
 	kmemleak_init();
+#endif
 	pgtable_init();
 	debug_objects_mem_init();
 	vmalloc_init();
@@ -958,6 +966,10 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	 */
 	boot_cpu_init();
 	page_address_init();
+	// Gotta initialize safebuddy before SLUB
+#if defined(CONFIG_SAFESLAB) || defined(CONFIG_SLUB_USE_SAFEBUDDY) || defined(CONFIG_SLUB_USE_SAFESLAB_DOMAINS)
+	safebuddy_init();
+#endif
 	pr_notice("%s", linux_banner);
 	early_security_init();
 	setup_arch(&command_line);
@@ -1068,6 +1080,7 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	local_irq_enable();
 
 	kmem_cache_init_late();
+	kmem_cache_init_late_safeslab();
 
 	/*
 	 * HACK ALERT! This is early. We're enabling the console before
diff --git a/kernel/bpf/ringbuf.c b/kernel/bpf/ringbuf.c
index 80f4b4d88aaf..35a342b5c60a 100644
--- a/kernel/bpf/ringbuf.c
+++ b/kernel/bpf/ringbuf.c
@@ -140,7 +140,6 @@ static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)
 	rb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,
 		  VM_MAP | VM_USERMAP, PAGE_KERNEL);
 	if (rb) {
-		kmemleak_not_leak(pages);
 		rb->pages = pages;
 		rb->nr_pages = nr_pages;
 		return rb;
diff --git a/kernel/fork.c b/kernel/fork.c
index 9f7fe3541897..b05ea0e206cb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -921,7 +921,7 @@ void __init fork_init(void)
 	task_struct_whitelist(&useroffset, &usersize);
 	task_struct_cachep = kmem_cache_create_usercopy("task_struct",
 			arch_task_struct_size, align,
-			SLAB_PANIC|SLAB_ACCOUNT,
+			SLAB_PANIC|SLAB_ACCOUNT|SLAB_SAFE_CACHE,
 			useroffset, usersize, NULL);
 #endif
 
diff --git a/kernel/module/debug_kmemleak.c b/kernel/module/debug_kmemleak.c
index 12a569d361e8..67ce0318aa6d 100644
--- a/kernel/module/debug_kmemleak.c
+++ b/kernel/module/debug_kmemleak.c
@@ -12,19 +12,5 @@
 void kmemleak_load_module(const struct module *mod,
 			  const struct load_info *info)
 {
-	unsigned int i;
-
-	/* only scan the sections containing data */
-	kmemleak_scan_area(mod, sizeof(struct module), GFP_KERNEL);
-
-	for (i = 1; i < info->hdr->e_shnum; i++) {
-		/* Scan all writable sections that's not executable */
-		if (!(info->sechdrs[i].sh_flags & SHF_ALLOC) ||
-		    !(info->sechdrs[i].sh_flags & SHF_WRITE) ||
-		    (info->sechdrs[i].sh_flags & SHF_EXECINSTR))
-			continue;
-
-		kmemleak_scan_area((void *)info->sechdrs[i].sh_addr,
-				   info->sechdrs[i].sh_size, GFP_KERNEL);
-	}
+	return;
 }
diff --git a/kernel/module/main.c b/kernel/module/main.c
index 4ac3fe43e6c8..ccd2872b1686 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -2129,7 +2129,6 @@ static int move_module(struct module *mod, struct load_info *info)
 	 * which is inside the block. Just mark it as not being a
 	 * leak.
 	 */
-	kmemleak_not_leak(ptr);
 	if (!ptr)
 		return -ENOMEM;
 
@@ -2144,7 +2143,6 @@ static int move_module(struct module *mod, struct load_info *info)
 		 * scanned as it contains data and code that will be freed
 		 * after the module is initialized.
 		 */
-		kmemleak_ignore(ptr);
 		if (!ptr) {
 			module_memfree(mod->core_layout.base);
 			return -ENOMEM;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2a4918a1faa9..016a32ed3854 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -6616,6 +6616,7 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 	}
 }
 
+// Safeslab: will not instrument
 void __noreturn do_task_dead(void)
 {
 	/* Causes final put_task_struct in finish_task_switch(): */
@@ -6675,6 +6676,13 @@ static void sched_update_worker(struct task_struct *tsk)
 	}
 }
 
+#ifdef CONFIG_DEBUG_KMEMLEAK
+extern bool world_is_stopped;
+#else
+#define world_is_stopped 0
+#endif
+
+// Safeslab: called from kernel context ==> will instrument
 asmlinkage __visible void __sched schedule(void)
 {
 	struct task_struct *tsk = current;
@@ -6684,7 +6692,7 @@ asmlinkage __visible void __sched schedule(void)
 		preempt_disable();
 		__schedule(SM_NONE);
 		sched_preempt_enable_no_resched();
-	} while (need_resched());
+	} while (need_resched() || world_is_stopped);
 	sched_update_worker(tsk);
 }
 EXPORT_SYMBOL(schedule);
@@ -6699,6 +6707,7 @@ EXPORT_SYMBOL(schedule);
  * schedule_idle() is similar to schedule_preempt_disable() except that it
  * never enables preemption because it does not call sched_submit_work().
  */
+// Safeslab: will not instrument
 void __sched schedule_idle(void)
 {
 	/*
@@ -6746,6 +6755,7 @@ void __sched schedule_preempt_disabled(void)
 }
 
 #ifdef CONFIG_PREEMPT_RT
+// Safeslab: will not instrument 
 void __sched notrace schedule_rtlock(void)
 {
 	do {
@@ -6757,6 +6767,7 @@ void __sched notrace schedule_rtlock(void)
 NOKPROBE_SYMBOL(schedule_rtlock);
 #endif
 
+// Safeslab: called from kernel context ==> will instrument
 static void __sched notrace preempt_schedule_common(void)
 {
 	do {
@@ -6783,7 +6794,7 @@ static void __sched notrace preempt_schedule_common(void)
 		 * Check again in case we missed a preemption opportunity
 		 * between schedule and now.
 		 */
-	} while (need_resched());
+	} while (need_resched() || world_is_stopped);
 }
 
 #ifdef CONFIG_PREEMPTION
@@ -6839,6 +6850,8 @@ EXPORT_SYMBOL(dynamic_preempt_schedule);
  * instead of preempt_schedule() to exit user context if needed before
  * calling the scheduler.
  */
+// Safeslab: called from kernel context for tracing, or from user context
+// ==> will not instrument
 asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 {
 	enum ctx_state prev_ctx;
@@ -6906,6 +6919,7 @@ EXPORT_SYMBOL(dynamic_preempt_schedule_notrace);
  * Note, that this is called and return with irqs disabled. This will
  * protect us against recursive calling from irq.
  */
+// Safeslab: called from end of irq context ==> should we instrument?
 asmlinkage __visible void __sched preempt_schedule_irq(void)
 {
 	enum ctx_state prev_state;
@@ -6923,6 +6937,9 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 
+	// Safeslab
+	WARN_ON_ONCE(true);
+
 	exception_exit(prev_state);
 }
 
diff --git a/kernel/seccomp.c b/kernel/seccomp.c
index e9852d1b4a5e..9999314b89ed 100644
--- a/kernel/seccomp.c
+++ b/kernel/seccomp.c
@@ -19,7 +19,6 @@
 #include <linux/audit.h>
 #include <linux/compat.h>
 #include <linux/coredump.h>
-#include <linux/kmemleak.h>
 #include <linux/nospec.h>
 #include <linux/prctl.h>
 #include <linux/sched.h>
@@ -2397,8 +2396,6 @@ static int __init seccomp_sysctl_init(void)
 	hdr = register_sysctl_paths(seccomp_sysctl_path, seccomp_sysctl_table);
 	if (!hdr)
 		pr_warn("sysctl registration failed\n");
-	else
-		kmemleak_not_leak(hdr);
 
 	return 0;
 }
diff --git a/kernel/ucount.c b/kernel/ucount.c
index ee8e57fd6f90..9bee8f2577df 100644
--- a/kernel/ucount.c
+++ b/kernel/ucount.c
@@ -5,7 +5,6 @@
 #include <linux/slab.h>
 #include <linux/cred.h>
 #include <linux/hash.h>
-#include <linux/kmemleak.h>
 #include <linux/user_namespace.h>
 
 struct ucounts init_ucounts = {
@@ -365,7 +364,6 @@ static __init int user_namespace_sysctl_init(void)
 	 * properly.
 	 */
 	user_header = register_sysctl("user", empty);
-	kmemleak_ignore(user_header);
 	BUG_ON(!user_header);
 	BUG_ON(!setup_userns_sysctls(&init_user_ns));
 #endif
diff --git a/lib/generic-radix-tree.c b/lib/generic-radix-tree.c
index f25eb111c051..3572771cf7c2 100644
--- a/lib/generic-radix-tree.c
+++ b/lib/generic-radix-tree.c
@@ -2,7 +2,6 @@
 #include <linux/export.h>
 #include <linux/generic-radix-tree.h>
 #include <linux/gfp.h>
-#include <linux/kmemleak.h>
 
 #define GENRADIX_ARY		(PAGE_SIZE / sizeof(struct genradix_node *))
 #define GENRADIX_ARY_SHIFT	ilog2(GENRADIX_ARY)
@@ -82,18 +81,11 @@ static inline struct genradix_node *genradix_alloc_node(gfp_t gfp_mask)
 
 	node = (struct genradix_node *)__get_free_page(gfp_mask|__GFP_ZERO);
 
-	/*
-	 * We're using pages (not slab allocations) directly for kernel data
-	 * structures, so we need to explicitly inform kmemleak of them in order
-	 * to avoid false positive memory leak reports.
-	 */
-	kmemleak_alloc(node, PAGE_SIZE, 1, gfp_mask);
 	return node;
 }
 
 static inline void genradix_free_node(struct genradix_node *node)
 {
-	kmemleak_free(node);
 	free_page((unsigned long)node);
 }
 
diff --git a/lib/radix-tree.c b/lib/radix-tree.c
index 049ba132f7ef..4a89ecb05dcd 100644
--- a/lib/radix-tree.c
+++ b/lib/radix-tree.c
@@ -18,7 +18,6 @@
 #include <linux/idr.h>
 #include <linux/init.h>
 #include <linux/kernel.h>
-#include <linux/kmemleak.h>
 #include <linux/percpu.h>
 #include <linux/preempt.h>		/* in_interrupt() */
 #include <linux/radix-tree.h>
@@ -268,7 +267,6 @@ radix_tree_node_alloc(gfp_t gfp_mask, struct radix_tree_node *parent,
 		 * Update the allocation stack trace as this is more useful
 		 * for debugging.
 		 */
-		kmemleak_update_trace(ret);
 		goto out;
 	}
 	ret = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 8d7519a8f308..7d2e7936c4a6 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -8,7 +8,6 @@
 #include <linux/slab.h>
 #include <linux/scatterlist.h>
 #include <linux/highmem.h>
-#include <linux/kmemleak.h>
 
 /**
  * sg_next - return the next scatterlist entry in a list
@@ -159,7 +158,6 @@ static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
 		 * intermediate allocations.
 		 */
 		void *ptr = (void *) __get_free_page(gfp_mask);
-		kmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);
 		return ptr;
 	} else
 		return kmalloc_array(nents, sizeof(struct scatterlist),
@@ -169,7 +167,6 @@ static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
 static void sg_kfree(struct scatterlist *sg, unsigned int nents)
 {
 	if (nents == SG_MAX_SINGLE_ALLOC) {
-		kmemleak_free(sg);
 		free_page((unsigned long) sg);
 	} else
 		kfree(sg);
diff --git a/mm/Kconfig b/mm/Kconfig
index ff7b209dec05..87b200cac0b8 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -290,6 +290,71 @@ config SLAB_FREELIST_HARDENED
 	  sanity-checking than others. This option is most effective with
 	  CONFIG_SLUB.
 
+config SLUB_USE_SAFEBUDDY
+	default n
+	bool "Use quarantine based safe page allocator"
+	depends on SLUB && !SLUB_TINY && !SAFESLAB
+
+config SLUB_USE_SAFESLAB_DOMAINS
+	default n
+	bool "Make SLUB domain aware"
+	depends on SLUB && !SLUB_TINY && !SAFESLAB
+
+config SAFESLAB
+	default n
+	bool "Introduce option for hardened caches managed by safeslab allocator"
+	depends on SLUB && !SLUB_TINY
+
+config SAFESLAB_DEBUG
+	default n
+	bool "Activate debug mode for safeslab"
+	depends on SLUB && !SLUB_TINY && SAFESLAB
+
+config SAFESLAB_DEBUG_PRINT
+	default n
+	bool "Get debug prints for safeslab"
+	depends on SLUB && !SLUB_TINY && SAFESLAB && SAFESLAB_DEBUG
+
+config SAFESLAB_NO_SAFEBUDDY
+	default n
+	bool "Do not quarantine safeslab slabs. Only for debug, should not be used in actual systems!"
+	depends on SLUB && !SLUB_TINY && SAFESLAB && SAFESLAB_DEBUG
+
+config SAFESLAB_TRACE
+	default n
+	bool "Enables tracing events"
+	depends on SLUB && !SLUB_TINY && SAFESLAB && SAFESLAB_DEBUG
+
+config SAFESLAB_MEMBENCH
+	default n
+	bool "Enable for collecting memory consumption statistics"
+	depends on SLUB && !SLUB_TINY && SAFESLAB && SAFESLAB_DEBUG
+
+config SAFESLAB_ALL_CACHES
+	default n
+	bool "Make all caches system wide managed by safeslab"
+	depends on SLUB && !SLUB_TINY && SAFESLAB 
+
+config SAFESLAB_PK
+	default n
+	bool "Enable HW protection keys for isolating safeslab domains"
+	depends on SLUB && !SLUB_TINY && SAFESLAB 
+
+config SAFESLAB_ADDITIONAL_CHECKS
+	default n
+	bool "Additional checks for safeslab"
+	depends on SLUB && SAFESLAB && !SLUB_TINY
+
+config SAFEBUDDY_DEBUG
+	default n
+	bool "Have verbose debug out for quarantining safebuddy"
+	depends on SLUB && (SLUB_USE_SAFEBUDDY || SAFESLAB)
+
+config SAFESLAB_HARDENED_USERCOPY
+	default n
+	bool "Track useroffset/usersize for kmalloc caches and skip those regions in pointer scanner"
+	depends on SLUB && !SLUB_TINY && SAFESLAB
+
 config SLUB_STATS
 	default n
 	bool "Enable SLUB performance statistics"
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3e29..7d7f43174ecb 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -87,6 +87,7 @@ obj-$(CONFIG_KSM) += ksm.o
 obj-$(CONFIG_PAGE_POISONING) += page_poison.o
 obj-$(CONFIG_SLAB) += slab.o
 obj-$(CONFIG_SLUB) += slub.o
+obj-$(CONFIG_SAFESLAB) += safeslab.o
 obj-$(CONFIG_KASAN)	+= kasan/
 obj-$(CONFIG_KFENCE) += kfence/
 obj-$(CONFIG_KMSAN)	+= kmsan/
@@ -138,3 +139,16 @@ obj-$(CONFIG_IO_MAPPING) += io-mapping.o
 obj-$(CONFIG_HAVE_BOOTMEM_INFO_NODE) += bootmem_info.o
 obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o
 obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o
+
+obj-$(CONFIG_SAFESLAB) += safeslab.o
+
+ifdef CONFIG_SAFESLAB
+obj-y += safebuddy.o
+obj-y += exactfit.o
+else ifdef CONFIG_SLUB_USE_SAFEBUDDY
+obj-y += safebuddy.o
+else ifdef CONFIG_SLUB_USE_DOMAINS
+obj-y += safebuddy.o
+endif
+
+# obj-y += ksmap.o
\ No newline at end of file
diff --git a/mm/bootmem_info.c b/mm/bootmem_info.c
index b1efebfcf94b..f18a631e7479 100644
--- a/mm/bootmem_info.c
+++ b/mm/bootmem_info.c
@@ -12,7 +12,6 @@
 #include <linux/memblock.h>
 #include <linux/bootmem_info.h>
 #include <linux/memory_hotplug.h>
-#include <linux/kmemleak.h>
 
 void get_page_bootmem(unsigned long info, struct page *page, unsigned long type)
 {
@@ -34,7 +33,6 @@ void put_page_bootmem(struct page *page)
 		ClearPagePrivate(page);
 		set_page_private(page, 0);
 		INIT_LIST_HEAD(&page->lru);
-		kmemleak_free_part(page_to_virt(page), PAGE_SIZE);
 		free_reserved_page(page);
 	}
 }
diff --git a/mm/exactfit.c b/mm/exactfit.c
new file mode 100644
index 000000000000..aba18a2d6f4d
--- /dev/null
+++ b/mm/exactfit.c
@@ -0,0 +1,211 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#include <linux/mm.h>
+#include <linux/swap.h> /* struct reclaim_state */
+#include <linux/module.h>
+
+#include <linux/ksmap.h>
+
+#define EXACTFIT_HEAP_PAGES 8
+#define EXACTFIT_HEAP_SIZE EXACTFIT_HEAP_PAGES * PAGE_SIZE
+
+uint64_t nr_exactfit_pages = 0;
+
+struct exactfit_freechunk {
+	union {
+		struct list_head list_entry; // used for regular freechunks
+		uint64_t size; // used by topchunk only
+	};
+};
+
+struct exactfit_fastbin {
+	void* freelist;
+	unsigned long tid;
+};
+
+struct topchunk { // needed for cmpxchg_double
+	void *address;
+	uint64_t size;
+	spinlock_t spinlock;
+} topchunk;
+
+struct exactfit_fastbin *fastbins_cache;
+uint64_t nr_fastbins = 0;
+
+/* Unused yet */
+//struct exactfit_heap {
+//	struct exactfit_freechunk *topchunk;
+//	spinlock_t topchunk_spinlock;
+//	struct list_head fastbins;
+//	struct list_head heaps_entry;
+//};
+
+/* Unused yet */
+//LIST_HEAD(exactfit_heaps);
+
+#ifdef CONFIG_SLUB_HARDENED_FREELIST_AS_MAP_64_ALIGNED
+#define EXACTFIT_CHUNK_ALIGNMENT 64
+#else
+#define EXACTFIT_CHUNK_ALIGNMENT 8
+#endif
+
+static inline unsigned int req2allocsize(unsigned int size)
+{
+	return ALIGN(size, EXACTFIT_CHUNK_ALIGNMENT);
+}
+
+static inline unsigned int get_fastbinidx(unsigned int size)
+{
+	VM_BUG_ON(size > (PAGE_SIZE * sizeof(void *) / sizeof(struct exactfit_fastbin)));
+	return (size / EXACTFIT_CHUNK_ALIGNMENT) - 1;
+}
+
+static inline struct exactfit_fastbin *get_fastbin(unsigned int size)
+{
+	return &fastbins_cache[get_fastbinidx(size)];
+}
+
+static inline void exactfit_new_topchunk(void)
+{
+	struct page *pages;
+	int order = get_order(EXACTFIT_HEAP_SIZE);
+
+	nr_exactfit_pages += EXACTFIT_HEAP_PAGES;
+
+#ifdef CONFIG_ISLAB_SMAP
+	pages = get_isolated_pages(order);
+#else
+	pages = alloc_pages(GFP_KERNEL, order);
+#endif
+
+	topchunk.address = page_address(pages);
+
+	topchunk.size = EXACTFIT_HEAP_SIZE;
+}
+
+static void exactfit_config_heap(void)
+{
+	size_t i;
+	exactfit_new_topchunk();
+	spin_lock_init(&topchunk.spinlock);
+	
+    	//KAI used to be 0
+	fastbins_cache = page_address(alloc_pages(GFP_KERNEL, 3)); // alloc 1 page for storing fastbin struct info
+
+	for (i = 0; i <= get_fastbinidx(PAGE_SIZE); i++)
+		fastbins_cache[i].tid = 0;
+} 
+
+void __init exactfit_allocator_init(void)
+{
+	exactfit_config_heap();
+}
+
+inline void *exactfit_alloc(unsigned int size, gfp_t gfp_flags)
+{
+	struct exactfit_fastbin *fb;
+	unsigned long tid;
+	void *address;
+	void *freelist;
+	uint64_t old_topchunk_size;
+
+	size = req2allocsize(size);
+
+	fb = get_fastbin(size);
+
+redo:
+	tid = fb->tid;
+
+	/* We need to make sure that previous freelist will not be used with current tid */
+	barrier();
+	freelist = fb->freelist;
+
+	if (freelist) {
+		void *next;
+
+		copy_from_kernel_nofault(&next, freelist, sizeof(void*));
+
+		/* Atomically remove entry from freelist */
+		if(unlikely(!cmpxchg_double(&fb->freelist, &fb->tid, 
+						freelist, tid,
+						next, tid+1)))
+			goto redo;
+
+		address = freelist;
+	} else {
+new_chunk:
+		old_topchunk_size = topchunk.size;
+		
+		if (old_topchunk_size >= size) {
+			address = topchunk.address;
+			
+			if (!cmpxchg_double(&topchunk.address, &topchunk.size, 
+					    address, old_topchunk_size,
+					    address + size, old_topchunk_size - size))
+				goto new_chunk;
+		} else { // must enlarge topchunk
+			int order;
+			struct page *pages;
+			unsigned long flags;
+
+			if (gfpflags_allow_blocking(gfp_flags))
+				local_irq_save(flags);
+			
+			spin_lock(&topchunk.spinlock);
+
+			/* After waiting to acquire the spin_lock the topchunk might've been
+			 * changed by another task. In that case, unlock the spinlock and
+			 * redo the allocation process */
+			if(size <= topchunk.size) {
+				spin_unlock(&topchunk.spinlock);
+				if (gfpflags_allow_blocking(gfp_flags))
+					local_irq_restore(flags);
+				goto redo;
+			}
+
+			order = get_order(EXACTFIT_HEAP_SIZE);
+			nr_exactfit_pages += EXACTFIT_HEAP_PAGES;
+			//printk("current nr of pages: %lld\n", nr_exactfit_pages);
+#ifdef CONFIG_ISLAB_SMAP
+			pages = get_isolated_pages(order);
+#else
+			pages = alloc_pages(GFP_KERNEL, order);
+#endif
+			address = page_address(pages);
+			
+			topchunk.address = address + size;
+			topchunk.size = EXACTFIT_HEAP_SIZE - size;
+
+			spin_unlock(&topchunk.spinlock);
+			if (gfpflags_allow_blocking(gfp_flags))
+				local_irq_restore(flags);
+		}
+	}
+
+	return address;
+}
+EXPORT_SYMBOL(exactfit_alloc);
+
+inline void exactfit_free(void *address, unsigned int size)
+{
+	struct exactfit_fastbin *fb;
+	void *freelist;
+	unsigned long tid;
+
+	size = req2allocsize(size);
+	fb = get_fastbin(size);
+
+redo:
+	tid = fb->tid;
+
+	/* Same comment for alloc */
+	barrier();
+	freelist = fb->freelist;
+	*((void**)address) = freelist;
+
+	if(unlikely(!cmpxchg_double(&fb->freelist, &fb->tid,
+					freelist, tid,
+					address, tid + 1)))
+		goto redo;
+}
+EXPORT_SYMBOL(exactfit_free);
diff --git a/mm/kmemleak.c b/mm/kmemleak.c
index 55dc8b8b0616..1c02e8bce730 100644
--- a/mm/kmemleak.c
+++ b/mm/kmemleak.c
@@ -1,4 +1,6 @@
-// SPDX-License-Identifier: GPL-2.0-only
+//  Kai: I am butchering kmemleak ....
+//
+// SPDX-License-Identfier: GPL-2.0-only
 /*
  * mm/kmemleak.c
  *
@@ -95,6 +97,7 @@
 #include <linux/workqueue.h>
 #include <linux/crc32.h>
 
+#include <linux/exactfit.h>
 #include <asm/sections.h>
 #include <asm/processor.h>
 #include <linux/atomic.h>
@@ -104,13 +107,29 @@
 #include <linux/kmemleak.h>
 #include <linux/memory_hotplug.h>
 
+#include <linux/timekeeping.h>
+#include <linux/safebuddy.h>
+#include <linux/safeslab.h>
+#include "safeslab.h"
+#include "slab.h"
+#include <linux/slub_def.h>
+
+#include <trace/events/safeslab_trace.h>
+
+#include <asm/nmi.h>
+#include <asm/apic.h>
+
+#include <asm/pgalloc.h>
+#include <asm/tlbflush.h>
+
+#define DM_SIZE 0x4000000000
 /*
  * Kmemleak configuration and common defines.
  */
 #define MAX_TRACE		16	/* stack trace length */
 #define MSECS_MIN_AGE		5000	/* minimum object age for reporting */
-#define SECS_FIRST_SCAN		60	/* delay before the first scan */
-#define SECS_SCAN_WAIT		600	/* subsequent auto scanning delay */
+#define SECS_FIRST_SCAN		40	/* delay before the first scan */
+#define SECS_SCAN_WAIT		5	/* subsequent auto scanning delay */
 #define MAX_SCAN_SIZE		4096	/* maximum size of a scanned block */
 
 #define BYTES_PER_POINTER	sizeof(void *)
@@ -121,52 +140,9 @@
 				 __GFP_NORETRY | __GFP_NOMEMALLOC | \
 				 __GFP_NOWARN)
 
-/* scanning area inside a memory block */
-struct kmemleak_scan_area {
-	struct hlist_node node;
-	unsigned long start;
-	size_t size;
-};
-
-#define KMEMLEAK_GREY	0
-#define KMEMLEAK_BLACK	-1
-
-/*
- * Structure holding the metadata for each allocated memory block.
- * Modifications to such objects should be made while holding the
- * object->lock. Insertions or deletions from object_list, gray_list or
- * rb_node are already protected by the corresponding locks or mutex (see
- * the notes on locking above). These objects are reference-counted
- * (use_count) and freed using the RCU mechanism.
- */
-struct kmemleak_object {
-	raw_spinlock_t lock;
-	unsigned int flags;		/* object status flags */
-	struct list_head object_list;
-	struct list_head gray_list;
-	struct rb_node rb_node;
-	struct rcu_head rcu;		/* object_list lockless traversal */
-	/* object usage count; object freed when use_count == 0 */
-	atomic_t use_count;
-	unsigned long pointer;
-	size_t size;
-	/* pass surplus references to this pointer */
-	unsigned long excess_ref;
-	/* minimum number of a pointers found before it is considered leak */
-	int min_count;
-	/* the total number of pointers found pointing to this object */
-	int count;
-	/* checksum for detecting modified objects */
-	u32 checksum;
-	/* memory ranges to be scanned inside an object (empty for all) */
-	struct hlist_head area_list;
-	depot_stack_handle_t trace_handle;
-	unsigned long jiffies;		/* creation timestamp */
-	pid_t pid;			/* pid of the current task */
-	char comm[TASK_COMM_LEN];	/* executable name */
-};
 
 /* flag representing the memory block allocation status */
+// KAI: struct objects can have this flag set for an extended amount of time now. (In order to scan for them) 
 #define OBJECT_ALLOCATED	(1 << 0)
 /* flag set after the first reporting of an unreference object */
 #define OBJECT_REPORTED		(1 << 1)
@@ -187,39 +163,25 @@ struct kmemleak_object {
 /* max number of lines to be printed */
 #define HEX_MAX_LINES		2
 
-/* the list of all allocated objects */
-static LIST_HEAD(object_list);
 /* the list of gray-colored objects (see color_gray comment below) */
 static LIST_HEAD(gray_list);
-/* memory pool allocation */
-static struct kmemleak_object mem_pool[CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE];
-static int mem_pool_free_count = ARRAY_SIZE(mem_pool);
-static LIST_HEAD(mem_pool_free_list);
-/* search tree for object boundaries */
-static struct rb_root object_tree_root = RB_ROOT;
-/* search tree for object (with OBJECT_PHYS flag) boundaries */
-static struct rb_root object_phys_tree_root = RB_ROOT;
-/* protecting the access to object_list, object_tree_root (or object_phys_tree_root) */
-static DEFINE_RAW_SPINLOCK(kmemleak_lock);
-
-/* allocation caches for kmemleak internal data */
-static struct kmem_cache *object_cache;
-static struct kmem_cache *scan_area_cache;
-
+DEFINE_RAW_SPINLOCK(kmemleak_lock);
+EXPORT_SYMBOL(kmemleak_lock);
 /* set if tracing memory operations is enabled */
+///KAI: kmemleak disabled for benchmarking pkey things
 static int kmemleak_enabled = 1;
 /* same as above but only for the kmemleak_free() callback */
 static int kmemleak_free_enabled = 1;
 /* set in the late_initcall if there were no errors */
 static int kmemleak_initialized;
-/* set if a kmemleak warning was issued */
-static int kmemleak_warning;
+/* KAI: killed the kmemleak cache but they are used as  a check if we are done with the early initialisation phase */
+static int kmemleak_early_initialized; 
 /* set if a fatal kmemleak error has occurred */
 static int kmemleak_error;
 
 /* minimum and maximum address that may be valid pointers */
-static unsigned long min_addr = ULONG_MAX;
-static unsigned long max_addr;
+//static unsigned long min_addr = ULONG_MAX;
+//static unsigned long max_addr;
 
 static struct task_struct *scan_thread;
 /* used to avoid reporting of recently allocated objects */
@@ -230,17 +192,19 @@ static unsigned long jiffies_scan_wait;
 /* enables or disables the task stacks scanning */
 static int kmemleak_stack_scan = 1;
 /* protects the memory scanning, parameters and debug/kmemleak file access */
-static DEFINE_MUTEX(scan_mutex);
+DEFINE_MUTEX(scan_mutex);
+EXPORT_SYMBOL(scan_mutex);
 /* setting kmemleak=on, will set this var, skipping the disable */
 static int kmemleak_skip_disable;
-/* If there are leaks that can be reported */
-static bool kmemleak_found_leaks;
 
-static bool kmemleak_verbose;
+static bool kmemleak_verbose = 1;
 module_param_named(verbose, kmemleak_verbose, bool, 0600);
 
 static void kmemleak_disable(void);
 
+//unsigned long per_domain_count[16] = {0};
+//EXPORT_SYMBOL(per_domain_count);
+
 /*
  * Print a warning and dump the stack trace.
  */
@@ -267,1104 +231,379 @@ static void kmemleak_disable(void);
 		pr_warn(fmt, ##__VA_ARGS__);		\
 } while (0)
 
-static void warn_or_seq_hex_dump(struct seq_file *seq, int prefix_type,
-				 int rowsize, int groupsize, const void *buf,
-				 size_t len, bool ascii)
-{
-	if (seq)
-		seq_hex_dump(seq, HEX_PREFIX, prefix_type, rowsize, groupsize,
-			     buf, len, ascii);
-	else
-		print_hex_dump(KERN_WARNING, pr_fmt(HEX_PREFIX), prefix_type,
-			       rowsize, groupsize, buf, len, ascii);
-}
-
-/*
- * Printing of the objects hex dump to the seq file. The number of lines to be
- * printed is limited to HEX_MAX_LINES to prevent seq file spamming. The
- * actual number of printed bytes depends on HEX_ROW_SIZE. It must be called
- * with the object->lock held.
- */
-static void hex_dump_object(struct seq_file *seq,
-			    struct kmemleak_object *object)
-{
-	const u8 *ptr = (const u8 *)object->pointer;
-	size_t len;
-
-	if (WARN_ON_ONCE(object->flags & OBJECT_PHYS))
-		return;
-
-	/* limit the number of lines to HEX_MAX_LINES */
-	len = min_t(size_t, object->size, HEX_MAX_LINES * HEX_ROW_SIZE);
-
-	warn_or_seq_printf(seq, "  hex dump (first %zu bytes):\n", len);
-	kasan_disable_current();
-	warn_or_seq_hex_dump(seq, DUMP_PREFIX_NONE, HEX_ROW_SIZE,
-			     HEX_GROUP_SIZE, kasan_reset_tag((void *)ptr), len, HEX_ASCII);
-	kasan_enable_current();
-}
-
-/*
- * Object colors, encoded with count and min_count:
- * - white - orphan object, not enough references to it (count < min_count)
- * - gray  - not orphan, not marked as false positive (min_count == 0) or
- *		sufficient references to it (count >= min_count)
- * - black - ignore, it doesn't contain references (e.g. text section)
- *		(min_count == -1). No function defined for this color.
- * Newly created objects don't have any color assigned (object->count == -1)
- * before the next memory scan when they become white.
- */
-static bool color_white(const struct kmemleak_object *object)
-{
-	return object->count != KMEMLEAK_BLACK &&
-		object->count < object->min_count;
-}
-
-static bool color_gray(const struct kmemleak_object *object)
-{
-	return object->min_count != KMEMLEAK_BLACK &&
-		object->count >= object->min_count;
-}
-
-/*
- * Objects are considered unreferenced only if their color is white, they have
- * not be deleted and have a minimum age to avoid false positives caused by
- * pointers temporarily stored in CPU registers.
- */
-static bool unreferenced_object(struct kmemleak_object *object)
-{
-	return (color_white(object) && object->flags & OBJECT_ALLOCATED) &&
-		time_before_eq(object->jiffies + jiffies_min_age,
-			       jiffies_last_scan);
-}
-
-/*
- * Printing of the unreferenced objects information to the seq file. The
- * print_unreferenced function must be called with the object->lock held.
- */
-static void print_unreferenced(struct seq_file *seq,
-			       struct kmemleak_object *object)
-{
-	int i;
-	unsigned long *entries;
-	unsigned int nr_entries;
-	unsigned int msecs_age = jiffies_to_msecs(jiffies - object->jiffies);
-
-	nr_entries = stack_depot_fetch(object->trace_handle, &entries);
-	warn_or_seq_printf(seq, "unreferenced object 0x%08lx (size %zu):\n",
-			  object->pointer, object->size);
-	warn_or_seq_printf(seq, "  comm \"%s\", pid %d, jiffies %lu (age %d.%03ds)\n",
-			   object->comm, object->pid, object->jiffies,
-			   msecs_age / 1000, msecs_age % 1000);
-	hex_dump_object(seq, object);
-	warn_or_seq_printf(seq, "  backtrace:\n");
-
-	for (i = 0; i < nr_entries; i++) {
-		void *ptr = (void *)entries[i];
-		warn_or_seq_printf(seq, "    [<%pK>] %pS\n", ptr, ptr);
-	}
-}
-
-/*
- * Print the kmemleak_object information. This function is used mainly for
- * debugging special cases when kmemleak operations. It must be called with
- * the object->lock held.
- */
-static void dump_object_info(struct kmemleak_object *object)
-{
-	pr_notice("Object 0x%08lx (size %zu):\n",
-			object->pointer, object->size);
-	pr_notice("  comm \"%s\", pid %d, jiffies %lu\n",
-			object->comm, object->pid, object->jiffies);
-	pr_notice("  min_count = %d\n", object->min_count);
-	pr_notice("  count = %d\n", object->count);
-	pr_notice("  flags = 0x%x\n", object->flags);
-	pr_notice("  checksum = %u\n", object->checksum);
-	pr_notice("  backtrace:\n");
-	if (object->trace_handle)
-		stack_depot_print(object->trace_handle);
-}
-
-/*
- * Look-up a memory block metadata (kmemleak_object) in the object search
- * tree based on a pointer value. If alias is 0, only values pointing to the
- * beginning of the memory block are allowed. The kmemleak_lock must be held
- * when calling this function.
- */
-static struct kmemleak_object *__lookup_object(unsigned long ptr, int alias,
-					       bool is_phys)
-{
-	struct rb_node *rb = is_phys ? object_phys_tree_root.rb_node :
-			     object_tree_root.rb_node;
-	unsigned long untagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);
-
-	while (rb) {
-		struct kmemleak_object *object;
-		unsigned long untagged_objp;
-
-		object = rb_entry(rb, struct kmemleak_object, rb_node);
-		untagged_objp = (unsigned long)kasan_reset_tag((void *)object->pointer);
-
-		if (untagged_ptr < untagged_objp)
-			rb = object->rb_node.rb_left;
-		else if (untagged_objp + object->size <= untagged_ptr)
-			rb = object->rb_node.rb_right;
-		else if (untagged_objp == untagged_ptr || alias)
-			return object;
-		else {
-			kmemleak_warn("Found object by alias at 0x%08lx\n",
-				      ptr);
-			dump_object_info(object);
-			break;
-		}
-	}
-	return NULL;
-}
-
-/* Look-up a kmemleak object which allocated with virtual address. */
-static struct kmemleak_object *lookup_object(unsigned long ptr, int alias)
-{
-	return __lookup_object(ptr, alias, false);
-}
-
-/*
- * Increment the object use_count. Return 1 if successful or 0 otherwise. Note
- * that once an object's use_count reached 0, the RCU freeing was already
- * registered and the object should no longer be used. This function must be
- * called under the protection of rcu_read_lock().
- */
-static int get_object(struct kmemleak_object *object)
-{
-	return atomic_inc_not_zero(&object->use_count);
-}
-
-/*
- * Memory pool allocation and freeing. kmemleak_lock must not be held.
- */
-static struct kmemleak_object *mem_pool_alloc(gfp_t gfp)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-
-	/* try the slab allocator first */
-	if (object_cache) {
-		object = kmem_cache_alloc(object_cache, gfp_kmemleak_mask(gfp));
-		if (object)
-			return object;
-	}
-
-	/* slab allocation failed, try the memory pool */
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-	object = list_first_entry_or_null(&mem_pool_free_list,
-					  typeof(*object), object_list);
-	if (object)
-		list_del(&object->object_list);
-	else if (mem_pool_free_count)
-		object = &mem_pool[--mem_pool_free_count];
-	else
-		pr_warn_once("Memory pool empty, consider increasing CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE\n");
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
-
-	return object;
-}
-
-/*
- * Return the object to either the slab allocator or the memory pool.
- */
-static void mem_pool_free(struct kmemleak_object *object)
-{
-	unsigned long flags;
-
-	if (object < mem_pool || object >= mem_pool + ARRAY_SIZE(mem_pool)) {
-		kmem_cache_free(object_cache, object);
-		return;
-	}
-
-	/* add the object to the memory pool free list */
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-	list_add(&object->object_list, &mem_pool_free_list);
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
-}
-
-/*
- * RCU callback to free a kmemleak_object.
- */
-static void free_object_rcu(struct rcu_head *rcu)
-{
-	struct hlist_node *tmp;
-	struct kmemleak_scan_area *area;
-	struct kmemleak_object *object =
-		container_of(rcu, struct kmemleak_object, rcu);
-
-	/*
-	 * Once use_count is 0 (guaranteed by put_object), there is no other
-	 * code accessing this object, hence no need for locking.
-	 */
-	hlist_for_each_entry_safe(area, tmp, &object->area_list, node) {
-		hlist_del(&area->node);
-		kmem_cache_free(scan_area_cache, area);
-	}
-	mem_pool_free(object);
-}
 
 /*
- * Decrement the object use_count. Once the count is 0, free the object using
- * an RCU callback. Since put_object() may be called via the kmemleak_free() ->
- * delete_object() path, the delayed RCU freeing ensures that there is no
- * recursive call to the kernel allocator. Lock-less RCU object_list traversal
- * is also possible.
- */
-static void put_object(struct kmemleak_object *object)
-{
-	if (!atomic_dec_and_test(&object->use_count))
-		return;
-
-	/* should only get here after delete_object was called */
-	WARN_ON(object->flags & OBJECT_ALLOCATED);
-
-	/*
-	 * It may be too early for the RCU callbacks, however, there is no
-	 * concurrent object_list traversal when !object_cache and all objects
-	 * came from the memory pool. Free the object directly.
-	 */
-	if (object_cache)
-		call_rcu(&object->rcu, free_object_rcu);
-	else
-		free_object_rcu(&object->rcu);
-}
-
-/*
- * Look up an object in the object search tree and increase its use_count.
- */
-static struct kmemleak_object *__find_and_get_object(unsigned long ptr, int alias,
-						     bool is_phys)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-
-	rcu_read_lock();
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-	object = __lookup_object(ptr, alias, is_phys);
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
-
-	/* check whether the object is still available */
-	if (object && !get_object(object))
-		object = NULL;
-	rcu_read_unlock();
-
-	return object;
-}
-
-/* Look up and get an object which allocated with virtual address. */
-static struct kmemleak_object *find_and_get_object(unsigned long ptr, int alias)
-{
-	return __find_and_get_object(ptr, alias, false);
-}
-
-/*
- * Remove an object from the object_tree_root (or object_phys_tree_root)
- * and object_list. Must be called with the kmemleak_lock held _if_ kmemleak
- * is still enabled.
- */
-static void __remove_object(struct kmemleak_object *object)
-{
-	rb_erase(&object->rb_node, object->flags & OBJECT_PHYS ?
-				   &object_phys_tree_root :
-				   &object_tree_root);
-	list_del_rcu(&object->object_list);
-}
-
-/*
- * Look up an object in the object search tree and remove it from both
- * object_tree_root (or object_phys_tree_root) and object_list. The
- * returned object's use_count should be at least 1, as initially set
- * by create_object().
+ * Memory scanning is a long process and it needs to be interruptible. This
+ * function checks whether such interrupt condition occurred.
  */
-static struct kmemleak_object *find_and_remove_object(unsigned long ptr, int alias,
-						      bool is_phys)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-	object = __lookup_object(ptr, alias, is_phys);
-	if (object)
-		__remove_object(object);
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
+//static int scan_should_stop(void)
+//{
+//	if (!kmemleak_enabled)
+//		return 1;
+//
+//	/*
+//	 * This function may be called from either process or kthread context,
+//	 * hence the need to check for both stop conditions.
+//	 */
+//	if (current->mm)
+//		return signal_pending(current);
+//	else
+//		return kthread_should_stop();
+//
+//	return 0;
+//}
+
+unsigned int set_sid = 1;
+EXPORT_SYMBOL(set_sid);
+unsigned int check_sid = 1;
+EXPORT_SYMBOL(check_sid);
+
+bool scanner_started;
+EXPORT_SYMBOL(scanner_started);
+
+//#define kmemleak_printk(fmt, ...) printk("[KMEMLEAK] %s " #fmt, __FUNCTION__, ##__VA_ARGS__)
+#define kmemleak_printk(fmt, ...) (void)fmt
+
+//#define kmemleak_prio_printk(fmt, ...) printk(fmt, ##__VA_ARGS__)
+#define kmemleak_prio_printk(fmt, ...) (void)fmt
+
+//#define KMEMLEAK_COUNT_DP_FREQ
+
+#ifdef KMEMLEAK_COUNT_DP_FREQ
+
+struct pfn_freq_entry {
+	int pfn;
+	int freq;
+};
 
-	return object;
-}
+#define PFN_FREQ_LEN 2 * 4096
+static struct pfn_freq_entry pfn_freq[PFN_FREQ_LEN];
+static int last_pfn_idx = 0;
 
-static noinline depot_stack_handle_t set_track_prepare(void)
+static inline void reset_freq(void)
 {
-	depot_stack_handle_t trace_handle;
-	unsigned long entries[MAX_TRACE];
-	unsigned int nr_entries;
-
-	if (!kmemleak_initialized)
-		return 0;
-	nr_entries = stack_trace_save(entries, ARRAY_SIZE(entries), 3);
-	trace_handle = stack_depot_save(entries, nr_entries, GFP_NOWAIT);
-
-	return trace_handle;
+	last_pfn_idx = 0;
 }
 
-/*
- * Create the metadata (struct kmemleak_object) corresponding to an allocated
- * memory block and add it to the object_list and object_tree_root (or
- * object_phys_tree_root).
- */
-static void __create_object(unsigned long ptr, size_t size,
-			    int min_count, gfp_t gfp, bool is_phys)
+static inline void inc_freq_pfn(int pfn)
 {
-	unsigned long flags;
-	struct kmemleak_object *object, *parent;
-	struct rb_node **link, *rb_parent;
-	unsigned long untagged_ptr;
-	unsigned long untagged_objp;
-
-	object = mem_pool_alloc(gfp);
-	if (!object) {
-		pr_warn("Cannot allocate a kmemleak_object structure\n");
-		kmemleak_disable();
+	if (last_pfn_idx == PFN_FREQ_LEN) {
+		pr_warn("SCANNER: reached max pfn freq\n");	
 		return;
 	}
 
-	INIT_LIST_HEAD(&object->object_list);
-	INIT_LIST_HEAD(&object->gray_list);
-	INIT_HLIST_HEAD(&object->area_list);
-	raw_spin_lock_init(&object->lock);
-	atomic_set(&object->use_count, 1);
-	object->flags = OBJECT_ALLOCATED | (is_phys ? OBJECT_PHYS : 0);
-	object->pointer = ptr;
-	object->size = kfence_ksize((void *)ptr) ?: size;
-	object->excess_ref = 0;
-	object->min_count = min_count;
-	object->count = 0;			/* white color initially */
-	object->jiffies = jiffies;
-	object->checksum = 0;
-
-	/* task information */
-	if (in_hardirq()) {
-		object->pid = 0;
-		strncpy(object->comm, "hardirq", sizeof(object->comm));
-	} else if (in_serving_softirq()) {
-		object->pid = 0;
-		strncpy(object->comm, "softirq", sizeof(object->comm));
-	} else {
-		object->pid = current->pid;
-		/*
-		 * There is a small chance of a race with set_task_comm(),
-		 * however using get_task_comm() here may cause locking
-		 * dependency issues with current->alloc_lock. In the worst
-		 * case, the command line is not correct.
-		 */
-		strncpy(object->comm, current->comm, sizeof(object->comm));
-	}
-
-	/* kernel backtrace */
-	object->trace_handle = set_track_prepare();
-
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-
-	untagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);
-	/*
-	 * Only update min_addr and max_addr with object
-	 * storing virtual address.
-	 */
-	if (!is_phys) {
-		min_addr = min(min_addr, untagged_ptr);
-		max_addr = max(max_addr, untagged_ptr + size);
-	}
-	link = is_phys ? &object_phys_tree_root.rb_node :
-		&object_tree_root.rb_node;
-	rb_parent = NULL;
-	while (*link) {
-		rb_parent = *link;
-		parent = rb_entry(rb_parent, struct kmemleak_object, rb_node);
-		untagged_objp = (unsigned long)kasan_reset_tag((void *)parent->pointer);
-		if (untagged_ptr + size <= untagged_objp)
-			link = &parent->rb_node.rb_left;
-		else if (untagged_objp + parent->size <= untagged_ptr)
-			link = &parent->rb_node.rb_right;
-		else {
-			kmemleak_stop("Cannot insert 0x%lx into the object search tree (overlaps existing)\n",
-				      ptr);
-			/*
-			 * No need for parent->lock here since "parent" cannot
-			 * be freed while the kmemleak_lock is held.
-			 */
-			dump_object_info(parent);
-			kmem_cache_free(object_cache, object);
-			goto out;
+	for (int i = 0; i < last_pfn_idx; i++) {
+		if (pfn_freq[i].pfn == pfn) {
+			pfn_freq[i].freq++;
+			return;
 		}
 	}
-	rb_link_node(&object->rb_node, rb_parent, link);
-	rb_insert_color(&object->rb_node, is_phys ? &object_phys_tree_root :
-					  &object_tree_root);
-	list_add_tail_rcu(&object->object_list, &object_list);
-out:
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
-}
-
-/* Create kmemleak object which allocated with virtual address. */
-static void create_object(unsigned long ptr, size_t size,
-			  int min_count, gfp_t gfp)
-{
-	__create_object(ptr, size, min_count, gfp, false);
-}
-
-/* Create kmemleak object which allocated with physical address. */
-static void create_object_phys(unsigned long ptr, size_t size,
-			       int min_count, gfp_t gfp)
-{
-	__create_object(ptr, size, min_count, gfp, true);
-}
-
-/*
- * Mark the object as not allocated and schedule RCU freeing via put_object().
- */
-static void __delete_object(struct kmemleak_object *object)
-{
-	unsigned long flags;
-
-	WARN_ON(!(object->flags & OBJECT_ALLOCATED));
-	WARN_ON(atomic_read(&object->use_count) < 1);
-
-	/*
-	 * Locking here also ensures that the corresponding memory block
-	 * cannot be freed when it is being scanned.
-	 */
-	raw_spin_lock_irqsave(&object->lock, flags);
-	object->flags &= ~OBJECT_ALLOCATED;
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-	put_object(object);
-}
 
-/*
- * Look up the metadata (struct kmemleak_object) corresponding to ptr and
- * delete it.
- */
-static void delete_object_full(unsigned long ptr)
-{
-	struct kmemleak_object *object;
-
-	object = find_and_remove_object(ptr, 0, false);
-	if (!object) {
-#ifdef DEBUG
-		kmemleak_warn("Freeing unknown object at 0x%08lx\n",
-			      ptr);
-#endif
-		return;
-	}
-	__delete_object(object);
+	pfn_freq[last_pfn_idx].pfn = pfn;
+	pfn_freq[last_pfn_idx].freq = 1;
+	last_pfn_idx++;
 }
-
-/*
- * Look up the metadata (struct kmemleak_object) corresponding to ptr and
- * delete it. If the memory block is partially freed, the function may create
- * additional metadata for the remaining parts of the block.
- */
-static void delete_object_part(unsigned long ptr, size_t size, bool is_phys)
-{
-	struct kmemleak_object *object;
-	unsigned long start, end;
-
-	object = find_and_remove_object(ptr, 1, is_phys);
-	if (!object) {
-#ifdef DEBUG
-		kmemleak_warn("Partially freeing unknown object at 0x%08lx (size %zu)\n",
-			      ptr, size);
+#else
+#define reset_freq()
+#define inc_freq_pfn(int)
 #endif
-		return;
-	}
-
-	/*
-	 * Create one or two objects that may result from the memory block
-	 * split. Note that partial freeing is only done by free_bootmem() and
-	 * this happens before kmemleak_init() is called.
-	 */
-	start = object->pointer;
-	end = object->pointer + object->size;
-	if (ptr > start)
-		__create_object(start, ptr - start, object->min_count,
-			      GFP_KERNEL, is_phys);
-	if (ptr + size < end)
-		__create_object(ptr + size, end - ptr - size, object->min_count,
-			      GFP_KERNEL, is_phys);
-
-	__delete_object(object);
-}
-
-static void __paint_it(struct kmemleak_object *object, int color)
-{
-	object->min_count = color;
-	if (color == KMEMLEAK_BLACK)
-		object->flags |= OBJECT_NO_SCAN;
-}
-
-static void paint_it(struct kmemleak_object *object, int color)
-{
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	__paint_it(object, color);
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-}
-
-static void paint_ptr(unsigned long ptr, int color, bool is_phys)
-{
-	struct kmemleak_object *object;
-
-	object = __find_and_get_object(ptr, 0, is_phys);
-	if (!object) {
-		kmemleak_warn("Trying to color unknown object at 0x%08lx as %s\n",
-			      ptr,
-			      (color == KMEMLEAK_GREY) ? "Grey" :
-			      (color == KMEMLEAK_BLACK) ? "Black" : "Unknown");
-		return;
-	}
-	paint_it(object, color);
-	put_object(object);
-}
-
-/*
- * Mark an object permanently as gray-colored so that it can no longer be
- * reported as a leak. This is used in general to mark a false positive.
- */
-static void make_gray_object(unsigned long ptr)
-{
-	paint_ptr(ptr, KMEMLEAK_GREY, false);
-}
-
-/*
- * Mark the object as black-colored so that it is ignored from scans and
- * reporting.
- */
-static void make_black_object(unsigned long ptr, bool is_phys)
-{
-	paint_ptr(ptr, KMEMLEAK_BLACK, is_phys);
-}
-
-/*
- * Add a scanning area to the object. If at least one such area is added,
- * kmemleak will only scan these ranges rather than the whole memory block.
- */
-static void add_scan_area(unsigned long ptr, size_t size, gfp_t gfp)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-	struct kmemleak_scan_area *area = NULL;
-	unsigned long untagged_ptr;
-	unsigned long untagged_objp;
-
-	object = find_and_get_object(ptr, 1);
-	if (!object) {
-		kmemleak_warn("Adding scan area to unknown object at 0x%08lx\n",
-			      ptr);
-		return;
-	}
-
-	untagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);
-	untagged_objp = (unsigned long)kasan_reset_tag((void *)object->pointer);
-
-	if (scan_area_cache)
-		area = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	if (!area) {
-		pr_warn_once("Cannot allocate a scan area, scanning the full object\n");
-		/* mark the object for full scan to avoid false positives */
-		object->flags |= OBJECT_FULL_SCAN;
-		goto out_unlock;
-	}
-	if (size == SIZE_MAX) {
-		size = untagged_objp + object->size - untagged_ptr;
-	} else if (untagged_ptr + size > untagged_objp + object->size) {
-		kmemleak_warn("Scan area larger than object 0x%08lx\n", ptr);
-		dump_object_info(object);
-		kmem_cache_free(scan_area_cache, area);
-		goto out_unlock;
-	}
-
-	INIT_HLIST_NODE(&area->node);
-	area->start = ptr;
-	area->size = size;
-
-	hlist_add_head(&area->node, &object->area_list);
-out_unlock:
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-	put_object(object);
-}
-
-/*
- * Any surplus references (object already gray) to 'ptr' are passed to
- * 'excess_ref'. This is used in the vmalloc() case where a pointer to
- * vm_struct may be used as an alternative reference to the vmalloc'ed object
- * (see free_thread_stack()).
- */
-static void object_set_excess_ref(unsigned long ptr, unsigned long excess_ref)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-
-	object = find_and_get_object(ptr, 0);
-	if (!object) {
-		kmemleak_warn("Setting excess_ref on unknown object at 0x%08lx\n",
-			      ptr);
-		return;
-	}
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	object->excess_ref = excess_ref;
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-	put_object(object);
-}
-
-/*
- * Set the OBJECT_NO_SCAN flag for the object corresponding to the give
- * pointer. Such object will not be scanned by kmemleak but references to it
- * are searched.
- */
-static void object_no_scan(unsigned long ptr)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-
-	object = find_and_get_object(ptr, 0);
-	if (!object) {
-		kmemleak_warn("Not scanning unknown object at 0x%08lx\n", ptr);
-		return;
-	}
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	object->flags |= OBJECT_NO_SCAN;
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-	put_object(object);
-}
-
-/**
- * kmemleak_alloc - register a newly allocated object
- * @ptr:	pointer to beginning of the object
- * @size:	size of the object
- * @min_count:	minimum number of references to this object. If during memory
- *		scanning a number of references less than @min_count is found,
- *		the object is reported as a memory leak. If @min_count is 0,
- *		the object is never reported as a leak. If @min_count is -1,
- *		the object is ignored (not scanned and not reported as a leak)
- * @gfp:	kmalloc() flags used for kmemleak internal memory allocations
- *
- * This function is called from the kernel allocators when a new object
- * (memory block) is allocated (kmem_cache_alloc, kmalloc etc.).
- */
-void __ref kmemleak_alloc(const void *ptr, size_t size, int min_count,
-			  gfp_t gfp)
-{
-	pr_debug("%s(0x%p, %zu, %d)\n", __func__, ptr, size, min_count);
-
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		create_object((unsigned long)ptr, size, min_count, gfp);
-}
-EXPORT_SYMBOL_GPL(kmemleak_alloc);
 
-/**
- * kmemleak_alloc_percpu - register a newly allocated __percpu object
- * @ptr:	__percpu pointer to beginning of the object
- * @size:	size of the object
- * @gfp:	flags used for kmemleak internal memory allocations
- *
- * This function is called from the kernel percpu allocator when a new object
- * (memory block) is allocated (alloc_percpu).
- */
-void __ref kmemleak_alloc_percpu(const void __percpu *ptr, size_t size,
-				 gfp_t gfp)
-{
-	unsigned int cpu;
-
-	pr_debug("%s(0x%p, %zu)\n", __func__, ptr, size);
-
-	/*
-	 * Percpu allocations are only scanned and not reported as leaks
-	 * (min_count is set to 0).
-	 */
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		for_each_possible_cpu(cpu)
-			create_object((unsigned long)per_cpu_ptr(ptr, cpu),
-				      size, 0, gfp);
-}
-EXPORT_SYMBOL_GPL(kmemleak_alloc_percpu);
+//LOOK AT KGDB stuff, they get their cpus into a holding pattern with the kgdb nmi handler (kgdb_cpu_enter gets called there - might be interesting too, not sure yet...)!!!!
+bool world_is_stopped = 0;
+EXPORT_SYMBOL(world_is_stopped);
 
-/**
- * kmemleak_vmalloc - register a newly vmalloc'ed object
- * @area:	pointer to vm_struct
- * @size:	size of the object
- * @gfp:	__vmalloc() flags used for kmemleak internal memory allocations
- *
- * This function is called from the vmalloc() kernel allocator when a new
- * object (memory block) is allocated.
- */
-void __ref kmemleak_vmalloc(const struct vm_struct *area, size_t size, gfp_t gfp)
+static int stw_nmi_handler(unsigned int reason, struct pt_regs * regs)
 {
-	pr_debug("%s(0x%p, %zu)\n", __func__, area, size);
+	// if we're in user mode, just return
+	if(user_mode(regs))
+		return NMI_HANDLED;
 
-	/*
-	 * A min_count = 2 is needed because vm_struct contains a reference to
-	 * the virtual address of the vmalloc'ed block.
-	 */
-	if (kmemleak_enabled) {
-		create_object((unsigned long)area->addr, size, 2, gfp);
-		object_set_excess_ref((unsigned long)area,
-				      (unsigned long)area->addr);
+	// if we're in an IRQ, just return	
+	if(irq_count())
+		return NMI_HANDLED;
+	
+	while (world_is_stopped) {
+		// schedule another task if we're not running with preemption disabled
+		if (!in_atomic())
+			schedule();
 	}
-}
-EXPORT_SYMBOL_GPL(kmemleak_vmalloc);
-
-/**
- * kmemleak_free - unregister a previously registered object
- * @ptr:	pointer to beginning of the object
- *
- * This function is called from the kernel allocators when an object (memory
- * block) is freed (kmem_cache_free, kfree, vfree etc.).
- */
-void __ref kmemleak_free(const void *ptr)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_free_enabled && ptr && !IS_ERR(ptr))
-		delete_object_full((unsigned long)ptr);
-}
-EXPORT_SYMBOL_GPL(kmemleak_free);
-
-/**
- * kmemleak_free_part - partially unregister a previously registered object
- * @ptr:	pointer to the beginning or inside the object. This also
- *		represents the start of the range to be freed
- * @size:	size to be unregistered
- *
- * This function is called when only a part of a memory block is freed
- * (usually from the bootmem allocator).
- */
-void __ref kmemleak_free_part(const void *ptr, size_t size)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		delete_object_part((unsigned long)ptr, size, false);
-}
-EXPORT_SYMBOL_GPL(kmemleak_free_part);
-
-/**
- * kmemleak_free_percpu - unregister a previously registered __percpu object
- * @ptr:	__percpu pointer to beginning of the object
- *
- * This function is called from the kernel percpu allocator when an object
- * (memory block) is freed (free_percpu).
- */
-void __ref kmemleak_free_percpu(const void __percpu *ptr)
-{
-	unsigned int cpu;
-
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_free_enabled && ptr && !IS_ERR(ptr))
-		for_each_possible_cpu(cpu)
-			delete_object_full((unsigned long)per_cpu_ptr(ptr,
-								      cpu));
-}
-EXPORT_SYMBOL_GPL(kmemleak_free_percpu);
-
-/**
- * kmemleak_update_trace - update object allocation stack trace
- * @ptr:	pointer to beginning of the object
- *
- * Override the object allocation stack trace for cases where the actual
- * allocation place is not always useful.
- */
-void __ref kmemleak_update_trace(const void *ptr)
-{
-	struct kmemleak_object *object;
-	unsigned long flags;
-
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (!kmemleak_enabled || IS_ERR_OR_NULL(ptr))
-		return;
-
-	object = find_and_get_object((unsigned long)ptr, 1);
-	if (!object) {
-#ifdef DEBUG
-		kmemleak_warn("Updating stack trace for unknown object at %p\n",
-			      ptr);
-#endif
-		return;
-	}
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	object->trace_handle = set_track_prepare();
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-
-	put_object(object);
-}
-EXPORT_SYMBOL(kmemleak_update_trace);
-
-/**
- * kmemleak_not_leak - mark an allocated object as false positive
- * @ptr:	pointer to beginning of the object
- *
- * Calling this function on an object will cause the memory block to no longer
- * be reported as leak and always be scanned.
- */
-void __ref kmemleak_not_leak(const void *ptr)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		make_gray_object((unsigned long)ptr);
-}
-EXPORT_SYMBOL(kmemleak_not_leak);
-
-/**
- * kmemleak_ignore - ignore an allocated object
- * @ptr:	pointer to beginning of the object
- *
- * Calling this function on an object will cause the memory block to be
- * ignored (not scanned and not reported as a leak). This is usually done when
- * it is known that the corresponding block is not a leak and does not contain
- * any references to other allocated memory blocks.
- */
-void __ref kmemleak_ignore(const void *ptr)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		make_black_object((unsigned long)ptr, false);
-}
-EXPORT_SYMBOL(kmemleak_ignore);
-
-/**
- * kmemleak_scan_area - limit the range to be scanned in an allocated object
- * @ptr:	pointer to beginning or inside the object. This also
- *		represents the start of the scan area
- * @size:	size of the scan area
- * @gfp:	kmalloc() flags used for kmemleak internal memory allocations
- *
- * This function is used when it is known that only certain parts of an object
- * contain references to other objects. Kmemleak will only scan these areas
- * reducing the number false negatives.
- */
-void __ref kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_enabled && ptr && size && !IS_ERR(ptr))
-		add_scan_area((unsigned long)ptr, size, gfp);
-}
-EXPORT_SYMBOL(kmemleak_scan_area);
-
-/**
- * kmemleak_no_scan - do not scan an allocated object
- * @ptr:	pointer to beginning of the object
- *
- * This function notifies kmemleak not to scan the given memory block. Useful
- * in situations where it is known that the given object does not contain any
- * references to other objects. Kmemleak will not scan such objects reducing
- * the number of false negatives.
- */
-void __ref kmemleak_no_scan(const void *ptr)
-{
-	pr_debug("%s(0x%p)\n", __func__, ptr);
-
-	if (kmemleak_enabled && ptr && !IS_ERR(ptr))
-		object_no_scan((unsigned long)ptr);
-}
-EXPORT_SYMBOL(kmemleak_no_scan);
-
-/**
- * kmemleak_alloc_phys - similar to kmemleak_alloc but taking a physical
- *			 address argument
- * @phys:	physical address of the object
- * @size:	size of the object
- * @gfp:	kmalloc() flags used for kmemleak internal memory allocations
- */
-void __ref kmemleak_alloc_phys(phys_addr_t phys, size_t size, gfp_t gfp)
-{
-	pr_debug("%s(0x%pa, %zu)\n", __func__, &phys, size);
-
-	if (kmemleak_enabled)
-		/*
-		 * Create object with OBJECT_PHYS flag and
-		 * assume min_count 0.
-		 */
-		create_object_phys((unsigned long)phys, size, 0, gfp);
-}
-EXPORT_SYMBOL(kmemleak_alloc_phys);
-
-/**
- * kmemleak_free_part_phys - similar to kmemleak_free_part but taking a
- *			     physical address argument
- * @phys:	physical address if the beginning or inside an object. This
- *		also represents the start of the range to be freed
- * @size:	size to be unregistered
- */
-void __ref kmemleak_free_part_phys(phys_addr_t phys, size_t size)
-{
-	pr_debug("%s(0x%pa)\n", __func__, &phys);
 
-	if (kmemleak_enabled)
-		delete_object_part((unsigned long)phys, size, true);
+	return NMI_HANDLED;
 }
-EXPORT_SYMBOL(kmemleak_free_part_phys);
 
-/**
- * kmemleak_ignore_phys - similar to kmemleak_ignore but taking a physical
- *			  address argument
- * @phys:	physical address of the object
- */
-void __ref kmemleak_ignore_phys(phys_addr_t phys)
+void stw_from_irq_kernel(void)
 {
-	pr_debug("%s(0x%pa)\n", __func__, &phys);
-
-	if (kmemleak_enabled)
-		make_black_object((unsigned long)phys, true);
+	do {
+		// schedule another task if we're not running with preemption disabled
+		if (!in_atomic())
+			schedule();
+	} while (world_is_stopped);
 }
-EXPORT_SYMBOL(kmemleak_ignore_phys);
 
-/*
- * Update an object's checksum and return true if it was modified.
- */
-static bool update_checksum(struct kmemleak_object *object)
+static void stop_the_world(void)
 {
-	u32 old_csum = object->checksum;
+	preempt_disable();
+	WRITE_ONCE(world_is_stopped, 1);
+	register_nmi_handler(NMI_LOCAL, stw_nmi_handler, NMI_FLAG_FIRST, "stop_the_world");
+	apic_send_IPI_allbutself(NMI_VECTOR);
+}
 
-	if (WARN_ON_ONCE(object->flags & OBJECT_PHYS))
-		return false;
+static void restart_the_world(void)
+{
+	world_is_stopped = 0;
+	preempt_enable();
+	unregister_nmi_handler(NMI_LOCAL,"stop_the_world");
+}
 
-	kasan_disable_current();
-	kcsan_disable_current();
-	object->checksum = crc32(0, kasan_reset_tag((void *)object->pointer), object->size);
-	kasan_enable_current();
-	kcsan_enable_current();
+//static unsigned long dangling_ptrs = 0;
 
-	return object->checksum != old_csum;
+static void kmemleak_populate_pte(pte_t *ptep, unsigned long address, unsigned int pkey)
+{
+	struct page *page =  virt_to_page(address);
+	unsigned long offset, pfn = page_to_pfn(page);
+	for (offset=0; offset < PTRS_PER_PTE; offset++, pfn++) {
+		set_pte(ptep, pfn_pte(pfn,__pgprot(__PAGE_KERNEL_PKEY)));
+		set_pte(ptep, pte_set_pkey(*ptep, pkey));
+		ptep++;
+	}
 }
 
-/*
- * Update an object's references. object->lock must be held by the caller.
- */
-static void update_refs(struct kmemleak_object *object)
+// The definiton of KAI_FAULTDEBUG results in printing debug output
+// in fault handler for protection key related faults
+#define safeslab_fault_printk(...)
+//#define safeslab_fault_printk(fmt, ...) printk("[SAFESLAB] %s " #fmt, __FUNCTION__, ##__VA_ARGS__)
+
+static void kmemleak_split_huge_page(pmd_t *pmdp)
 {
-	if (!color_white(object)) {
-		/* non-orphan, ignored or new */
+	pte_t *new_ptep;
+	unsigned long address;
+	unsigned int  pkey;
+
+	if (!(new_ptep = pte_alloc_one_kernel(&init_mm))) {
+		panic("[SAFESLAB] failed to split: oom\n");
 		return;
 	}
 
-	/*
-	 * Increase the object's reference count (number of pointers to the
-	 * memory block). If this count reaches the required minimum, the
-	 * object's color will become gray and it will be added to the
-	 * gray_list.
-	 */
-	object->count++;
-	if (color_gray(object)) {
-		/* put_object() called when removing from gray_list */
-		WARN_ON(!get_object(object));
-		list_add_tail(&object->gray_list, &gray_list);
-	}
+	safeslab_fault_printk("Splitting hugepage!\n");
+
+#ifdef CONFIG_SAFESLAB_TRACE
+	trace_split_huge_page(_RET_IP_);
+#endif
+
+	address = pmd_page_vaddr(*pmdp);
+	pkey = pte_flags_pkey(pmd_flags(*pmdp));
+	kmemleak_populate_pte(new_ptep, address, pkey);
+	
+	smp_wmb();
+	set_pmd(pmdp,__pmd(__pa(new_ptep) | _KERNPG_TABLE | _PAGE_NX));
 }
 
-/*
- * Memory scanning is a long process and it needs to be interruptible. This
- * function checks whether such interrupt condition occurred.
- */
-static int scan_should_stop(void)
-{
-	if (!kmemleak_enabled)
-		return 1;
+#define NR_DMS 16
 
-	/*
-	 * This function may be called from either process or kthread context,
-	 * hence the need to check for both stop conditions.
-	 */
-	if (current->mm)
-		return signal_pending(current);
-	else
-		return kthread_should_stop();
+struct safeslab_pfn_arr {
+	unsigned int *pfns;
+	unsigned short len;
+};
 
-	return 0;
+struct safeslab_pfn_arr pfn_arrs[2][32];
+
+struct safeslab_pfn_arr *unmapped_pfns_ptr = pfn_arrs[0];
+unsigned char unmapped_pfns_len = 0;
+
+struct safeslab_pfn_arr *tomap_pfns_ptr = pfn_arrs[1];
+unsigned char tomap_pfns_len = 0;
+unsigned short vas_remapped = 0, vas_unmapped = 0;
+
+void safeslab_process_pfns(struct safeslab_pfn_arr *pfn_arrs, unsigned short pfn_arrs_len, bool map)
+{
+	pgd_t* pgdp;
+	p4d_t* p4dp;
+	pud_t* pudp;
+	pmd_t* pmdp;
+	pte_t* ptep;
+	unsigned long flags, pfn;
+	struct page *page;
+
+	spin_lock_irqsave(&init_mm.page_table_lock, flags);
+
+	for (int k = 0; k < pfn_arrs_len; k++) {
+		unsigned int *pfn_arr = pfn_arrs[k].pfns;
+		unsigned short pfn_arr_len = pfn_arrs[k].len;
+		
+		for (int j = 0; j < pfn_arr_len; j++) {
+			pfn = pfn_arr[j];
+			page = pfn_to_page(pfn);
+
+			for (int i = 1; i < NR_DMS; i++) {
+
+				// unmap/remap slab for tmds it was marked in
+				if (!map && page->safeslab->unmapped_sid[i] != set_sid)
+					continue;
+				else if (map && (page->safeslab->unmapped_sid[i] != (set_sid - 1)))
+					continue;
+				
+				// unmap/remap all pages in the slab
+				for (int l = 0; l < (1 << SAFEBUDDY_ORDER_EXTEND); l++) {
+					unsigned long va_tmd = page_to_virt_dm(page + l, i);
+
+					pgdp = pgd_offset_k(va_tmd);
+					p4dp = p4d_offset(pgdp, va_tmd);
+					pudp = pud_offset(p4dp, va_tmd);
+					pmdp = pmd_offset(pudp, va_tmd);
+
+					// huge pages (2MB only) are being handled here
+					if (pmd_large(*pmdp))
+						kmemleak_split_huge_page(pmdp);
+
+					ptep = pte_offset_kernel(pmdp, va_tmd);
+					if (map) {
+						set_pte(ptep, pte_mkwrite(*ptep));
+						vas_remapped++;
+					} else {
+						// TODO: make it invalid here
+						set_pte(ptep, pte_wrprotect(*ptep));
+						vas_unmapped++;
+					}
+				}
+			}
+		}
+	}
+	
+	spin_unlock_irqrestore(&init_mm.page_table_lock, flags);
 }
 
+//unsigned long unmapped_pfns[2][512];
+//unsigned long *unmapped_pfns_ptr = unmapped_pfns[0];
+//
+//unsigned long *tomap_pfns_ptr = NULL;
+//unsigned short tomap_pfns_len = 0;
+
 /*
  * Scan a memory block (exclusive range) for valid pointers and add those
  * found to the gray list.
  */
-static void scan_block(void *_start, void *_end,
-		       struct kmemleak_object *scanned)
+static inline void scan_block(void *_start, void *_end)
 {
-	unsigned long *ptr;
+	unsigned long *curr_word;
 	unsigned long *start = PTR_ALIGN(_start, BYTES_PER_POINTER);
 	unsigned long *end = _end - (BYTES_PER_POINTER - 1);
-	unsigned long flags;
-	unsigned long untagged_ptr;
-
-	raw_spin_lock_irqsave(&kmemleak_lock, flags);
-	for (ptr = start; ptr < end; ptr++) {
-		struct kmemleak_object *object;
-		unsigned long pointer;
-		unsigned long excess_ref;
-
-		if (scan_should_stop())
-			break;
-
-		kasan_disable_current();
-		pointer = *(unsigned long *)kasan_reset_tag((void *)ptr);
-		kasan_enable_current();
-
-		untagged_ptr = (unsigned long)kasan_reset_tag((void *)pointer);
-		if (untagged_ptr < min_addr || untagged_ptr >= max_addr)
+	
+	for (curr_word = start; curr_word < end; curr_word++) {
+		unsigned long pointer = *curr_word;
+		struct page* page;
+		struct safeslab_slab *slab;
+		unsigned char pg_idx, slab_order, dom;
+
+		// check if it points into the cloned direct map at all
+		if ((pointer < (page_offset_base + DM_SIZE)) || (pointer >= (page_offset_base + (DM_SIZE * 16))))
 			continue;
-
-		/*
-		 * No need for get_object() here since we hold kmemleak_lock.
-		 * object->use_count cannot be dropped to 0 while the object
-		 * is still present in object_tree_root and object_list
-		 * (with updates protected by kmemleak_lock).
-		 */
-		object = lookup_object(pointer, 1);
-		if (!object)
+		
+		page = virt_to_page((void *)pointer);
+			
+		// fale positives might yield pages outside of RAM, need to check for that
+		if (!pfn_valid(page_to_pfn(page)))
 			continue;
-		if (object == scanned)
-			/* self referenced, ignore */
+	
+		// get order-aligned slab	
+		page = pfn_to_page(page_to_pfn(page) & (~((1 << SAFEBUDDY_ORDER_EXTEND) - 1)));
+
+		// check if page has ever been allocated as part of a slab before
+		if (page->safeslab == NULL)
 			continue;
 
-		/*
-		 * Avoid the lockdep recursive warning on object->lock being
-		 * previously acquired in scan_object(). These locks are
-		 * enclosed by scan_mutex.
-		 */
-		raw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
-		/* only pass surplus references (object already gray) */
-		if (color_gray(object)) {
-			excess_ref = object->excess_ref;
-			/* no need for update_refs() if object already gray */
-		} else {
-			excess_ref = 0;
-			update_refs(object);
+		// get the head of the slab
+		slab = (struct safeslab_slab *)compound_head(page);
+		
+		// get TMD	
+		dom = (pointer - page_offset_base) / DM_SIZE;
+		
+		// if it points into an allocated safeslab, and that slab is in the same TMD, 
+		// just skip it, as the quarantine will take care we don't reuse this page until 
+		// the next scan; essentially means the address is legitimately in use
+		if (likely(PageSafeslab((struct page *)slab))) {
+			if (likely(slab->dm_idx == dom))
+				continue;
 		}
-		raw_spin_unlock(&object->lock);
 
-		if (excess_ref) {
-			object = lookup_object(excess_ref, 0);
-			if (!object)
-				continue;
-			if (object == scanned)
-				/* circular reference, ignore */
+		// check if page was used as part of slab since last scan
+		if ((page->safeslab->slab[dom].sid != check_sid) 
+			&& (page->safeslab->unmapped_sid[dom] != check_sid))
+			continue;
+	
+		// get idx this page had last time it was part of slab
+		pg_idx = page->safeslab->slab[dom].idx;
+		
+		// statistics	
+		inc_freq_pfn(page_to_pfn(page));
+
+		// get head of that slab
+		page = page - pg_idx;
+	
+		// get slab order	
+		slab_order = page->safeslab->slab[dom].order;
+
+		// walk over all pages in that slab and mark for unmapping
+		for (int i = 0; i < (1 << slab_order); i++, page += (1 << SAFEBUDDY_ORDER_EXTEND)) {
+
+			// skip if we've already marked this page for this alias in this scan
+			if (page->safeslab->unmapped_sid[dom] == set_sid)
 				continue;
-			raw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
-			update_refs(object);
-			raw_spin_unlock(&object->lock);
+			
+			// mark the page's alias for unmapping
+			page->safeslab->unmapped_sid[dom] = set_sid;
+	
+			// check if we've already added this page in the PFNs stack
+			if (page->safeslab->added_sid != set_sid) {
+				struct safeslab_pfn_arr *pfn_arr = &unmapped_pfns_ptr[unmapped_pfns_len - 1];
+				
+				pfn_arr->pfns[pfn_arr->len] = page_to_pfn(page);
+				pfn_arr->len++;
+				if (pfn_arr->len == 256) {
+					unmapped_pfns_ptr[unmapped_pfns_len].pfns = __get_free_pages(GFP_KERNEL, 1);
+					unmapped_pfns_ptr[unmapped_pfns_len].len = 0;
+					unmapped_pfns_len++;
+					BUG_ON(unmapped_pfns_len > 31);
+				}
+				
+				page->safeslab->added_sid = set_sid;
+			}
+		}
+
+		kmemleak_printk("potential dangling pointer %llx\n", pointer);
+	}
+}
+
+/*
+ * Scan a safeslab slab for dangling pointers; only touches allocated objects. 
+ */
+static inline void scan_block_safeslab(struct safeslab_slab *slab)
+{
+	struct kmem_cache* slab_cache;
+	unsigned long *bitmap;
+	void *curr_obj;
+
+	slab_cache = slab->slab_cache;
+	bitmap = slab->bitmap;
+
+	curr_obj = slab_address((struct slab *)slab);	
+	for (int i = 0; i < slab->inuse; i++) {
+		// only scan allocated objects
+		if (!test_bit(i, bitmap)) {
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+			if ((slab_cache->flags & SLAB_KMALLOC) && safeslab_get_extended_bitmap_length(slab_cache, slab, i) != 0 ) {
+				int offset = safeslab_get_extended_bitmap_offset(slab_cache, slab, i);
+				int length = safeslab_get_extended_bitmap_length(slab_cache, slab, i);
+				scan_block(curr_obj, curr_obj + offset);
+				scan_block(curr_obj + offset + length, curr_obj + slab_cache->size);
+			} else if (slab_cache->usersize != 0) {
+				scan_block(curr_obj, curr_obj + slab_cache->useroffset);
+				scan_block(curr_obj + slab_cache->useroffset + slab_cache->usersize, curr_obj + slab_cache->size);
+			} else
+#endif
+				scan_block(curr_obj, curr_obj + slab_cache->size);
 		}
+		curr_obj += slab_cache->size;	
 	}
-	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 }
 
 /*
@@ -1377,185 +616,38 @@ static void scan_large_block(void *start, void *end)
 
 	while (start < end) {
 		next = min(start + MAX_SCAN_SIZE, end);
-		scan_block(start, next, NULL);
+		scan_block(start, next);
 		start = next;
 		cond_resched();
 	}
 }
 #endif
 
-/*
- * Scan a memory block corresponding to a kmemleak_object. A condition is
- * that object->use_count >= 1.
- */
-static void scan_object(struct kmemleak_object *object)
-{
-	struct kmemleak_scan_area *area;
-	unsigned long flags;
-	void *obj_ptr;
-
-	/*
-	 * Once the object->lock is acquired, the corresponding memory block
-	 * cannot be freed (the same lock is acquired in delete_object).
-	 */
-	raw_spin_lock_irqsave(&object->lock, flags);
-	if (object->flags & OBJECT_NO_SCAN)
-		goto out;
-	if (!(object->flags & OBJECT_ALLOCATED))
-		/* already freed object */
-		goto out;
-
-	obj_ptr = object->flags & OBJECT_PHYS ?
-		  __va((phys_addr_t)object->pointer) :
-		  (void *)object->pointer;
-
-	if (hlist_empty(&object->area_list) ||
-	    object->flags & OBJECT_FULL_SCAN) {
-		void *start = obj_ptr;
-		void *end = obj_ptr + object->size;
-		void *next;
-
-		do {
-			next = min(start + MAX_SCAN_SIZE, end);
-			scan_block(start, next, object);
-
-			start = next;
-			if (start >= end)
-				break;
-
-			raw_spin_unlock_irqrestore(&object->lock, flags);
-			cond_resched();
-			raw_spin_lock_irqsave(&object->lock, flags);
-		} while (object->flags & OBJECT_ALLOCATED);
-	} else
-		hlist_for_each_entry(area, &object->area_list, node)
-			scan_block((void *)area->start,
-				   (void *)(area->start + area->size),
-				   object);
-out:
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-}
-
-/*
- * Scan the objects already referenced (gray objects). More objects will be
- * referenced and, if there are no memory leaks, all the objects are scanned.
- */
-static void scan_gray_list(void)
-{
-	struct kmemleak_object *object, *tmp;
-
-	/*
-	 * The list traversal is safe for both tail additions and removals
-	 * from inside the loop. The kmemleak objects cannot be freed from
-	 * outside the loop because their use_count was incremented.
-	 */
-	object = list_entry(gray_list.next, typeof(*object), gray_list);
-	while (&object->gray_list != &gray_list) {
-		cond_resched();
-
-		/* may add new objects to the list */
-		if (!scan_should_stop())
-			scan_object(object);
-
-		tmp = list_entry(object->gray_list.next, typeof(*object),
-				 gray_list);
-
-		/* remove the object from the list and release it */
-		list_del(&object->gray_list);
-		put_object(object);
-
-		object = tmp;
-	}
-	WARN_ON(!list_empty(&gray_list));
-}
-
-/*
- * Conditionally call resched() in an object iteration loop while making sure
- * that the given object won't go away without RCU read lock by performing a
- * get_object() if !pinned.
- *
- * Return: false if can't do a cond_resched() due to get_object() failure
- *	   true otherwise
- */
-static bool kmemleak_cond_resched(struct kmemleak_object *object, bool pinned)
-{
-	if (!pinned && !get_object(object))
-		return false;
-
-	rcu_read_unlock();
-	cond_resched();
-	rcu_read_lock();
-	if (!pinned)
-		put_object(object);
-	return true;
-}
+static bool roai_scanned = false;
 
 /*
  * Scan data sections and all the referenced memory blocks allocated via the
  * kernel's standard allocators. This function must be called with the
  * scan_mutex held.
  */
-static void kmemleak_scan(void)
+static noinline void kmemleak_scan(void)
 {
-	struct kmemleak_object *object;
 	struct zone *zone;
 	int __maybe_unused i;
-	int new_leaks = 0;
-	int loop_cnt = 0;
-
+	unsigned long page_cnt = 0;
+	
 	jiffies_last_scan = jiffies;
 
-	/* prepare the kmemleak_object's */
-	rcu_read_lock();
-	list_for_each_entry_rcu(object, &object_list, object_list) {
-		bool obj_pinned = false;
-
-		raw_spin_lock_irq(&object->lock);
-#ifdef DEBUG
-		/*
-		 * With a few exceptions there should be a maximum of
-		 * 1 reference to any object at this point.
-		 */
-		if (atomic_read(&object->use_count) > 1) {
-			pr_debug("object->use_count = %d\n",
-				 atomic_read(&object->use_count));
-			dump_object_info(object);
-		}
-#endif
-
-		/* ignore objects outside lowmem (paint them black) */
-		if ((object->flags & OBJECT_PHYS) &&
-		   !(object->flags & OBJECT_NO_SCAN)) {
-			unsigned long phys = object->pointer;
-
-			if (PHYS_PFN(phys) < min_low_pfn ||
-			    PHYS_PFN(phys + object->size) >= max_low_pfn)
-				__paint_it(object, KMEMLEAK_BLACK);
-		}
-
-		/* reset the reference count (whiten the object) */
-		object->count = 0;
-		if (color_gray(object) && get_object(object)) {
-			list_add_tail(&object->gray_list, &gray_list);
-			obj_pinned = true;
-		}
-
-		raw_spin_unlock_irq(&object->lock);
-
-		/*
-		 * Do a cond_resched() every 64k objects to avoid soft lockup.
-		 */
-		if (!(++loop_cnt & 0xffff) &&
-		    !kmemleak_cond_resched(object, obj_pinned))
-			loop_cnt--; /* Try again on next object */
-	}
-	rcu_read_unlock();
+	stop_the_world();
 
 #ifdef CONFIG_SMP
 	/* per-cpu sections scanning */
-	for_each_possible_cpu(i)
+	for_each_possible_cpu(i) {
 		scan_large_block(__per_cpu_start + per_cpu_offset(i),
-				 __per_cpu_end + per_cpu_offset(i));
+				__per_cpu_end + per_cpu_offset(i));
+		//if (ptr_cnt)
+		//	kmemleak_prio_printk("Found %lld pointers in cpu area %d\n", ptr_cnt, i);
+	}
 #endif
 
 	/*
@@ -1567,24 +659,48 @@ static void kmemleak_scan(void)
 		unsigned long end_pfn = zone_end_pfn(zone);
 		unsigned long pfn;
 
-		for (pfn = start_pfn; pfn < end_pfn; pfn++) {
+		kmemleak_printk("Start pfn %lu, end pfn %lu\n", start_pfn, end_pfn);
+		
+		pfn = start_pfn;
+		while (pfn < end_pfn) {
+			unsigned int pfn_inc = 1;
 			struct page *page = pfn_to_online_page(pfn);
 
 			if (!page)
-				continue;
+				goto cont;
 
 			/* only scan pages belonging to this zone */
 			if (page_zone(page) != zone)
-				continue;
+				goto cont;
+			
 			/* only scan if page is in use */
+			/* KAI: that is not set for the intermediate pages in a buddy! */
 			if (page_count(page) == 0)
-				continue;
-			scan_block(page, page + 1, NULL);
+				goto cont;
+
+			if (PageSafeslab(page)) {
+				int order = folio_order((struct folio *)page);
+				
+				scan_block_safeslab((struct safeslab_slab *)page);
+				
+				page_cnt += (1 << order);
+				pfn_inc = 1 << order;
+			}
+			
 			if (!(pfn & 63))
 				cond_resched();
+
+cont:
+			pfn += pfn_inc;
 		}
+		
 	}
 	put_online_mems();
+		
+	kmemleak_printk("Scanned %li zone pages\n", page_cnt);
+	
+	//if (ptr_cnt_zones)
+	//	kmemleak_prio_printk("Found %lld pointers in slabs\n", ptr_cnt_zones);
 
 	/*
 	 * Scanning the task stacks (may introduce false negatives).
@@ -1592,143 +708,107 @@ static void kmemleak_scan(void)
 	if (kmemleak_stack_scan) {
 		struct task_struct *p, *g;
 
-		rcu_read_lock();
+		//rcu_read_lock();
 		for_each_process_thread(g, p) {
 			void *stack = try_get_task_stack(p);
 			if (stack) {
-				scan_block(stack, stack + THREAD_SIZE, NULL);
+				scan_block(stack, stack + THREAD_SIZE);
 				put_task_stack(p);
+				
 			}
 		}
-		rcu_read_unlock();
-	}
-
-	/*
-	 * Scan the objects already referenced from the sections scanned
-	 * above.
-	 */
-	scan_gray_list();
-
-	/*
-	 * Check for new or unreferenced objects modified since the previous
-	 * scan and color them gray until the next scan.
-	 */
-	rcu_read_lock();
-	loop_cnt = 0;
-	list_for_each_entry_rcu(object, &object_list, object_list) {
-		/*
-		 * Do a cond_resched() every 64k objects to avoid soft lockup.
-		 */
-		if (!(++loop_cnt & 0xffff) &&
-		    !kmemleak_cond_resched(object, false))
-			loop_cnt--;	/* Try again on next object */
-
-		/*
-		 * This is racy but we can save the overhead of lock/unlock
-		 * calls. The missed objects, if any, should be caught in
-		 * the next scan.
-		 */
-		if (!color_white(object))
-			continue;
-		raw_spin_lock_irq(&object->lock);
-		if (color_white(object) && (object->flags & OBJECT_ALLOCATED)
-		    && update_checksum(object) && get_object(object)) {
-			/* color it gray temporarily */
-			object->count = object->min_count;
-			list_add_tail(&object->gray_list, &gray_list);
-		}
-		raw_spin_unlock_irq(&object->lock);
+		//rcu_read_unlock();
+		//if (ptr_cnt)
+		//	kmemleak_prio_printk("Found %lld pointers on stacks\n", ptr_cnt);
+	}
+
+	scan_large_block((void *)_sdata, (void *) _edata);
+	scan_large_block((void *) __bss_start, (void *) __bss_stop);
+	if (unlikely(roai_scanned) && (&__start_ro_after_init < &_sdata || &__end_ro_after_init > &_edata)) {
+		scan_large_block((void *) __start_ro_after_init, (void *) __end_ro_after_init);
+		roai_scanned = true;
+	}
+	
+	//if (ptr_cnt_data)
+	//	kmemleak_prio_printk("Found %lld pointers in data section\n", ptr_cnt_data);
+	//if (ptr_cnt_bss)
+	//	kmemleak_prio_printk("Found %lld pointers in bss section\n", ptr_cnt_bss);
+	//if (ptr_cnt_roai)
+	//	kmemleak_prio_printk("Found %lld pointers in roai section\n", ptr_cnt_roai);
+
+	safeslab_process_pfns(unmapped_pfns_ptr, unmapped_pfns_len, false);
+	
+	safeslab_process_pfns(tomap_pfns_ptr, tomap_pfns_len, true);
+
+	kmemleak_prio_printk("%d %-5d remapped and unmapped virtual pages in scan # %u\n", vas_remapped, vas_unmapped, set_sid);
+	vas_remapped = 0;
+	vas_unmapped = 0;
+	
+	// TODO: need TLB shootdown here?
+	flush_tlb_local();
+
+	restart_the_world();
+
+	while (tomap_pfns_len > 1) {
+		tomap_pfns_len--;
+		free_pages(tomap_pfns_ptr[tomap_pfns_len].pfns, 1);
+	}
+
+	if (unmapped_pfns_ptr == pfn_arrs[0]) {
+		unmapped_pfns_ptr = pfn_arrs[1];
+		tomap_pfns_ptr = pfn_arrs[0];
+	} else {
+		unmapped_pfns_ptr = pfn_arrs[0];
+		tomap_pfns_ptr = pfn_arrs[1];
 	}
-	rcu_read_unlock();
-
-	/*
-	 * Re-scan the gray list for modified unreferenced objects.
-	 */
-	scan_gray_list();
-
-	/*
-	 * If scanning was stopped do not report any new unreferenced objects.
-	 */
-	if (scan_should_stop())
-		return;
+	tomap_pfns_len = unmapped_pfns_len;
+	unmapped_pfns_len = 1;
+	unmapped_pfns_ptr[0].len = 0;
 
 	/*
 	 * Scanning result reporting.
 	 */
-	rcu_read_lock();
-	loop_cnt = 0;
-	list_for_each_entry_rcu(object, &object_list, object_list) {
-		/*
-		 * Do a cond_resched() every 64k objects to avoid soft lockup.
-		 */
-		if (!(++loop_cnt & 0xffff) &&
-		    !kmemleak_cond_resched(object, false))
-			loop_cnt--;	/* Try again on next object */
-
-		/*
-		 * This is racy but we can save the overhead of lock/unlock
-		 * calls. The missed objects, if any, should be caught in
-		 * the next scan.
-		 */
-		if (!color_white(object))
-			continue;
-		raw_spin_lock_irq(&object->lock);
-		if (unreferenced_object(object) &&
-		    !(object->flags & OBJECT_REPORTED)) {
-			object->flags |= OBJECT_REPORTED;
-
-			if (kmemleak_verbose)
-				print_unreferenced(NULL, object);
 
-			new_leaks++;
-		}
-		raw_spin_unlock_irq(&object->lock);
-	}
-	rcu_read_unlock();
-
-	if (new_leaks) {
-		kmemleak_found_leaks = true;
-
-		pr_info("%d new suspected memory leaks (see /sys/kernel/debug/kmemleak)\n",
-			new_leaks);
-	}
+#ifdef KMEMLEAK_COUNT_DP_FREQ
+	for (int i = 0 ; i < 16 ; i++)
+		printk("Found %lld pointers in domain %i \n", per_domain_count[i], i);
 
+	printk("%lld Found dangling pointers to %lld different PFNs\n", set_sid, last_pfn_idx);
+#endif
+	
+	reset_freq();
 }
 
 /*
- * Thread function performing automatic memory scanning. Unreferenced objects
- * at the end of a memory scan are reported but only the first time.
+ * On-demand kernel memory scanner. 
  */
 static int kmemleak_scan_thread(void *arg)
 {
-	static int first_run = IS_ENABLED(CONFIG_DEBUG_KMEMLEAK_AUTO_SCAN);
-
-	pr_info("Automatic memory scanning thread started\n");
 	set_user_nice(current, 10);
 
-	/*
-	 * Wait before the first scan to allow the system to fully initialize.
-	 */
-	if (first_run) {
-		signed long timeout = msecs_to_jiffies(SECS_FIRST_SCAN * 1000);
-		first_run = 0;
-		while (timeout && !kthread_should_stop())
-			timeout = schedule_timeout_interruptible(timeout);
-	}
-
 	while (!kthread_should_stop()) {
-		signed long timeout = READ_ONCE(jiffies_scan_wait);
+		
+		// park myself and do nothing until someone unparks me
+		kthread_park(scan_thread);
+		while (kthread_should_park())
+			kthread_parkme();
+	
+#ifdef CONFIG_SAFESLAB_TRACE
+		trace_kmemleak_scan(_RET_IP_);
+#endif
+		
+		scanner_started = true;
 
-		mutex_lock(&scan_mutex);
-		kmemleak_scan();
-		mutex_unlock(&scan_mutex);
+		kmemleak_printk("STARTING THE SCAN\n");
 
-		/* wait before the next scan */
-		while (timeout && !kthread_should_stop())
-			timeout = schedule_timeout_interruptible(timeout);
-	}
+		set_sid++;
 
-	pr_info("Automatic memory scanning thread ended\n");
+		kmemleak_scan();
+		
+		check_sid++;
+		
+		kmemleak_printk("SCAN TOOK %lli nsecs\n", post_time - pre_time);
+	}
 
 	return 0;
 }
@@ -1741,11 +821,21 @@ static void start_scan_thread(void)
 {
 	if (scan_thread)
 		return;
+	
 	scan_thread = kthread_run(kmemleak_scan_thread, NULL, "kmemleak");
+	
 	if (IS_ERR(scan_thread)) {
 		pr_warn("Failed to create the scan thread\n");
 		scan_thread = NULL;
 	}
+
+	pfn_arrs[0][0].pfns = __get_free_pages(GFP_KERNEL, 1);
+	pfn_arrs[0][0].len = 0;
+
+	pfn_arrs[1][0].pfns = __get_free_pages(GFP_KERNEL, 1);
+	pfn_arrs[1][0].len = 0;
+	
+	unmapped_pfns_len = 1;
 }
 
 /*
@@ -1759,258 +849,7 @@ static void stop_scan_thread(void)
 	}
 }
 
-/*
- * Iterate over the object_list and return the first valid object at or after
- * the required position with its use_count incremented. The function triggers
- * a memory scanning when the pos argument points to the first position.
- */
-static void *kmemleak_seq_start(struct seq_file *seq, loff_t *pos)
-{
-	struct kmemleak_object *object;
-	loff_t n = *pos;
-	int err;
-
-	err = mutex_lock_interruptible(&scan_mutex);
-	if (err < 0)
-		return ERR_PTR(err);
-
-	rcu_read_lock();
-	list_for_each_entry_rcu(object, &object_list, object_list) {
-		if (n-- > 0)
-			continue;
-		if (get_object(object))
-			goto out;
-	}
-	object = NULL;
-out:
-	return object;
-}
-
-/*
- * Return the next object in the object_list. The function decrements the
- * use_count of the previous object and increases that of the next one.
- */
-static void *kmemleak_seq_next(struct seq_file *seq, void *v, loff_t *pos)
-{
-	struct kmemleak_object *prev_obj = v;
-	struct kmemleak_object *next_obj = NULL;
-	struct kmemleak_object *obj = prev_obj;
-
-	++(*pos);
-
-	list_for_each_entry_continue_rcu(obj, &object_list, object_list) {
-		if (get_object(obj)) {
-			next_obj = obj;
-			break;
-		}
-	}
-
-	put_object(prev_obj);
-	return next_obj;
-}
-
-/*
- * Decrement the use_count of the last object required, if any.
- */
-static void kmemleak_seq_stop(struct seq_file *seq, void *v)
-{
-	if (!IS_ERR(v)) {
-		/*
-		 * kmemleak_seq_start may return ERR_PTR if the scan_mutex
-		 * waiting was interrupted, so only release it if !IS_ERR.
-		 */
-		rcu_read_unlock();
-		mutex_unlock(&scan_mutex);
-		if (v)
-			put_object(v);
-	}
-}
-
-/*
- * Print the information for an unreferenced object to the seq file.
- */
-static int kmemleak_seq_show(struct seq_file *seq, void *v)
-{
-	struct kmemleak_object *object = v;
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	if ((object->flags & OBJECT_REPORTED) && unreferenced_object(object))
-		print_unreferenced(seq, object);
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-	return 0;
-}
-
-static const struct seq_operations kmemleak_seq_ops = {
-	.start = kmemleak_seq_start,
-	.next  = kmemleak_seq_next,
-	.stop  = kmemleak_seq_stop,
-	.show  = kmemleak_seq_show,
-};
-
-static int kmemleak_open(struct inode *inode, struct file *file)
-{
-	return seq_open(file, &kmemleak_seq_ops);
-}
-
-static int dump_str_object_info(const char *str)
-{
-	unsigned long flags;
-	struct kmemleak_object *object;
-	unsigned long addr;
-
-	if (kstrtoul(str, 0, &addr))
-		return -EINVAL;
-	object = find_and_get_object(addr, 0);
-	if (!object) {
-		pr_info("Unknown object at 0x%08lx\n", addr);
-		return -EINVAL;
-	}
-
-	raw_spin_lock_irqsave(&object->lock, flags);
-	dump_object_info(object);
-	raw_spin_unlock_irqrestore(&object->lock, flags);
-
-	put_object(object);
-	return 0;
-}
-
-/*
- * We use grey instead of black to ensure we can do future scans on the same
- * objects. If we did not do future scans these black objects could
- * potentially contain references to newly allocated objects in the future and
- * we'd end up with false positives.
- */
-static void kmemleak_clear(void)
-{
-	struct kmemleak_object *object;
-
-	rcu_read_lock();
-	list_for_each_entry_rcu(object, &object_list, object_list) {
-		raw_spin_lock_irq(&object->lock);
-		if ((object->flags & OBJECT_REPORTED) &&
-		    unreferenced_object(object))
-			__paint_it(object, KMEMLEAK_GREY);
-		raw_spin_unlock_irq(&object->lock);
-	}
-	rcu_read_unlock();
-
-	kmemleak_found_leaks = false;
-}
-
-static void __kmemleak_do_cleanup(void);
-
-/*
- * File write operation to configure kmemleak at run-time. The following
- * commands can be written to the /sys/kernel/debug/kmemleak file:
- *   off	- disable kmemleak (irreversible)
- *   stack=on	- enable the task stacks scanning
- *   stack=off	- disable the tasks stacks scanning
- *   scan=on	- start the automatic memory scanning thread
- *   scan=off	- stop the automatic memory scanning thread
- *   scan=...	- set the automatic memory scanning period in seconds (0 to
- *		  disable it)
- *   scan	- trigger a memory scan
- *   clear	- mark all current reported unreferenced kmemleak objects as
- *		  grey to ignore printing them, or free all kmemleak objects
- *		  if kmemleak has been disabled.
- *   dump=...	- dump information about the object found at the given address
- */
-static ssize_t kmemleak_write(struct file *file, const char __user *user_buf,
-			      size_t size, loff_t *ppos)
-{
-	char buf[64];
-	int buf_size;
-	int ret;
-
-	buf_size = min(size, (sizeof(buf) - 1));
-	if (strncpy_from_user(buf, user_buf, buf_size) < 0)
-		return -EFAULT;
-	buf[buf_size] = 0;
-
-	ret = mutex_lock_interruptible(&scan_mutex);
-	if (ret < 0)
-		return ret;
-
-	if (strncmp(buf, "clear", 5) == 0) {
-		if (kmemleak_enabled)
-			kmemleak_clear();
-		else
-			__kmemleak_do_cleanup();
-		goto out;
-	}
-
-	if (!kmemleak_enabled) {
-		ret = -EPERM;
-		goto out;
-	}
-
-	if (strncmp(buf, "off", 3) == 0)
-		kmemleak_disable();
-	else if (strncmp(buf, "stack=on", 8) == 0)
-		kmemleak_stack_scan = 1;
-	else if (strncmp(buf, "stack=off", 9) == 0)
-		kmemleak_stack_scan = 0;
-	else if (strncmp(buf, "scan=on", 7) == 0)
-		start_scan_thread();
-	else if (strncmp(buf, "scan=off", 8) == 0)
-		stop_scan_thread();
-	else if (strncmp(buf, "scan=", 5) == 0) {
-		unsigned secs;
-		unsigned long msecs;
-
-		ret = kstrtouint(buf + 5, 0, &secs);
-		if (ret < 0)
-			goto out;
-
-		msecs = secs * MSEC_PER_SEC;
-		if (msecs > UINT_MAX)
-			msecs = UINT_MAX;
-
-		stop_scan_thread();
-		if (msecs) {
-			WRITE_ONCE(jiffies_scan_wait, msecs_to_jiffies(msecs));
-			start_scan_thread();
-		}
-	} else if (strncmp(buf, "scan", 4) == 0)
-		kmemleak_scan();
-	else if (strncmp(buf, "dump=", 5) == 0)
-		ret = dump_str_object_info(buf + 5);
-	else
-		ret = -EINVAL;
-
-out:
-	mutex_unlock(&scan_mutex);
-	if (ret < 0)
-		return ret;
-
-	/* ignore the rest of the buffer, only one command at a time */
-	*ppos += size;
-	return size;
-}
-
-static const struct file_operations kmemleak_fops = {
-	.owner		= THIS_MODULE,
-	.open		= kmemleak_open,
-	.read		= seq_read,
-	.write		= kmemleak_write,
-	.llseek		= seq_lseek,
-	.release	= seq_release,
-};
-
-static void __kmemleak_do_cleanup(void)
-{
-	struct kmemleak_object *object, *tmp;
 
-	/*
-	 * Kmemleak has already been disabled, no need for RCU list traversal
-	 * or kmemleak_lock held.
-	 */
-	list_for_each_entry_safe(object, tmp, &object_list, object_list) {
-		__remove_object(object);
-		__delete_object(object);
-	}
-}
 
 /*
  * Stop the memory scanning thread and free the kmemleak internal objects if
@@ -2020,21 +859,6 @@ static void __kmemleak_do_cleanup(void)
 static void kmemleak_do_cleanup(struct work_struct *work)
 {
 	stop_scan_thread();
-
-	mutex_lock(&scan_mutex);
-	/*
-	 * Once it is made sure that kmemleak_scan has stopped, it is safe to no
-	 * longer track object freeing. Ordering of the scan thread stopping and
-	 * the memory accesses below is guaranteed by the kthread_stop()
-	 * function.
-	 */
-	kmemleak_free_enabled = 0;
-	mutex_unlock(&scan_mutex);
-
-	if (!kmemleak_found_leaks)
-		__kmemleak_do_cleanup();
-	else
-		pr_info("Kmemleak disabled without freeing internal data. Reclaim the memory with \"echo clear > /sys/kernel/debug/kmemleak\".\n");
 }
 
 static DECLARE_WORK(cleanup_work, kmemleak_do_cleanup);
@@ -2051,12 +875,16 @@ static void kmemleak_disable(void)
 
 	/* stop any memory operation tracing */
 	kmemleak_enabled = 0;
+	//KAI: FUCK IT... 
+	kmemleak_enabled = 1;
+	printk("KAI: KMEMLEAK_DISABLE called -> IGNORING!\n");
+	return;
 
 	/* check whether it is too early for a kernel thread */
 	if (kmemleak_initialized)
 		schedule_work(&cleanup_work);
 	else
-		kmemleak_free_enabled = 0;
+		kmemleak_free_enabled = 1; //KAI: again!
 
 	pr_info("Kernel memory leak detector disabled\n");
 }
@@ -2085,32 +913,21 @@ early_param("kmemleak", kmemleak_boot_config);
  */
 void __init kmemleak_init(void)
 {
-#ifdef CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF
-	if (!kmemleak_skip_disable) {
-		kmemleak_disable();
-		return;
-	}
-#endif
-
 	if (kmemleak_error)
 		return;
 
 	jiffies_min_age = msecs_to_jiffies(MSECS_MIN_AGE);
 	jiffies_scan_wait = msecs_to_jiffies(SECS_SCAN_WAIT * 1000);
 
-	object_cache = KMEM_CACHE(kmemleak_object, SLAB_NOLEAKTRACE);
-	scan_area_cache = KMEM_CACHE(kmemleak_scan_area, SLAB_NOLEAKTRACE);
-
-	/* register the data/bss sections */
-	create_object((unsigned long)_sdata, _edata - _sdata,
-		      KMEMLEAK_GREY, GFP_ATOMIC);
-	create_object((unsigned long)__bss_start, __bss_stop - __bss_start,
-		      KMEMLEAK_GREY, GFP_ATOMIC);
-	/* only register .data..ro_after_init if not within .data */
-	if (&__start_ro_after_init < &_sdata || &__end_ro_after_init > &_edata)
-		create_object((unsigned long)__start_ro_after_init,
-			      __end_ro_after_init - __start_ro_after_init,
-			      KMEMLEAK_GREY, GFP_ATOMIC);
+	kmemleak_early_initialized = 1;
+}
+
+/*
+* Kmemleak initialization.
+*/
+void kmemleak_unpark(void)
+{
+	kthread_unpark(scan_thread);
 }
 
 /*
@@ -2120,8 +937,6 @@ static int __init kmemleak_late_init(void)
 {
 	kmemleak_initialized = 1;
 
-	debugfs_create_file("kmemleak", 0644, NULL, NULL, &kmemleak_fops);
-
 	if (kmemleak_error) {
 		/*
 		 * Some error occurred and kmemleak was disabled. There is a
@@ -2133,15 +948,13 @@ static int __init kmemleak_late_init(void)
 		return -ENOMEM;
 	}
 
-	if (IS_ENABLED(CONFIG_DEBUG_KMEMLEAK_AUTO_SCAN)) {
-		mutex_lock(&scan_mutex);
-		start_scan_thread();
-		mutex_unlock(&scan_mutex);
-	}
-
-	pr_info("Kernel memory leak detector initialized (mem pool available: %d)\n",
-		mem_pool_free_count);
+	mutex_lock(&scan_mutex);
+	start_scan_thread();
+	mutex_unlock(&scan_mutex);
 
+	if (scan_thread)
+		printk("kmemleak scan thread active\n");
+	
 	return 0;
 }
 late_initcall(kmemleak_late_init);
diff --git a/mm/ksmap.c b/mm/ksmap.c
new file mode 100644
index 000000000000..86df1a793f48
--- /dev/null
+++ b/mm/ksmap.c
@@ -0,0 +1,387 @@
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/irqflags.h>
+#include <linux/ksmap.h>
+#include <linux/page-flags.h>
+#include <linux/mm_types.h>
+
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+
+#include <asm/set_memory.h>
+#include <asm/pgalloc.h>
+
+#include <linux/bit_spinlock.h>
+
+#ifdef CONFIG_KSMAP
+
+#ifdef CONFIG_KSMAP_DEBUG
+#define ksmap_printk(fmt, ...) printk("[kSMAP] %s" #fmt, __FUNCTION__, ##__VA_ARGS__)
+#else
+#define ksmap_printk(fmt, ...) (void)fmt
+#endif
+
+#define KSMAP_ORDER_2MB 9
+#define KSMAP_MAX_ORDER 9
+#define KSMAP_MIN_NR_2MB_PAGES 2
+#define KSMAP_ISOLATED 1
+#define KSMAP_UNISOLATED 0
+
+struct list_head ksmap_freelists[KSMAP_MAX_ORDER + 1];
+unsigned int chunks = 0;
+
+spinlock_t ksmap_spinlock;
+
+static __always_inline void ksmap_populate_pte(pte_t *ptep, unsigned long address, int level)
+{
+	struct page *page = virt_to_page(address);
+	unsigned long offset, pfn = page_to_pfn(page), increment;
+
+	if (level == PG_LEVEL_1G)
+		increment = 512;
+	else
+		increment = 1;
+
+
+	for (offset = 0; offset < PTRS_PER_PTE; offset++, pfn += increment) {
+		set_pte(ptep++, pfn_pte(pfn, __pgprot(__PAGE_KERNEL)));
+	}
+}
+
+void ksmap_split_huge_page(struct page *page, pte_t *ptep, int level)
+{
+	pte_t *new_ptep;
+
+	if (level == PG_LEVEL_4K)
+		return;
+
+	if (!(new_ptep = pte_alloc_one_kernel(&init_mm)))
+		return;
+
+	if (level == PG_LEVEL_2M) {
+		pmd_t *pmdp;
+		unsigned long address;
+		
+		pmdp = (pmd_t *)ptep;
+		address = pmd_page_vaddr(*pmdp);
+	
+		ksmap_printk("Will split 2M address %llx", address);
+		
+		ksmap_populate_pte(new_ptep, address, level);
+
+		smp_wmb();
+
+		spin_lock(&init_mm.page_table_lock);
+
+		set_pmd(pmdp, __pmd(__pa(new_ptep) | _KERNPG_TABLE | _PAGE_NX));
+		flush_tlb_all();
+
+		spin_unlock(&init_mm.page_table_lock);
+	} else if (level == PG_LEVEL_1G) {
+		pr_err("Splitting 1GB pages not supported yet");
+		BUG_ON(true);
+	} 
+}
+
+static pte_t *ksmap_fetch_ptep(struct page *page, unsigned int *level)
+{
+	unsigned long addr;
+	pte_t *ptep;
+
+	addr = (unsigned long)page_address(page);
+	ptep = lookup_address(addr, level);
+	
+	ksmap_printk("Found PTE at level %u for page %px: %016lx\n", *level, page_address(page), ptep->pte);
+	
+	return ptep;
+}
+
+static void __ksmap_isolate_page(struct page *page, bool split)
+{
+	unsigned int level;
+	pte_t *ptep = ksmap_fetch_ptep(page, &level);
+
+	if (split)
+		ksmap_split_huge_page(page, ptep, level);
+	
+	set_pte(ptep, pte_set_flags(*ptep, _PAGE_USER));
+	
+	//flush_tlb_all();
+}
+
+static void __ksmap_deisolate_page(struct page *page)
+{
+	unsigned int level;
+	pte_t *ptep = ksmap_fetch_ptep(page, &level);
+
+	set_pte(ptep, pte_clear_flags(*ptep, _PAGE_USER));
+	
+	//flush_tlb_all();
+}
+
+static inline unsigned int ksmap_buddy_order(struct page *buddy)
+{
+	return page_private(buddy);
+}
+
+static inline bool page_is_ksmap_buddy(struct page *buddy, unsigned int order)
+{
+	if (!PageKsmap(buddy))
+		return false;
+
+	if (ksmap_buddy_order(buddy) != order)
+		return false;
+
+	return true;
+}
+
+static inline void ksmap_set_buddy_order(struct page *buddy, unsigned int order)
+{
+	set_page_private(buddy, order);
+}
+
+static void __ksmap_insert_to_freelist(struct page *buddy, unsigned int order)
+{
+	__SetPageKsmap(buddy);
+	ksmap_set_buddy_order(buddy, order);
+	list_add(&buddy->lru, &ksmap_freelists[order]);
+}
+
+static void __ksmap_remove_from_freelist(struct page *buddy)
+{
+	__ClearPageKsmap(buddy);
+	ksmap_set_buddy_order(buddy, -1);
+	list_del(&buddy->lru);
+}
+
+static void __ksmap_isolate_struct_page(struct page *page, uint64_t order)
+{
+	uint64_t obj_count = 1 << order;
+	uint64_t mem_page_count = (sizeof(struct page) * obj_count) / PAGE_SIZE;
+	struct page *page_page = virt_to_page(page);
+	int rc;
+	
+	ksmap_printk("sizeof(struct page) %lld, obj_count %lld, mem_page_count %lld, struct page %llx, struct page page %llx", sizeof(struct page), obj_count, mem_page_count, page, page_page);
+
+	if ((rc = split_huge_page(page_page))) {
+		pr_err("[kSMAP] %s Split huge pages failed with %d", __FUNCTION__, rc);
+		BUG_ON(true);
+	}
+
+	for (; mem_page_count; mem_page_count--) {
+		__ksmap_isolate_page(page_page, true);
+		ksmap_printk("Done isolating page of struct pages %llx", page_address(page_page));
+		page_page++;
+	}
+}
+
+static void __ksmap_increase_memory(void)
+{
+	struct page *new_mem;
+	
+	ksmap_printk("Increasing memory by 2 MB\n");
+	
+	new_mem = alloc_pages(GFP_KERNEL | __GFP_ZERO, KSMAP_ORDER_2MB);
+	if (!new_mem) {
+		pr_err("Couldn't allocate pages");
+		BUG_ON(true);
+	}
+
+	__ksmap_insert_to_freelist(new_mem, KSMAP_ORDER_2MB);
+	chunks++;
+
+	ksmap_printk("New order 9 buddy at 0x%lx. kSMAP now holds %d pages.\n", (unsigned long)page_to_virt(new_mem), chunks);
+	
+	__ksmap_isolate_page(new_mem, false);
+	ksmap_printk("Isolated VA at 0x%lx\n", (unsigned long)page_to_virt(new_mem));
+
+	__ksmap_isolate_struct_page(new_mem, KSMAP_ORDER_2MB);
+}
+
+void __init ksmap_init(void)
+{
+	int i;
+
+	ksmap_printk("Initializing\n");
+
+	spin_lock_init(&ksmap_spinlock);
+
+	for (i = 0; i <= KSMAP_MAX_ORDER; i++)
+		INIT_LIST_HEAD(&ksmap_freelists[i]);
+
+	__ksmap_increase_memory();
+}
+
+static inline unsigned long
+__ksmap_find_buddy_pfn(unsigned long page_pfn, unsigned int order)
+{
+	return page_pfn ^ (1 << order);
+}
+
+static inline unsigned long
+__ksmap_find_primary_buddy(unsigned long page_pfn, unsigned int order)
+{
+	return page_pfn & ~(1 << order);
+}
+
+static struct page *__ksmap_get_pages_from_freelist(unsigned int order)
+{
+	struct page *page;
+	
+	ksmap_printk("Called with order %d\n", order);
+	
+	if (!list_empty(&ksmap_freelists[order]))
+	{
+		page = list_first_entry(&ksmap_freelists[order], struct page, lru);
+	
+		__ksmap_remove_from_freelist(page);	
+		
+		ksmap_printk("Found buddy in freelist at 0x%lx\n", (unsigned long)page_to_virt(page));
+		return page;
+	}
+	
+	ksmap_printk("Freelist empty\n");
+	
+	return NULL;
+}
+
+static struct page *__ksmap_split_higher_order(unsigned int order)
+{
+	unsigned int cur = order + 1;
+	struct page *fst_buddy = NULL, *snd_buddy = NULL;
+
+	ksmap_printk("Need order %d\n", order);
+
+	while (cur <= KSMAP_MAX_ORDER) {
+		fst_buddy = __ksmap_get_pages_from_freelist(cur);
+		if (fst_buddy) {
+			ksmap_printk("Found entry for order %d\n", cur);
+			goto split;
+		}
+
+		cur++;
+		ksmap_printk("[kSMAP] __ksmap_split_higher_order: no freelist entry for order %d\n", cur);
+	}
+
+	ksmap_printk("Out of memory, allocating additional 2 MB\n");
+	
+	__ksmap_increase_memory();
+	
+	fst_buddy = __ksmap_get_pages_from_freelist(KSMAP_ORDER_2MB);
+	
+	cur--; // cur == 9 after this line
+	
+	// since the loop treats every order as if it had a buddy but oder 9 does not, we have to perform
+	// this check once in case order == 9
+	if (cur == order) {
+		ksmap_printk("Buddy for order %d found: 0x%lx\n", cur, (unsigned long)page_to_virt(fst_buddy));
+		return fst_buddy;
+	}
+	
+split:
+	ksmap_printk("start splitting found buddy until required order is found\n");
+	ksmap_printk("first buddy at 0x%lx with order %d\n", (unsigned long)page_to_virt(fst_buddy), cur);
+	
+	while (cur >= 0) { // order must be >= 0, cur is always >= 1 at this point
+		
+		cur--;
+		
+		snd_buddy = pfn_to_page(__ksmap_find_buddy_pfn(page_to_pfn(fst_buddy), cur));
+		
+		ksmap_printk("split at 0x%lx with order %d\n", (unsigned long)page_to_virt(snd_buddy), cur);
+		
+		__ksmap_insert_to_freelist(snd_buddy, cur);
+
+		if (cur == order) {
+			ksmap_printk("buddy for order %d found: 0x%lx\n", cur, (unsigned long)page_to_virt(fst_buddy));
+			return fst_buddy;
+		}
+	}
+	
+	return NULL;
+}
+
+/*
+	Function that returns 2^order pages which are marked as user space pages and can
+	only be accessed if smap protection is temporarily turned off.
+	Returns struct page ptr if successful and -ENOMEM if something went wrong
+*/
+struct page *ksmap_get_isolated_pages(unsigned int order)
+{
+	struct page *page;
+	
+	ksmap_printk("called for order %d\n", order);
+		
+	spin_lock(&ksmap_spinlock);
+
+	page = __ksmap_get_pages_from_freelist(order);
+	if (!page) {
+		page = __ksmap_split_higher_order(order);
+		ksmap_printk("split pages\n");
+	} else {
+		ksmap_printk("page received from freelist\n");
+	}
+	
+	spin_unlock(&ksmap_spinlock);
+
+	return page;
+}
+EXPORT_SYMBOL(ksmap_get_isolated_pages);
+
+void ksmap_free_isolated_pages(struct page *page, unsigned int order)
+{
+	struct page *buddy;
+	unsigned long address;
+
+	ksmap_printk("called for buddy at 0x%lx with order %d\n", (unsigned long)page_to_virt(page), order);
+	
+	spin_lock(&ksmap_spinlock);
+	
+	while (order < KSMAP_MAX_ORDER) { // order 9 does not have a buddy
+		
+		buddy = pfn_to_page(__ksmap_find_buddy_pfn(page_to_pfn(page), order));
+		if (!page_is_ksmap_buddy(buddy, order)) { // easiest scenario, buddy is in use, just insert page into freelist
+			ksmap_printk("buddy in use\n");
+			
+			__ksmap_insert_to_freelist(page, order);	
+			spin_unlock(&ksmap_spinlock);
+			
+			return;
+		}
+		
+		// buddy is free
+		__ksmap_remove_from_freelist(buddy);
+		
+		ksmap_printk("deleted unused buddy from its freelist\n");
+		
+		// now neither page nor buddy are in any freelist and can be merged
+		// get the primary buddy for next iteration
+		ksmap_printk("merged buddy at 0x%lx with buddy at 0x%lx for new order %d\n", (unsigned long)page_to_virt(page), (unsigned long)page_to_virt(buddy), order+1);
+		
+		page = pfn_to_page(__ksmap_find_primary_buddy(page_to_pfn(page), order));
+		
+		order++;
+	}
+
+	// we merged all buddies along the way and ended up at order 9
+	if (chunks > KSMAP_MIN_NR_2MB_PAGES) {
+		chunks--;
+		
+		spin_unlock(&ksmap_spinlock);
+		
+		__ksmap_deisolate_page(page);
+		
+		address = (unsigned long)page_address(page);
+		ksmap_printk("Releasing page with start address %lx\n", address);
+		
+		__free_pages(page, KSMAP_ORDER_2MB);
+		
+		ksmap_printk("Released page back to buddy. kSMAP holds %d pages currently.\n", chunks);
+	} else {
+		__ksmap_insert_to_freelist(page, KSMAP_ORDER_2MB);
+		spin_unlock(&ksmap_spinlock);
+	}
+}
+EXPORT_SYMBOL(ksmap_free_isolated_pages);
+
+#endif /* CONFIG_KSMAP */
diff --git a/mm/memblock.c b/mm/memblock.c
index d036c7861310..9ca1e9dd05ae 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -13,7 +13,6 @@
 #include <linux/poison.h>
 #include <linux/pfn.h>
 #include <linux/debugfs.h>
-#include <linux/kmemleak.h>
 #include <linux/seq_file.h>
 #include <linux/memblock.h>
 
@@ -846,7 +845,6 @@ int __init_memblock memblock_phys_free(phys_addr_t base, phys_addr_t size)
 	memblock_dbg("%s: [%pa-%pa] %pS\n", __func__,
 		     &base, &end, (void *)_RET_IP_);
 
-	kmemleak_free_part_phys(base, size);
 	return memblock_remove_range(&memblock.reserved, base, size);
 }
 
@@ -1414,14 +1412,6 @@ phys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,
 	 * Skip kmemleak for those places like kasan_init() and
 	 * early_pgtable_alloc() due to high volume.
 	 */
-	if (end != MEMBLOCK_ALLOC_NOLEAKTRACE)
-		/*
-		 * Memblock allocated blocks are never reported as
-		 * leaks. This is because many of these blocks are
-		 * only referred via the physical address which is
-		 * not looked up by kmemleak.
-		 */
-		kmemleak_alloc_phys(found, size, 0);
 
 	return found;
 }
@@ -1635,7 +1625,6 @@ void __init memblock_free_late(phys_addr_t base, phys_addr_t size)
 	end = base + size - 1;
 	memblock_dbg("%s: [%pa-%pa] %pS\n",
 		     __func__, &base, &end, (void *)_RET_IP_);
-	kmemleak_free_part_phys(base, size);
 	cursor = PFN_UP(base);
 	end = PFN_DOWN(base + size);
 
@@ -2076,6 +2065,28 @@ static void __init memmap_init_reserved_pages(void)
 	}
 }
 
+#define NR_DMS 16
+void safeslab_test_cloned_dms(void)
+{
+	struct page *p = alloc_page(GFP_KERNEL);
+	unsigned long *dm0 = (unsigned long *)page_address(p);
+	*dm0 = 0x42;
+	barrier();
+	printk("Sanity test for %d cloned DMs\n", NR_DMS);
+	//printk("dm[0] vaddr = %llx pa = %llx\n", dm0, __pa(dm0));
+	for (int i = 1; i < NR_DMS; i++) {
+		unsigned long *dmi = page_address_dm(p, i);
+		BUG_ON(*dmi != 0x42); 
+		BUG_ON(__pa(dmi) != __pa(dm0));
+		BUG_ON(slow_virt_to_phys(dmi) != __pa(dmi));
+		//printk("dm[%d] vaddr = %llx pa = %llx *vaddr = %llx\n", i, dmi, __pa(dmi));
+		//printk("slow_virt_to_phys(dmi) = %llx", slow_virt_to_phys(dmi));
+		//printk("*dmi = %llx\n", *dmi);
+	}
+}
+
+// calls the buddy freeing routines on available memory managed by memblock, 
+// which builds the buddy lists of physical memory available for allocation
 static unsigned long __init free_low_memory_core_early(void)
 {
 	unsigned long count = 0;
@@ -2084,6 +2095,7 @@ static unsigned long __init free_low_memory_core_early(void)
 
 	memblock_clear_hotplug(0, -1);
 
+	// mark struct page of unusable pages as reserved (eg NOMAP)
 	memmap_init_reserved_pages();
 
 	/*
@@ -2091,9 +2103,21 @@ static unsigned long __init free_low_memory_core_early(void)
 	 *  because in some case like Node0 doesn't have RAM installed
 	 *  low ram will be on Node1
 	 */
+	
+	// these are the DM pages we need to clone, as they are free for use
 	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,
-				NULL)
+				NULL) {
 		count += __free_memory_core(start, end);
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_PK
+		safeslab_clone_memory_mapping(start, end, PAGE_KERNEL_PKEY);
+#else
+		safeslab_clone_memory_mapping(start, end, PAGE_KERNEL);
+#endif
+#endif
+	}
+	
+	safeslab_test_cloned_dms();
 
 	return count;
 }
@@ -2128,9 +2152,12 @@ void __init memblock_free_all(void)
 {
 	unsigned long pages;
 
+	// just returning when CONFIG_SPARSEMEM_VMEMMAP=y 
 	free_unused_memmap();
+	// just initializes fields in node mem control structs
 	reset_all_zones_managed_pages();
 
+	// builds freelists for buddy
 	pages = free_low_memory_core_early();
 	totalram_pages_add(pages);
 }
diff --git a/mm/mempool.c b/mm/mempool.c
index 734bcf5afbb7..0cd637859fc3 100644
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@ -409,7 +409,6 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 		 * Update the allocation stack trace as this is more useful
 		 * for debugging.
 		 */
-		kmemleak_update_trace(element);
 		return element;
 	}
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 3bb3484563ed..8b4fa5591b6b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -53,7 +53,6 @@
 #include <linux/fault-inject.h>
 #include <linux/page-isolation.h>
 #include <linux/debugobjects.h>
-#include <linux/kmemleak.h>
 #include <linux/compaction.h>
 #include <trace/events/kmem.h>
 #include <trace/events/oom.h>
@@ -2463,11 +2462,16 @@ static inline bool should_skip_init(gfp_t flags)
 	return (flags & __GFP_SKIP_ZERO);
 }
 
+static inline bool safeslab_should_skip_init(gfp_t flags)
+{
+	return (flags & __GFP_SAFESLAB_SKIP_ZERO);
+}
+
 inline void post_alloc_hook(struct page *page, unsigned int order,
 				gfp_t gfp_flags)
 {
 	bool init = !want_init_on_free() && want_init_on_alloc(gfp_flags) &&
-			!should_skip_init(gfp_flags);
+			!should_skip_init(gfp_flags) && !safeslab_should_skip_init(gfp_flags);
 	bool init_tags = init && (gfp_flags & __GFP_ZEROTAGS);
 	int i;
 
@@ -9134,7 +9138,6 @@ void *__init alloc_large_system_hash(const char *tablename,
 			 * alloc_pages_exact() automatically does
 			 */
 			table = alloc_pages_exact(size, gfp_flags);
-			kmemleak_alloc(table, size, 1, gfp_flags);
 		}
 	} while (!table && size > PAGE_SIZE && --log2qty);
 
diff --git a/mm/page_ext.c b/mm/page_ext.c
index 4ee522fd381c..8f67f6173582 100644
--- a/mm/page_ext.c
+++ b/mm/page_ext.c
@@ -5,7 +5,6 @@
 #include <linux/page_ext.h>
 #include <linux/memory.h>
 #include <linux/vmalloc.h>
-#include <linux/kmemleak.h>
 #include <linux/page_owner.h>
 #include <linux/page_idle.h>
 #include <linux/page_table_check.h>
@@ -293,7 +292,6 @@ static void *__meminit alloc_page_ext(size_t size, int nid)
 
 	addr = alloc_pages_exact_nid(nid, size, flags);
 	if (addr) {
-		kmemleak_alloc(addr, size, 1, flags);
 		return addr;
 	}
 
@@ -321,7 +319,6 @@ static int __meminit init_section_page_ext(unsigned long pfn, int nid)
 	 * and it does not point to the memory block allocated above,
 	 * causing kmemleak false positives.
 	 */
-	kmemleak_not_leak(base);
 
 	if (!base) {
 		pr_err("page ext allocation failure\n");
@@ -349,7 +346,6 @@ static void free_page_ext(void *addr)
 		table_size = page_ext_size * PAGES_PER_SECTION;
 
 		BUG_ON(PageReserved(page));
-		kmemleak_free(addr);
 		free_pages_exact(addr, table_size);
 	}
 }
diff --git a/mm/percpu.c b/mm/percpu.c
index acd78da0493b..c5e40896013b 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -83,7 +83,6 @@
 #include <linux/spinlock.h>
 #include <linux/vmalloc.h>
 #include <linux/workqueue.h>
-#include <linux/kmemleak.h>
 #include <linux/sched.h>
 #include <linux/sched/mm.h>
 #include <linux/memcontrol.h>
@@ -1875,7 +1874,6 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 		memset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);
 
 	ptr = __addr_to_pcpu_ptr(chunk->base_addr + off);
-	kmemleak_alloc_percpu(ptr, size, gfp);
 
 	trace_percpu_alloc_percpu(_RET_IP_, reserved, is_atomic, size, align,
 				  chunk->base_addr, off, ptr,
@@ -2262,7 +2260,6 @@ void free_percpu(void __percpu *ptr)
 	if (!ptr)
 		return;
 
-	kmemleak_free_percpu(ptr);
 
 	addr = __pcpu_ptr_to_addr(ptr);
 
@@ -3095,8 +3092,6 @@ int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
 			rc = -ENOMEM;
 			goto out_free_areas;
 		}
-		/* kmemleak tracks the percpu allocations separately */
-		kmemleak_ignore_phys(__pa(ptr));
 		areas[group] = ptr;
 
 		base = min(ptr, base);
@@ -3295,8 +3290,6 @@ int __init pcpu_page_first_chunk(size_t reserved_size, pcpu_fc_cpu_to_node_fn_t
 						psize_str, cpu);
 				goto enomem;
 			}
-			/* kmemleak tracks the percpu allocations separately */
-			kmemleak_ignore_phys(__pa(ptr));
 			pages[j++] = virt_to_page(ptr);
 		}
 	}
@@ -3408,8 +3401,6 @@ void __init setup_per_cpu_areas(void)
 	fc = memblock_alloc_from(unit_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS));
 	if (!ai || !fc)
 		panic("Failed to allocate memory for percpu areas.");
-	/* kmemleak tracks the percpu allocations separately */
-	kmemleak_ignore_phys(__pa(fc));
 
 	ai->dyn_size = unit_size;
 	ai->unit_size = unit_size;
diff --git a/mm/safebuddy.c b/mm/safebuddy.c
new file mode 100644
index 000000000000..65e3a9204401
--- /dev/null
+++ b/mm/safebuddy.c
@@ -0,0 +1,395 @@
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/irqflags.h>
+#include <linux/safebuddy.h>
+#include <linux/page-flags.h>
+#include <linux/mm_types.h>
+#include <linux/mm.h>
+#include "internal.h"
+#include <linux/gfp.h>
+#include <linux/sched/mm.h>
+#include <linux/cpuset.h>
+#include <linux/page_ref.h>
+#include <linux/exactfit.h>
+
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+
+#include <asm/set_memory.h>
+#include <asm/pgalloc.h>
+
+#include <linux/bit_spinlock.h>
+#include "./safeslab.h"
+#include "./slab.h"
+
+#include <linux/kmemleak.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/safeslab_trace.h>
+
+#ifdef CONFIG_SAFEBUDDY_DEBUG
+#define safebuddy_printk(fmt, ...) printk("[s-Buddy] %s " #fmt, __FUNCTION__, ##__VA_ARGS__)
+#else
+#define safebuddy_printk(fmt, ...) (void)fmt
+#endif
+
+#define SAFEBUDDY_MAX_ORDER (PAGE_ALLOC_COSTLY_ORDER + SAFEBUDDY_ORDER_EXTEND)
+
+#define SAFEBUDDY_THRESHOLD_DOM_SCAN 12
+#define SAFEBUDDY_MAX_DOM 15
+
+struct list_head safebuddy_memory[SAFEBUDDY_MAX_ORDER + 1];
+
+struct single_list_head safebuddy_quarantine_a[SAFEBUDDY_MAX_ORDER + 1];
+struct single_list_head safebuddy_quarantine_b[SAFEBUDDY_MAX_ORDER + 1];
+struct single_list_head safebuddy_quarantine_c[SAFEBUDDY_MAX_ORDER + 1];
+//struct single_list_head safebuddy_quarantine_d[SAFEBUDDY_MAX_ORDER + 1];
+//struct single_list_head safebuddy_quarantine_e[SAFEBUDDY_MAX_ORDER + 1];
+struct single_list_head *safebuddy_quarantine = safebuddy_quarantine_a;
+
+spinlock_t quarantine_spinlock;
+
+#define MAX_QUARANTINE_SIZE (1 << 20) /* 2^20*2^12 == 4GB mem */
+unsigned int quarantine_size;
+EXPORT_SYMBOL(quarantine_size);
+
+DEFINE_PER_CPU(unsigned long, safeslab_active_domain);
+EXPORT_PER_CPU_SYMBOL(safeslab_active_domain);
+
+static inline void INIT_SINGLE_LIST_HEAD(struct single_list_head *list)
+{
+	WRITE_ONCE(list->next, list);
+}
+
+static inline int single_list_empty(const struct single_list_head *head)
+{
+	return READ_ONCE(head->next) == head;
+}
+
+static inline void single_list_add(struct single_list_head *new,
+			      struct single_list_head *head)
+{
+	new->next = head->next;
+	WRITE_ONCE(head->next, new);
+}
+
+// ############################## Pageflag operations
+
+/*
+ * The atomic page->_mapcount, starts from -1: so that transitions
+ * both from it and to it can be tracked, using atomic_inc_and_test
+ * and atomic_add_negative(-1).
+ */
+static inline void safebuddy_page_mapcount_reset(struct page *page)
+{
+	atomic_set(&(page)->_mapcount, -1);
+}
+
+static inline void safebuddy_kill_compound_page(struct page *page, unsigned int order)
+{
+	int i;
+	int nr_pages = 1 << order;
+	struct page *p;
+
+	__ClearPageHead(page);
+	if (nr_pages > 1) {
+		atomic_set(compound_mapcount_ptr(page), -1);
+		atomic_set(subpages_mapcount_ptr(page), 0);
+		atomic_set(compound_pincount_ptr(page), 0);
+	}
+	for (i = 0; i < nr_pages; i++) {
+		p = page + i;
+		p->mapping = NULL;
+		safebuddy_page_mapcount_reset(p);
+	}
+}
+
+/*
+Compound page handling mimiced from page allocator
+*/
+static void safebuddy_prep_compound_tail(struct page *head, int tail_idx)
+{
+	struct page *p = head + tail_idx;
+
+	p->mapping = TAIL_MAPPING;
+	set_compound_head(p, head);
+	set_page_private(p, 0);
+}
+
+static void safebuddy_prep_compound_head(struct page *page, unsigned int order)
+{
+	set_compound_page_dtor(page, COMPOUND_PAGE_DTOR);
+	set_compound_order(page, order);
+	atomic_set(compound_mapcount_ptr(page), -1);
+	atomic_set(subpages_mapcount_ptr(page), 0);
+	atomic_set(compound_pincount_ptr(page), 0);
+}
+
+void safebuddy_prep_compound_page(struct page *page, unsigned int order)
+{
+	int i;
+	int nr_pages = 1 << order;
+
+	__SetPageHead(page);
+	for (i = 1; i < nr_pages; i++)
+		safebuddy_prep_compound_tail(page, i);
+
+	safebuddy_prep_compound_head(page, order);
+}
+
+static inline unsigned int safebuddy_buddy_order(struct page *buddy)
+{
+	return page_private(buddy);
+}
+
+/*
+Only used in free, that is why SafeBuddy flag is checked everytime
+*/
+static inline bool page_is_safebuddy_buddy(struct page *buddy, unsigned int order)
+{
+	if (safebuddy_buddy_order(buddy) != order)
+		return false;
+
+	// The Safebuddy flag is set once a buddy is inserted into the quarantine. We only merge quarantined pages!
+	if (!PageSafebuddy(buddy))
+		return false;
+
+	return true;
+}
+
+static inline void safebuddy_set_buddy_order(struct page *buddy, unsigned int order)
+{
+	set_page_private(buddy, order);
+}
+
+static inline void __safebuddy_insert_to_freelist(struct safeslab_slab *buddy, unsigned int order)
+{	
+	single_list_add(&buddy->slab_list, &safebuddy_quarantine[order]);
+}
+
+void __init safebuddy_init(void)
+{
+	int i;
+
+	safebuddy_printk("Initializing\n");
+
+	spin_lock_init(&quarantine_spinlock);
+
+	for (i = 0; i <= SAFEBUDDY_MAX_ORDER; i++) { 
+		INIT_LIST_HEAD(&safebuddy_memory[i]);
+		INIT_SINGLE_LIST_HEAD(&safebuddy_quarantine_a[i]);
+		INIT_SINGLE_LIST_HEAD(&safebuddy_quarantine_b[i]);
+		INIT_SINGLE_LIST_HEAD(&safebuddy_quarantine_c[i]);
+		//INIT_SINGLE_LIST_HEAD(&safebuddy_quarantine_d[i]);
+		//INIT_SINGLE_LIST_HEAD(&safebuddy_quarantine_e[i]);
+	}
+
+	// no need to disable preemption here as we're prolly the onlin cpu online	
+	this_cpu_write(safeslab_active_domain, 1);
+}
+
+void safebuddy_init_secondary(void)
+{
+	// no need to disable preemption here as we're prolly the onlin cpu online	
+	this_cpu_write(safeslab_active_domain, 1);
+}
+
+/*
+Calles after a domain switch. We can give back the quarantine because the pages are not accesible
+through dangling pointers anymore
+*/
+static inline void __safebuddy_release_quarantine(struct single_list_head *old_q)
+{
+	int order = SAFEBUDDY_ORDER_EXTEND;
+	struct safeslab_slab *slab;
+	struct folio* folio;
+	struct single_list_head *headptr, *cur, *nxt;
+
+	while (order <= SAFEBUDDY_MAX_ORDER) {
+		// free buddy pages
+		if (single_list_empty(&old_q[order])) {
+			order++;
+			continue;
+		}
+
+		headptr = &old_q[order];
+		nxt = headptr->next;
+		do {
+			unsigned int no_words;
+			cur = nxt;
+			nxt = cur->next;
+
+			slab = container_of(cur, struct safeslab_slab, slab_list);
+			
+			no_words = slab->objects / 8;
+			if(slab->objects % 8)
+				++no_words;
+			if(no_words)
+				exactfit_free((void *) (slab->bitmap), no_words);
+			
+			__slab_clear_pfmemalloc((struct slab*)slab);
+			
+			folio = slab_folio((struct slab*)slab);
+			folio->mapping = NULL; // Important for page freeing!
+			
+			/* Make the mapping reset visible before clearing the flag */
+			smp_wmb();
+
+			__folio_clear_slab(folio);
+			atomic_set(&(folio_page(folio, 0))->_mapcount, -1);
+			init_page_count(folio_page(folio, 0));
+			
+			//__ClearPageSafebuddy(page);
+				
+			__free_pages((struct page *)slab, order);
+		
+		} while (nxt != headptr);
+		headptr->next = headptr;
+		order++;
+	}
+	
+	// old_q should be empty now. There might still be pages left that hold legitimate objects
+}
+
+void safebuddy_switch_domain(void)
+{
+	unsigned long next_domain = this_cpu_read(safeslab_active_domain);
+
+	next_domain += 1;
+
+	if (next_domain == 16)
+		next_domain = 1;
+
+#ifdef CONFIG_SAFESLAB_PK
+	pkru_write_domain(next_domain);
+#endif
+
+	this_cpu_write(safeslab_active_domain, next_domain);
+	
+	safebuddy_printk("[SAFEBUDDY] Changed safeslab domain to %d on CPU %d", next_domain, smp_processor_id());
+}
+
+static inline void safebuddy_switch_domain_ipi(void)
+{
+	if (likely(num_active_cpus() != 1))
+		safeslab_send_update_pkru();
+	
+	safebuddy_switch_domain();
+}
+
+// returns old quarantine pointer
+static inline struct single_list_head *__safebuddy_swap_quarantines(void)
+{
+	if (safebuddy_quarantine == safebuddy_quarantine_a) {
+		safebuddy_quarantine = safebuddy_quarantine_b;
+		return safebuddy_quarantine_a;
+	}
+
+	if (safebuddy_quarantine == safebuddy_quarantine_b) {
+		safebuddy_quarantine = safebuddy_quarantine_c;
+		return safebuddy_quarantine_b;
+	}
+	//if (safebuddy_quarantine == safebuddy_quarantine_c) {
+	//	safebuddy_quarantine = safebuddy_quarantine_d;
+	//	return safebuddy_quarantine_c;
+	//}
+	//if (safebuddy_quarantine == safebuddy_quarantine_d) {
+	//	safebuddy_quarantine = safebuddy_quarantine_e;
+	//	return safebuddy_quarantine_d;
+	//}
+	safebuddy_quarantine = safebuddy_quarantine_a;
+	return safebuddy_quarantine_c;
+}
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+// scan id maintained by the scanner
+extern unsigned int set_sid;
+extern unsigned int check_sid;
+
+static inline void safebuddy_mark_slab_unsafe(struct safeslab_slab *slab, unsigned int order)
+{
+	// since we boost the value of order, all slabs will be a multiple of the booster,
+	// thus we can mark only the multiplee pages as unsafe 
+	for (int i = 0; i < (1 << (order - SAFEBUDDY_ORDER_EXTEND)); i++) {
+		unsigned int slab_idx = i * (1 << SAFEBUDDY_ORDER_EXTEND);
+		
+		slab[slab_idx].pg_md->unsafe_sid = set_sid;
+	}
+}
+#endif
+
+void safebuddy_quar_pages(struct safeslab_slab *slab, unsigned int order, bool was_safe)
+{
+	unsigned long irqflags;
+	bool switch_domain = false;
+	struct single_list_head *old_q;
+	unsigned int ad;
+#ifdef CONFIG_DEBUG_KMEMLEAK
+	unsigned int sd;
+#endif
+
+	if (order > SAFEBUDDY_MAX_ORDER) {
+		safebuddy_printk("Safebuddy max order surpassed: %u. Defaulting to free_pages\n", order);
+		__folio_clear_slab(slab_folio((struct slab*)slab));
+		__free_pages((struct page *)slab, order);
+		return;
+	}
+
+	safebuddy_printk("called for 0x%lx with order %d quarantine_size %lld\n", (unsigned long)page_to_virt(page), order, quarantine_size);
+
+	ad = this_cpu_read(safeslab_active_domain);
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+	sd = slab->dm_idx;
+	
+	if (was_safe) {
+		if (	sd <= SAFEBUDDY_MAX_DOM && // if valid domain and
+			((sd == SAFEBUDDY_THRESHOLD_DOM_SCAN) || // if slab was allocated in the scanning domain or
+			(sd > ad && sd < SAFEBUDDY_THRESHOLD_DOM_SCAN) || // slab could be reused in its domain without being scanned or
+			(ad > SAFEBUDDY_THRESHOLD_DOM_SCAN && (sd < SAFEBUDDY_THRESHOLD_DOM_SCAN  || sd > ad))) // same as the line above
+		    ) 	// end of if
+			safebuddy_mark_slab_unsafe(slab, order); // then mark slab unsafe
+	}
+#endif
+	
+	spin_lock_irqsave(&quarantine_spinlock, irqflags);
+	
+	quarantine_size += (1 << order);
+
+	__safebuddy_insert_to_freelist(slab, order);	
+
+	if (unlikely(quarantine_size >= MAX_QUARANTINE_SIZE && was_safe)) {
+	
+		safebuddy_printk("SAFESLAB: reached quarantine size %lld on cpu %d! new domain %d\n", quarantine_size, smp_processor_id(), ad);
+	
+		quarantine_size = 0;
+		
+		old_q = __safebuddy_swap_quarantines();
+
+		switch_domain = true;
+	}
+	
+	spin_unlock_irqrestore(&quarantine_spinlock, irqflags);
+	
+	if (unlikely(switch_domain)) {
+	
+		safebuddy_switch_domain_ipi();	
+	
+#ifdef CONFIG_SAFESLAB_TRACE
+		trace_switch_domain(_RET_IP_);
+#endif
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+		
+		//if ((ad % 8) == 0) { // use when you want to scan more frequently
+		if (ad == SAFEBUDDY_THRESHOLD_DOM_SCAN) {
+			kmemleak_unpark();
+#ifdef CONFIG_SAFESLAB_TRACE
+			trace_kmemleak_scan(_RET_IP_);
+#endif
+		}
+#endif	
+
+		__safebuddy_release_quarantine(old_q);
+	}
+}
diff --git a/mm/safeslab.c b/mm/safeslab.c
new file mode 100644
index 000000000000..a5940aeae9ef
--- /dev/null
+++ b/mm/safeslab.c
@@ -0,0 +1,1641 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Safeslab: A slab allocator that provides temporal memory safety
+ */
+
+#include <linux/mm.h>
+#include <linux/swap.h> /* struct reclaim_state */
+#include <linux/module.h>
+#include <linux/bit_spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/bitops.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/kasan.h>
+#include <linux/kmsan.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/mempolicy.h>
+#include <linux/ctype.h>
+#include <linux/stackdepot.h>
+#include <linux/debugobjects.h>
+#include <linux/kallsyms.h>
+#include <linux/kfence.h>
+#include <linux/memory.h>
+#include <linux/math64.h>
+#include <linux/fault-inject.h>
+#include <linux/stacktrace.h>
+#include <linux/prefetch.h>
+#include <linux/memcontrol.h>
+#include <kunit/test.h>
+
+#include <trace/events/kmem.h>
+#include <trace/events/safeslab_trace.h>
+
+#include <linux/exactfit.h>
+#include <linux/safebuddy.h>
+#include <linux/slab.h>
+#include "slab.h"
+#include <linux/safeslab.h>
+#include "safeslab.h"
+
+#include "internal.h"
+
+DEFINE_PER_CPU(unsigned long, safeslab_kernel_pkru);
+EXPORT_PER_CPU_SYMBOL(safeslab_kernel_pkru);
+
+#ifdef CONFIG_SAFESLAB_DEBUG_PRINT
+#define safeslab_printk(fmt, ...) printk("[SAFESLAB] %s " #fmt, __FUNCTION__, ##__VA_ARGS__)
+#else
+#define safeslab_printk(fmt, ...) (void)fmt
+#endif
+
+/*
+ * We could simply use migrate_disable()/enable() but as long as it's a
+ * function call even on !PREEMPT_RT, use inline preempt_disable() there.
+ */
+#ifndef CONFIG_PREEMPT_RT
+#define safeslab_get_cpu_ptr(var)		get_cpu_ptr(var)
+#define safeslab_put_cpu_ptr(var)		put_cpu_ptr(var)
+#else
+#define safeslab_get_cpu_ptr(var)		\
+({					\
+	migrate_disable();		\
+	this_cpu_ptr(var);		\
+})
+#define safeslab_put_cpu_ptr(var)		\
+do {					\
+	(void)(var);			\
+	migrate_enable();		\
+} while (0)
+#endif
+
+#define __fastpath_inline_safeslab __always_inline
+
+/* Safeslab domain, defined and set in safebuddy.c */
+DECLARE_PER_CPU(unsigned long, safeslab_active_domain);
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+
+long long safeslab_max_rss = 0;
+EXPORT_SYMBOL(safeslab_max_rss);
+
+long long safeslab_rss = 0;
+EXPORT_SYMBOL(safeslab_rss);
+
+unsigned long long safeslab_total_page_allocations = 0;
+EXPORT_SYMBOL(safeslab_total_page_allocations);
+
+unsigned long long safeslab_total_page_frees_norcu = 0;
+EXPORT_SYMBOL(safeslab_total_page_frees_norcu);
+
+unsigned long long safeslab_total_page_frees_rcu = 0;
+EXPORT_SYMBOL(safeslab_total_page_frees_rcu);
+#endif
+
+static inline void stat(const struct kmem_cache *s, enum stat_item si)
+{
+#ifdef CONFIG_SLUB_STATS
+	/*
+	 * The rmw is racy on a preemptible kernel but this is acceptable, so
+	 * avoid this_cpu_add()'s irq-disable overhead.
+	 */
+	raw_cpu_inc(s->cpu_slab->stat[si]);
+#endif
+}
+
+/** 
+ * Get virtual address of a slab depending on the currently active domain 
+ * This way safeslab can immediately react to a changed domain and does not
+ * produce objects that will instantly pagefault
+ */
+static inline void *slab_address_domain(struct safeslab_slab *slab) {
+	return (void *)((unsigned long)slab_address((struct slab *)slab) + (slab->dm_idx * DM_SIZE));
+}
+
+/**
+ * Init function for safeslab, currently no initialization needed, only indicates
+ * that safeslab is actually used
+*/
+
+void __init safeslab_init(void) {
+	printk("Safeslab initialized!\n"); 
+}
+
+/* Print out info of a slab, debug only */
+#ifdef CONFIG_SAFESLAB_DEBUG_PRINT
+static void print_slab_info(const struct safeslab_slab *slab)
+{
+	struct folio *folio = (struct folio *)slab_folio((struct slab *)slab);
+
+	printk("Slab 0x%lx cache=%s objects=%u used=%u free=0x%u flags=%pGp\n",
+	       (unsigned long)slab_address_domain(slab), slab->slab_cache->name, slab->objects, slab->inuse, slab->free_objs,
+	       folio_flags(folio, 0));
+}
+#else
+static void print_slab_info(const struct safeslab_slab *slab) {}
+#endif
+
+/**
+ * OO struct reused from SLUB. Holds nr of objects on slab and corresponding
+ * page order for the slab in one value
+*/
+#define OO_SHIFT_SAFESLAB	16
+#define OO_MASK_SAFESLAB		((1 << OO_SHIFT_SAFESLAB) - 1)
+#define MAX_OBJS_PER_PAGE_SAFESLAB	32767 /* since slab.objects is u15 */
+
+static inline unsigned int oo_order(struct kmem_cache_order_objects x)
+{
+	return x.x >> OO_SHIFT_SAFESLAB;
+}
+
+static inline unsigned int oo_objects(struct kmem_cache_order_objects x)
+{
+	return x.x & OO_MASK_SAFESLAB;
+}
+
+/**
+ * pfmemalloc tests for the "active" folio flag. If that flag is set
+ * another check is done if the gfp flags allow pfmemalloc
+*/
+static inline bool pfmemalloc_match(struct slab *slab, gfp_t gfpflags)
+{
+	if (unlikely(slab_test_pfmemalloc(slab)))
+		return gfp_pfmemalloc_allowed(gfpflags);
+
+	return true;
+}
+
+/* Per slab locking using the pagelock */
+static __always_inline void slab_lock(struct slab *slab)
+{
+	struct page *page = slab_page(slab);
+
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	bit_spin_lock(PG_locked, &page->flags);
+}
+
+static __always_inline void slab_unlock(struct slab *slab)
+{
+	struct page *page = slab_page(slab);
+
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	__bit_spin_unlock(PG_locked, &page->flags);
+}
+
+/**
+ * Simple prefetch for the next object since address
+ * of next object is known at allocation
+ */
+static void prefetch_next_obj(void *object)
+{
+	prefetchw(object);
+}
+
+// TODO: do we actually need TIDs?
+#ifdef CONFIG_PREEMPTION
+/*
+ * Calculate the next globally unique transaction for disambiguation.
+ * The transactions start with the cpu number and are then
+ * incremented by CONFIG_NR_CPUS.
+ */
+#define TID_STEP  roundup_pow_of_two(CONFIG_NR_CPUS)
+#else
+/*
+ * No preemption supported therefore also no need to check for
+ * different cpus.
+ */
+#define TID_STEP 1
+#endif /* CONFIG_PREEMPTION */
+
+static inline unsigned long next_tid(unsigned long tid)
+{
+	return tid + TID_STEP;
+}
+
+/* in slub.c, records for which nodes a kmem_cache_node is allocated */
+extern nodemask_t slab_nodes;
+
+/* Check if the objects in a per cpu structure fit numa locality expectations. */
+static inline int node_match(struct slab *slab, int node)
+{
+#ifdef CONFIG_NUMA
+	if (node != NUMA_NO_NODE && slab_nid(slab) != node)
+		return 0;
+#endif
+	return 1;
+}
+
+/* returns how many objects are free on the slab */
+static __always_inline int count_free(struct slab *slab)
+{
+	return slab->objects - slab->inuse;
+}
+
+// TODO: better OOM handling than just BUG_ON(true)
+/* Out of memory debug print. Dumps a lot of information on the cache */
+static noinline void
+slab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid)
+{
+	static DEFINE_RATELIMIT_STATE(slub_oom_rs, DEFAULT_RATELIMIT_INTERVAL,
+				      DEFAULT_RATELIMIT_BURST);
+
+	if ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slub_oom_rs))
+		return;
+
+	pr_warn("SLUB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\n",
+		nid, gfpflags, &gfpflags);
+	pr_warn("  cache: %s, object size: %u, buffer size: %u, default order: %u, min order: %u\n",
+		s->name, s->object_size, s->size, oo_order(s->oo),
+		oo_order(s->min));
+
+	BUG_ON(true);
+}
+
+/*
+   ##########################################################################
+   # 					Slab allocation and freeing							#
+   ##########################################################################
+*/
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+extern unsigned int check_sid;
+extern unsigned int set_sid;
+
+//uint64_t pg_md_cnt = 0;
+
+static inline bool safeslab_safe(struct safeslab_slab *safeslab, unsigned int order, unsigned long a_tmd)
+{
+	// walk all pages of the slab and check if they're safe for current domain
+	for (int i = 0; i < (1 << (order - SAFEBUDDY_ORDER_EXTEND)); i++) {
+		int slab_idx = i * (1 << SAFEBUDDY_ORDER_EXTEND);
+
+		if (unlikely(safeslab[slab_idx].pg_md == NULL)) {
+			safeslab[slab_idx].pg_md = exactfit_alloc(sizeof(struct safeslab_pg_md), 0);
+			memset(safeslab[slab_idx].pg_md, 0, sizeof(struct safeslab_pg_md));
+			//pg_md_cnt++;
+			//printk("Allocated %lld new pg_mds\n", pg_md_cnt);
+		} else if (unlikely((safeslab[slab_idx].pg_md->unmapped_sid[a_tmd] >= check_sid)
+			|| (safeslab[slab_idx].pg_md->unsafe_sid >= check_sid))) {
+			// if the page is unsafe quarantine the whole slab and retry allocating a new one
+
+#ifdef CONFIG_SAFESLAB_TRACE
+			trace_free_slab(_RET_IP_);
+			// count the page as free for our memeval
+			for(int j = 0; j < (1 << order); j++)
+				trace_free_slab_pagecount(_RET_IP_);
+#endif
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+			safeslab_total_page_frees_norcu += (1 << order);
+			
+			safeslab_rss -= (1 << order);
+#endif			
+			safebuddy_quar_pages(safeslab, order, false);
+			return false;
+		}
+		
+		safeslab[slab_idx].pg_md->slab[a_tmd].idx = slab_idx;
+		safeslab[slab_idx].pg_md->slab[a_tmd].sid = set_sid;
+	}
+
+	safeslab->pg_md->slab[a_tmd].order = order - SAFEBUDDY_ORDER_EXTEND;
+	
+	return true;
+}
+#endif
+
+static inline struct slab *alloc_slab_page(gfp_t flags, int node,
+		struct kmem_cache_order_objects oo)
+{
+	struct folio *folio;
+	struct slab *slab;
+	unsigned int order = oo_order(oo);
+	struct page *page;
+	unsigned long a_tmd;
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+retry:
+#endif
+
+	page = alloc_pages(flags, order);
+
+	if (!page)
+		return NULL;
+
+#ifdef CONFIG_SAFESLAB_TRACE
+	trace_alloc_slab_page(_RET_IP_);
+	for(int i=0; i < (1<<order); i++)
+		trace_alloc_slab_page_pagecount(_RET_IP_);
+#endif
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+	safeslab_total_page_allocations += 1<<order;
+	
+	safeslab_rss += 1<<order;
+	if (safeslab_rss > safeslab_max_rss)
+		safeslab_max_rss = safeslab_rss;
+#endif
+
+	a_tmd = this_cpu_read(safeslab_active_domain);
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+	if (!safeslab_safe((struct safeslab_slab *)page, order, a_tmd))
+		goto retry;
+#endif
+
+	folio = page_folio(page);
+
+	slab = folio_slab(folio);
+	__folio_set_slab(folio);
+	__folio_set_safeslab(folio);
+	
+	((struct safeslab_slab *)slab)->dm_idx = a_tmd;
+
+	/* Make the flag visible before any changes to folio->mapping */
+	smp_wmb(); // barrier
+	if (page_is_pfmemalloc(folio_page(folio, 0)))
+		slab_set_pfmemalloc(slab); // in mm/slab.h
+
+	return slab;
+}
+
+noinline struct slab *allocate_slab_safeslab(struct kmem_cache *s, gfp_t flags, int node)
+{
+	struct slab *slab;
+	struct kmem_cache_order_objects oo = s->oo;
+	gfp_t alloc_gfp;
+	int no_words;
+	unsigned long *bm;
+
+	flags &= gfp_allowed_mask;
+
+	flags |= s->allocflags;
+
+	/*
+	 * Let the initial higher-order allocation fail under memory pressure
+	 * so we fall-back to the minimum order allocation.
+	 */
+	alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
+	if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
+		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;
+	alloc_gfp |= __GFP_SAFESLAB_SKIP_ZERO;
+
+	slab = alloc_slab_page(alloc_gfp, node, oo);
+	if (unlikely(!slab)) {
+		oo = s->min;
+		alloc_gfp = flags | __GFP_SAFESLAB_SKIP_ZERO;
+
+		/*
+		 * Allocation may have failed due to fragmentation.
+		 * Try a lower order alloc if possible
+		 */
+		slab = alloc_slab_page(alloc_gfp, node, oo);
+		if (unlikely(!slab))
+			return NULL;
+	}
+
+	// Set counter values
+	slab->objects = oo_objects(oo);
+	slab->inuse = 0;
+	((struct safeslab_slab *)slab)->free_objs = 0;
+
+	slab->slab_cache = s;
+
+	// calculate how many bits we need for bitmap
+	no_words = oo_objects(oo) / 8;
+	if (oo_objects(oo) % 8)
+		no_words += 1;
+
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+	//TWO WORDS FOR EVERY OBJECT
+	if (s->flags & SLAB_KMALLOC)
+		no_words += 4 * oo_objects(oo);
+#endif
+
+	// allocate bitmap from exactfit allcator
+	bm = (unsigned long *)exactfit_alloc(no_words, 0);
+	
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+	if (s->flags & SLAB_KMALLOC)
+		no_words -= 4 * oo_objects(oo);
+#endif
+	
+	// initialize bitmap
+	memset(bm, 0, no_words);
+	
+	// register bitmap on slab
+	((struct safeslab_slab *)slab)->bitmap = bm;
+
+	return slab;
+}
+
+static struct slab *new_slab(struct kmem_cache *s, gfp_t flags, int node)
+{
+	if (unlikely(flags & GFP_SLAB_BUG_MASK))
+		flags = kmalloc_fix_flags(flags);
+
+	WARN_ON_ONCE(s->ctor && (flags & __GFP_ZERO));
+
+	return allocate_slab_safeslab(s,
+		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
+}
+
+#ifdef CONFIG_SAFESLAB_NO_SAFEBUDDY
+static void __free_slab_safeslab(struct kmem_cache *s, struct slab *slab)
+{
+	struct folio *folio = slab_folio(slab);
+	int order = folio_order(folio), pages = 1 << order, i = 0;
+	unsigned int no_words;
+
+	no_words = (slab->objects) / 8;
+	if (slab->objects % 8)
+		no_words += 1;
+	
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+	if (s->flags & SLAB_KMALLOC)
+		no_words += 4 * (slab->objects);
+#endif
+
+	if(no_words)
+		exactfit_free((void *) (((struct safeslab_slab *)slab)->bitmap), no_words);
+
+	__slab_clear_pfmemalloc(slab);
+
+	folio = slab_folio(slab);
+	folio->mapping = NULL; // Important for page freeing!
+
+	/* Make the mapping reset visible before clearing the flag */
+	smp_wmb();
+
+	__folio_clear_safeslab(folio);
+	__folio_clear_slab(folio);
+
+	atomic_set(&(folio_page(folio, 0))->_mapcount, -1);
+
+	init_page_count(folio_page(folio, 0));
+
+	if (current->reclaim_state)
+		current->reclaim_state->reclaimed_slab += pages;
+
+	unaccount_slab(slab, order, s); // Memcg stuff
+
+	__free_pages(folio_page(folio, 0), order);
+}
+#else
+static void __free_slab_safeslab(struct kmem_cache *s, struct slab *slab)
+{
+	struct folio *folio = slab_folio(slab);
+	int order = folio_order(folio);
+	int pages = 1 << order;
+
+#ifdef CONFIG_SAFESLAB_TRACE
+	trace_free_slab(_RET_IP_);
+#endif
+
+	__folio_clear_safeslab(folio);
+
+	// Note: we reset all the other page fields when releasing the q in safebuddy
+
+	if (current->reclaim_state)
+		current->reclaim_state->reclaimed_slab += pages;
+
+	unaccount_slab(slab, order, s); // Memcg stuff
+
+	safebuddy_quar_pages((struct safeslab_slab *)slab, order, true);
+}
+#endif
+
+static void rcu_free_slab(struct rcu_head *h)
+{
+	struct slab *slab = container_of(h, struct slab, rcu_head);
+
+	__free_slab_safeslab(slab->slab_cache, slab);
+}
+
+static void free_slab(struct kmem_cache *s, struct slab *slab)
+{
+	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
+#ifdef CONFIG_SAFESLAB_TRACE
+		for(int i = 0; i < (1 << slab_order(slab)); i++)
+			trace_free_slab_pagecount_rcu(_RET_IP_);
+#endif
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+		safeslab_total_page_frees_rcu += 1 << slab_order(slab);
+		safeslab_rss -= (1 << slab_order(slab));
+#endif
+
+		call_rcu(&slab->rcu_head, rcu_free_slab);
+		safeslab_printk("RCU slab freeing\n");
+	} else {
+#ifdef CONFIG_SAFESLAB_TRACE
+		for(int i = 0; i < (1 << slab_order(slab)); i++)
+			trace_free_slab_pagecount(_RET_IP_);
+#endif
+
+#ifdef CONFIG_SAFESLAB_MEMBENCH
+		safeslab_total_page_frees_norcu += 1 << slab_order(slab);
+		safeslab_rss -= (1 << slab_order(slab));
+#endif
+		__free_slab_safeslab(s, slab);
+	}
+}
+
+/* Use this to get rid of a slab */
+static void discard_slab(struct kmem_cache *s, struct slab *slab)
+{
+	free_slab(s, slab);
+}
+
+/*
+   ##########################################################################
+   # 	        Object allocation and freeing + Slab handling		    #
+   ##########################################################################
+*/
+
+/* Actually do the object allocation. */
+static inline void *__safeslab_get_object(struct kmem_cache *s, struct safeslab_slab *slab, unsigned long idx)
+{
+	void *object = NULL;
+
+	// this actually incs slab->inuse, needed by the scanner
+	atomic_inc((atomic_t *)&slab->counters);
+
+	// get address of object; if domain switch occurs here we should be safe, 
+	// since the PF handler will see it as allocated so it can migrate it into new domain
+	object = (void *) ((unsigned long) slab_address_domain(slab) + (s->size * idx));
+	
+	// already prefetch next object; 
+	// prolly not that useful for large objects, since it only brings in one cache line
+	prefetch_next_obj((void *)((unsigned long)object + s->size));
+
+	return object;
+}
+
+/* Structure holding parameters for get_partial() call chain */
+struct partial_context {
+	struct slab **slab;
+	gfp_t flags;
+	unsigned int orig_size;
+};
+
+static inline void remove_partial(struct kmem_cache_node *n, struct slab *slab)
+{
+	lockdep_assert_held(&n->list_lock);
+	list_del(&slab->slab_list);
+	n->nr_partial--;
+}
+
+/*
+ * Remove slab from the partial list, freeze it and
+ * allocate an object on it
+ */
+static inline void *acquire_slab(struct kmem_cache *s,
+		struct kmem_cache_node *n, struct slab *slab)
+{
+	lockdep_assert_held(&n->list_lock);
+
+	slab->frozen = 1;
+
+	// slab should not be full in partial list, deactivate_slab normally prevents this.
+	// Thus, below allocation should succeed
+	remove_partial(n, slab);
+	
+	return (void *)1;
+}
+
+/*
+ * Try to allocate a partial slab from a specific node.
+ */
+static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
+			      struct partial_context *pc)
+{
+	struct slab *slab, *slab2;
+	void *object = NULL;
+	unsigned long flags;
+
+	/*
+	 * Racy check. If we mistakenly see no partial slabs then we
+	 * just allocate an empty slab. If we mistakenly try to get a
+	 * partial slab and there is none available then get_partial()
+	 * will return NULL.
+	 */
+	if (!n || !n->nr_partial)
+		return NULL;
+
+	spin_lock_irqsave(&n->list_lock, flags);
+	list_for_each_entry_safe(slab, slab2, &n->partial, slab_list) {
+
+		if (!pfmemalloc_match(slab, pc->flags))
+			continue;
+
+		object = acquire_slab(s, n, slab);
+
+		// acquired slab and had successful allocation on it
+		*pc->slab = slab; // slab to promote to cpu slab
+		break;
+	}
+	spin_unlock_irqrestore(&n->list_lock, flags);
+	return object;
+}
+
+/*
+ * Get a slab from somewhere. Search in increasing NUMA distances.
+ */
+static void *get_any_partial(struct kmem_cache *s, struct partial_context *pc)
+{
+#ifdef CONFIG_NUMA
+	struct zonelist *zonelist;
+	struct zoneref *z;
+	struct zone *zone;
+	enum zone_type highest_zoneidx = gfp_zone(pc->flags);
+	void *object;
+	unsigned int cpuset_mems_cookie;
+
+	/*
+	 * The defrag ratio allows a configuration of the tradeoffs between
+	 * inter node defragmentation and node local allocations. A lower
+	 * defrag_ratio increases the tendency to do local allocations
+	 * instead of attempting to obtain partial slabs from other nodes.
+	 *
+	 * If the defrag_ratio is set to 0 then kmalloc() always
+	 * returns node local objects. If the ratio is higher then kmalloc()
+	 * may return off node objects because partial slabs are obtained
+	 * from other nodes and filled up.
+	 *
+	 * If /sys/kernel/slab/xx/remote_node_defrag_ratio is set to 100
+	 * (which makes defrag_ratio = 1000) then every (well almost)
+	 * allocation will first attempt to defrag slab caches on other nodes.
+	 * This means scanning over all nodes to look for partial slabs which
+	 * may be expensive if we do it every time we are trying to find a slab
+	 * with available objects.
+	 */
+	if (!s->remote_node_defrag_ratio ||
+			get_cycles() % 1024 > s->remote_node_defrag_ratio)
+		return NULL;
+
+	do {
+		cpuset_mems_cookie = read_mems_allowed_begin();
+		zonelist = node_zonelist(mempolicy_slab_node(), pc->flags);
+		for_each_zone_zonelist(zone, z, zonelist, highest_zoneidx) {
+			struct kmem_cache_node *n;
+
+			n = get_node(s, zone_to_nid(zone));
+
+			if (n && cpuset_zone_allowed(zone, pc->flags) &&
+					n->nr_partial > s->min_partial) {
+				object = get_partial_node(s, n, pc);
+				if (object) {
+					/*
+					 * Don't check read_mems_allowed_retry()
+					 * here - if mems_allowed was updated in
+					 * parallel, that was a harmless race
+					 * between allocation and the cpuset
+					 * update
+					 */
+					return object;
+				}
+			}
+		}
+	} while (read_mems_allowed_retry(cpuset_mems_cookie));
+#endif	/* CONFIG_NUMA */
+	return NULL;
+}
+
+/*
+ * Get a partial slab, freeze it and return it.
+ */
+static void *get_partial(struct kmem_cache *s, int node, struct partial_context *pc)
+{
+	void *object;
+	int searchnode = node;
+
+	if (node == NUMA_NO_NODE)
+		searchnode = numa_mem_id();
+
+	object = get_partial_node(s, get_node(s, searchnode), pc);
+	// object can be NULL if the slab in the partial list was full
+	// returns object if specific node was expected, even if obj is null -> new slab for that node is allocated
+	if (object || node != NUMA_NO_NODE)
+		return object;
+
+	// get object from any other node's slab
+	// pretty likely to return an object, if not, actually no slab present for the cache at all
+	return get_any_partial(s, pc);
+}
+
+/*
+ * Management of partially allocated slabs.
+ */
+static inline void
+__add_partial(struct kmem_cache_node *n, struct slab *slab)
+{
+	n->nr_partial++;
+	list_add(&slab->slab_list, &n->partial);
+}
+
+static inline void add_partial(struct kmem_cache_node *n, struct slab *slab)
+{
+	lockdep_assert_held(&n->list_lock);
+	__add_partial(n, slab);
+}
+
+static inline bool safeslab_want_init_on_alloc(gfp_t flags, struct kmem_cache *c)
+{
+	return true;
+	//if (static_branch_maybe(CONFIG_INIT_ON_ALLOC_DEFAULT_ON,
+	//			&init_on_alloc)) {
+	//	if (c->ctor)
+	//		return false;
+	//	if (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON))
+	//		return flags & __GFP_ZERO;
+	//	return true;
+	//}
+
+	//return flags & __GFP_ZERO;
+}
+
+static inline bool safeslab_want_init_on_free(struct kmem_cache *c)
+{
+	//if (static_branch_maybe(CONFIG_INIT_ON_FREE_DEFAULT_ON,
+	//			&init_on_free))
+	//	return !(c->ctor ||
+	//		 (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));
+	return false;
+}
+
+/*
+ * Finishes removing the cpu slab. Unfreezes the slabs and puts it on the proper list.
+ * Assumes the slab has been already safely taken away from kmem_cache_cpu by the caller.
+ */
+static void deactivate_slab(struct kmem_cache *s, struct safeslab_slab *slab)
+{
+	struct kmem_cache_node *n = get_node(s, slab_nid((struct slab *)slab));
+	unsigned long irqflags;
+
+	VM_BUG_ON(!slab->frozen);
+
+	// if we're we're partially free just unfreeze and add in partials list;
+	// since we're frozen, nobody else can allocate from us, so here we're 
+	// not racing with any remote cpu (, right??)
+	// TODO: do we use per-cpu partial lists?
+	if (slab->inuse != slab->objects) {
+		spin_lock_irqsave(&n->list_lock, irqflags);
+	
+		stat(s, DEACTIVATE_TO_HEAD);
+		
+		add_partial(n, (struct slab *)slab);
+		slab->frozen = 0;
+	
+		spin_unlock_irqrestore(&n->list_lock, irqflags);
+
+		return;
+	}
+
+	// if we're freeable then free slab;
+	// here we're racing with remote cpus, so we gotta do both the check and
+	// the set atomically
+	if (cmpxchg(&slab->free_objs, slab->objects, 0) == slab->objects) {
+		stat(s, FREE_SLAB);
+		discard_slab(s, (struct slab *)slab);
+	}
+	else // if we're just full simply unfreeze
+		slab->frozen = 0;
+}
+
+/*
+ * Allocate object from slab (per cpu)
+ */
+static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
+			  struct kmem_cache_cpu *c, unsigned int orig_size)
+{
+	struct slab *slab;
+	unsigned long flags;
+	struct partial_context pc;
+	void *object;
+
+reread_slab:
+
+	slab = READ_ONCE(c->slab);
+	if (!slab) {
+		/*
+		 * if the node is not online or has no normal memory, just
+		 * ignore the node constraint
+		 */
+		if (unlikely(node != NUMA_NO_NODE &&
+			     !node_isset(node, slab_nodes)))
+			node = NUMA_NO_NODE;
+		goto new_slab; // no active cpu slab -> get new one :)
+	}
+
+	if (unlikely(!node_match(slab, node))) {
+		/*
+		 * same as above but node_match() being false already
+		 * implies node != NUMA_NO_NODE
+		 */
+		if (!node_isset(node, slab_nodes)) {
+			node = NUMA_NO_NODE;
+		} else {
+			goto deactivate_slab_lock; // revokes current promoted cpu slab, results in new_slab:
+		}
+	}
+
+	/*
+	 * By rights, we should be searching for a slab page that was
+	 * PFMEMALLOC but right now, we are losing the pfmemalloc
+	 * information when the page leaves the per-cpu allocator
+	 */
+
+	if (unlikely(!pfmemalloc_match(slab, gfpflags)))
+		goto deactivate_slab_lock;
+
+	/* must check again c->slab in case we got preempted and it changed */
+	local_lock_irqsave(&s->cpu_slab->lock, flags);
+	if (unlikely(slab != c->slab)) {
+		local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+		goto reread_slab;
+	}
+
+	// Get object from slab
+	if (c->objects != c->inuse)
+		goto out_return;
+
+	// No object, slab was full
+	goto deactivate_slab_no_lock;
+	
+out_return:
+
+	lockdep_assert_held(this_cpu_ptr(&s->cpu_slab->lock));
+
+	/*
+	 * slab is pointing to the slab from which the objects are obtained.
+	 * That slab must be frozen for per cpu allocations to work.
+	 */
+	VM_BUG_ON(!c->slab->frozen);
+	object = __safeslab_get_object(s, (struct safeslab_slab *)slab, c->inuse);
+	c->inuse++;
+	c->tid = next_tid(c->tid);
+	local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+	return object;
+
+deactivate_slab_lock: // wrong node, demote current cpu slab
+		
+	stat(s, ALLOC_NODE_MISMATCH);
+	
+	local_lock_irqsave(&s->cpu_slab->lock, flags);
+	if (slab != c->slab) {
+		local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+		goto reread_slab;
+	}
+
+deactivate_slab_no_lock: // cpu slab full, demote it
+	
+	stat(s, ALLOC_SLOWPATH);
+
+	c->slab = NULL;
+	c->counters = 0;
+	c->tid = next_tid(c->tid);
+	
+	// uncomment this if you let alloc deactivate_slabs
+	//slab->inuse = c->inuse;
+	
+	local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+	
+	// uncomment this if you let alloc deactivate_slabs
+	//deactivate_slab(s, (struct safeslab_slab *)slab);
+
+new_slab:
+	pc.flags = gfpflags;
+	pc.slab = &slab;
+	pc.orig_size = orig_size;
+	object = get_partial(s, node, &pc);
+	// We got an object from a partial slab (preferrably from the requested node)
+	if (object) {
+		stat(s, ALLOC_FROM_PARTIAL);
+		goto check_new_slab;
+	}
+
+	stat(s, ALLOC_SLAB);
+	// No object from partials -> new slab needed!
+	safeslab_put_cpu_ptr(s->cpu_slab);
+	slab = new_slab(s, gfpflags, node);
+	c = safeslab_get_cpu_ptr(s->cpu_slab);
+
+	if (unlikely(!slab)) {
+		slab_out_of_memory(s, gfpflags, node);
+		// TODO: better OOM handling
+		BUG_ON(true);
+	}
+	
+	slab->frozen = 1; // slab will be promoted to cpu slab
+
+check_new_slab:
+
+	if (unlikely(!pfmemalloc_match(slab, gfpflags))) {
+		/*
+		 * For !pfmemalloc_match() case we don't promote the slab so that
+		 * we don't make further mismatched allocations
+		 */
+		deactivate_slab(s, (struct safeslab_slab *)slab);
+		return object;
+	}
+
+retry_load_slab:
+
+	// We might have been preempted and a full allocation went through and
+	// promoted a different cpu slab. This is not good, so we demote it again
+	// until we can safely set our new slab/the partial aqcuired slab as new
+	// cpu slab
+	local_lock_irqsave(&s->cpu_slab->lock, flags);
+	if (unlikely(c->slab)) {
+		struct slab *flush_slab = c->slab;
+	
+		flush_slab->inuse = c->inuse;
+		
+		c->slab = NULL;
+		c->counters = 0;
+		c->tid = next_tid(c->tid);
+
+		local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+
+		deactivate_slab(s, (struct safeslab_slab *)flush_slab);
+
+		goto retry_load_slab;
+	}
+	c->slab = slab;
+	c->inuse = slab->inuse;
+	c->objects = slab->objects;
+
+	goto out_return;
+}
+
+/*
+ * A wrapper for ___slab_alloc() for contexts where preemption is not yet
+ * disabled. Compensates for possible cpu changes by refetching the per cpu area
+ * pointer.
+ */
+static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
+			  struct kmem_cache_cpu *c, unsigned int orig_size)
+{
+	void *p;
+
+#ifdef CONFIG_PREEMPT_COUNT
+	/*
+	 * We may have been preempted and rescheduled on a different
+	 * cpu before disabling preemption. Need to reload cpu area
+	 * pointer.
+	 */
+	c = safeslab_get_cpu_ptr(s->cpu_slab);
+#endif
+
+	p = ___slab_alloc(s, gfpflags, node, c, orig_size);
+#ifdef CONFIG_PREEMPT_COUNT
+	safeslab_put_cpu_ptr(s->cpu_slab);
+#endif
+	return p;
+}
+
+static __always_inline void *__slab_alloc_node(struct kmem_cache *s,
+		gfp_t gfpflags, int node, size_t orig_size)
+{
+	struct kmem_cache_cpu *c;
+	struct slab *slab;
+	unsigned long tid, counters, inuse, objects;
+	void *object;
+
+redo:
+	/*
+	 * Must read kmem_cache cpu data via this cpu ptr. Preemption is
+	 * enabled. We may switch back and forth between cpus while
+	 * reading from one cpu area. That does not matter as long
+	 * as we end up on the original cpu again when doing the cmpxchg.
+	 *
+	 * We must guarantee that tid and kmem_cache_cpu are retrieved on the
+	 * same cpu. We read first the kmem_cache_cpu pointer and use it to read
+	 * the tid. If we are preempted and switched to another cpu between the
+	 * two reads, it's OK as the two are still associated with the same cpu
+	 * and cmpxchg later will validate the cpu.
+	 */
+	c = raw_cpu_ptr(s->cpu_slab);
+	tid = READ_ONCE(c->tid);
+
+	/*
+	 * Irqless object alloc/free algorithm used here depends on sequence
+	 * of fetching cpu_slab's data. tid should be fetched before anything
+	 * on c to guarantee that object and slab associated with previous tid
+	 * won't be used with current tid. If we fetch tid first, object and
+	 * slab could be one associated with next tid and our alloc/free
+	 * request will be failed. In this case, we will retry. So, no problem.
+	 */
+	barrier();
+
+	/*
+	 * The transaction ids are globally unique per cpu and per operation on
+	 * a per cpu queue. Thus they can be guarantee that the cmpxchg_double
+	 * occurs on the right processor and that there was no operation on the
+	 * linked list in between.
+	 */
+
+	slab = c->slab;
+	counters = c->counters;
+	inuse = c->inuse;
+	objects = c->objects;
+
+	if (unlikely((inuse == objects) || !slab || !node_match(slab, node))) {
+		object = __slab_alloc(s, gfpflags, node, c, orig_size);
+	} else {
+		/*
+		 * The cmpxchg will only match if there was no additional
+		 * operation and if we are on the right processor.
+		 *
+		 * The cmpxchg does the following atomically (without lock
+		 * semantics!)
+		 * 1. Relocate first pointer to the current per cpu area.
+		 * 2. Verify that tid and freelist have not been changed
+		 * 3. If they were not changed replace tid and freelist
+		 *
+		 * Since this is without lock semantics the protection is only
+		 * against code executing on this cpu *not* from access by
+		 * other cpus.
+		 */
+		if (unlikely(!this_cpu_cmpxchg_double(
+				s->cpu_slab->counters, s->cpu_slab->tid,
+				counters, tid,
+				counters+1, next_tid(tid)))) {
+
+			//note_cmpxchg_failure("slab_alloc", s, tid);
+			goto redo;
+		}
+
+		// at this stage we've won the race and successfully reserved one object for allocation
+
+		object = __safeslab_get_object(s, (struct safeslab_slab *)slab, inuse);	
+	
+		// if we've allocated the last object then we can unfreeze the slab
+		// so that slab_free can discard it whenever it frees the last object
+		if (inuse + 1 == objects)
+			slab->frozen = 0;
+
+		stat(s, ALLOC_FASTPATH);
+	}
+
+	return object;
+}
+
+/*
+ * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc)
+ * have the fastpath folded into their functions. So no function call
+ * overhead for requests that can be satisfied on the fastpath.
+ */
+static __fastpath_inline_safeslab void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,
+		gfp_t gfpflags, int node, size_t orig_size)
+{
+	void *object;
+	struct obj_cgroup *objcg = NULL;
+
+	// Should not produce any code
+	s = slab_pre_alloc_hook(s, lru, &objcg, 1, gfpflags);
+	if (!s)
+		return NULL;
+
+	object = __slab_alloc_node(s, gfpflags, node, orig_size);
+
+	/*
+	 * When init equals 'true', like for kzalloc() family, only
+	 * @orig_size bytes might be zeroed instead of s->object_size
+	 */
+	// For 0ing i.e., also completely in mm/slab.h
+	slab_post_alloc_hook(s, objcg, gfpflags, 1, &object, safeslab_want_init_on_alloc(gfpflags, s), orig_size);
+
+	return object;
+}
+
+static __fastpath_inline_safeslab void *slab_alloc(struct kmem_cache *s, struct list_lru *lru,
+		gfp_t gfpflags, size_t orig_size)
+{
+	return slab_alloc_node(s, lru, gfpflags, NUMA_NO_NODE, orig_size);
+}
+
+static __fastpath_inline_safeslab
+void *__kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru, gfp_t gfpflags)
+{
+	void *ret = slab_alloc(s, lru, gfpflags, s->size);
+    trace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, NUMA_NO_NODE);
+	return ret;
+}
+
+// Exposed, delegation for SLUB in case s is a safeslab cache
+void *kmem_cache_alloc_safeslab(struct kmem_cache *s, gfp_t gfpflags)
+{
+	return __kmem_cache_alloc_lru(s, NULL, gfpflags);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_safeslab);
+
+// Exposed, delegation for SLUB in case s is a safeslab cache
+void *kmem_cache_alloc_lru_safeslab(struct kmem_cache *s, struct list_lru *lru,
+			   gfp_t gfpflags)
+{
+	return __kmem_cache_alloc_lru(s, lru, gfpflags);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_lru_safeslab);
+
+void *__kmem_cache_alloc_node_safeslab(struct kmem_cache *s, gfp_t gfpflags,
+			      int node, size_t orig_size,
+			      unsigned long caller)
+{
+	(void) caller;
+	return slab_alloc_node(s, NULL, gfpflags, node,
+			       orig_size);
+}
+
+// Exposed, delegation for SLUB in case s is a safeslab cache
+void *kmem_cache_alloc_node_safeslab(struct kmem_cache *s, gfp_t gfpflags, int node)
+{
+	void *ret = slab_alloc_node(s, NULL, gfpflags, node, s->size);
+	trace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, node);
+	return ret;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_node_safeslab);
+
+void *__kmalloc_large_node_safeslab(size_t size, gfp_t flags, int node)
+{
+	struct page *page;
+	void *ptr = NULL;
+	unsigned int order = get_order(size);
+	unsigned long a_tmd;
+
+	if (unlikely(flags & GFP_SLAB_BUG_MASK))
+		flags = kmalloc_fix_flags(flags);
+
+	flags |= __GFP_COMP;
+
+	a_tmd = this_cpu_read(safeslab_active_domain);
+
+retry:
+	page = alloc_pages_node(node, flags, order);
+	if (page) {
+
+#ifdef CONFIG_SAFESLAB_TRACE
+		trace_alloc_slab_page(_RET_IP_);
+		for(int i=0;i < (1<<order); i++)
+			trace_alloc_slab_page_pagecount(_RET_IP_);
+#endif
+
+#ifdef CONFIG_DEBUG_KMEMLEAK
+		if (!safeslab_safe((struct safeslab_slab *)page, order, a_tmd))
+			goto retry;
+#endif
+
+		ptr = page_address(page);
+		mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
+				      PAGE_SIZE << order);
+	}
+
+	ptr = kasan_kmalloc_large(ptr, size, flags);
+	kmsan_kmalloc_large(ptr, size, flags);
+
+	return ptr;
+}
+EXPORT_SYMBOL(__kmalloc_large_node_safeslab);
+
+static inline int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags,
+			size_t size, void **p, struct obj_cgroup *objcg)
+{
+	struct kmem_cache_cpu *c;
+	int i;
+
+	/*
+	 * Drain objects in the per cpu slab, while disabling local
+	 * IRQs, which protects against PREEMPT and interrupts
+	 * handlers invoking normal fastpath.
+	 */
+	c = safeslab_get_cpu_ptr(s->cpu_slab);
+	local_lock_irq(&s->cpu_slab->lock);
+
+	for (i = 0; i < size; i++) {
+		if (unlikely(c->inuse == c->objects)) {
+			/*
+			 * CPU slab was full
+			 * Since ___slab_alloc() may reenable interrupts while
+			 * allocating memory, we should bump c->tid now.
+			 * We hold the per cpu slab lock, so noone could have freed anything
+			 * on the current slab. Thus, we do not have to check if we need to
+			 * discard here, we just unfreeze and leave it for later frees.
+			 */
+			c->tid = next_tid(c->tid);
+
+			local_unlock_irq(&s->cpu_slab->lock);
+			
+			// will promote a new cpu slab!
+			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+					    c, s->size);
+			if (unlikely(!p[i]))
+				goto error;
+
+			c = this_cpu_ptr(s->cpu_slab);
+
+			local_lock_irq(&s->cpu_slab->lock);
+
+			continue; /* goto for-loop */
+		}
+		
+		p[i] = __safeslab_get_object(s, (struct safeslab_slab *)c->slab, c->inuse);
+		c->inuse++;
+		if (c->inuse == c->objects)
+			c->slab->frozen = 0;
+			
+	}
+	c->tid = next_tid(c->tid);
+	local_unlock_irq(&s->cpu_slab->lock);
+	safeslab_put_cpu_ptr(s->cpu_slab);
+
+	return i;
+
+error:
+	safeslab_put_cpu_ptr(s->cpu_slab);
+	// in mm/slab.h
+	slab_post_alloc_hook(s, objcg, flags, i, p, false, s->size);
+	kmem_cache_free_bulk_safeslab(s, i, p);
+	return 0;
+
+}
+
+/* Note that interrupts must be enabled when calling this function. */
+// Exposed, delegation for SLUB in case s is a safeslab cache
+int kmem_cache_alloc_bulk_safeslab(struct kmem_cache *s, gfp_t flags, size_t size,
+			  void **p)
+{
+	int i;
+	struct obj_cgroup *objcg = NULL;
+
+	/* memcg and kmem_cache debug support */
+	// pre_alloc_hook in mm/slab.h
+	s = slab_pre_alloc_hook(s, NULL, &objcg, size, flags);
+	if (unlikely(!s))
+		return 0;
+
+	i = __kmem_cache_alloc_bulk(s, flags, size, p, objcg);
+
+	/*
+	 * memcg and kmem_cache debug support and memory initialization.
+	 * Done outside of the IRQ disabled fastpath loop.
+	 */
+	if (i != 0)
+		// pre_alloc_hook in mm/slab.h, same for want_init_on_alloc
+		slab_post_alloc_hook(s, objcg, flags, size, p, safeslab_want_init_on_alloc(flags, s), s->size);
+	return i;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_bulk_safeslab);
+
+/*
+ * Hooks for other subsystems that check memory allocations. In a typical
+ * production configuration these hooks all should produce no code at all.
+ */
+static __always_inline bool slab_free_hook(struct kmem_cache *s,
+						void *x, bool init)
+{
+	kmsan_slab_free(s, x);
+
+	debug_check_no_locks_freed(x, s->size);
+
+	if (!(s->flags & SLAB_DEBUG_OBJECTS))
+		debug_check_no_obj_freed(x, s->size);
+
+	/* Use KCSAN to help debug racy use-after-free. */
+	if (!(s->flags & SLAB_TYPESAFE_BY_RCU))
+		__kcsan_check_access(x, s->size,
+				     KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT);
+
+	/*
+	 * As memory initialization might be integrated into KASAN,
+	 * kasan_slab_free and initialization memset's must be
+	 * kept together to avoid discrepancies in behavior.
+	 *
+	 * The initialization memset's clear the object and the metadata,
+	 * but don't touch the SLAB redzone.
+	 */
+	if (init) {
+		int rsize;
+
+		if (!kasan_has_integrated_init())
+			memset(kasan_reset_tag(x), 0, s->size);
+		rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad : 0;
+		memset((char *)kasan_reset_tag(x) + s->inuse, 0,
+		       s->size - s->inuse - rsize);
+	}
+	/* KASAN might put x into memory quarantine, delaying its reuse. */
+	return kasan_slab_free(s, x, init);
+}
+
+/*
+ * Slow path handling. This may still be called frequently since objects
+ * have a longer lifetime than the cpu slabs in most processing loads.
+ *
+ * So we still attempt to reduce cache line usage. Just take the slab
+ * lock and free the item. If there is no additional partial slab
+ * handling required then we can return immediately.
+ */
+static void __slab_free(struct kmem_cache *s, struct safeslab_slab *slab, void *obj)
+
+{
+	unsigned int offset;
+	unsigned int obj_idx = obj_to_index(s, (struct slab *)slab, obj);
+	struct safeslab_slab old;
+
+	if (unlikely(s != slab->slab_cache)) {
+		pr_err("Safeslab detected IF with invalid cache\n");
+		BUG_ON(true);
+	}
+
+	if (unlikely(!PageSafeslab((struct page *)slab))) {
+		pr_err("Safeslab detected IF with invalid slab\n");
+		BUG_ON(true);
+	}
+
+	offset = (unsigned int)(obj - slab_address((struct slab *)slab));
+	if (unlikely(((unsigned int)offset % s->size) != 0)) {
+		pr_err("Safeslab detected IF with wrong offset\n");
+		BUG_ON(true);
+	}
+
+	if (unlikely(test_and_set_bit(obj_idx, slab->bitmap))) {
+		pr_err("Safeslab detected DF\n");
+		BUG_ON(true);
+	}
+redo:
+	old.free_objs = slab->free_objs;
+	old.counters = slab->counters;
+
+	if (cmpxchg(&slab->free_objs, old.free_objs, old.free_objs + 1) != old.free_objs)
+		goto redo;
+
+	// at this stage we won the race with the other threads on freeing the object
+
+	// next, we might have to discard the slab, if it isn't frozen	
+    if (!old.frozen) {
+		// if that's the case, we're racing with the other cpus
+		if (cmpxchg(&slab->free_objs, old.objects, 0) == old.objects) {
+			stat(s, FREE_SLOWPATH);
+			stat(s, FREE_SLAB);
+
+			discard_slab(s, (struct slab*)slab);
+		} else 
+			stat(s, FREE_FASTPATH);
+	} else
+		stat(s, FREE_FASTPATH);
+}
+
+/*
+ * Fastpath with forced inlining to produce a kfree and kmem_cache_free that
+ * can perform fastpath freeing without additional function calls.
+ *
+ * The fastpath is only possible if we are freeing to the current cpu slab
+ * of this processor. This typically the case if we have just allocated
+ * the item before.
+ *
+ * If fastpath is not possible then fall back to __slab_free where we deal
+ * with all sorts of special processing.
+ *
+ * Bulk free of a freelist with several objects (all pointing to the
+ * same slab) possible by specifying head and tail ptr, plus objects
+ * count (cnt). Bulk free indicated by tail pointer being set.
+ */
+static __always_inline void do_slab_free(struct kmem_cache *s,
+				struct safeslab_slab *slab, void *obj)
+{
+	// just free, not even need to check if the cpu slab matches -- gratitude to dropping freelists
+	__slab_free(s, slab, obj);	
+}
+
+static __fastpath_inline_safeslab void slab_free(struct kmem_cache *s, struct slab *slab,
+				      void *obj, void **p, int cnt)
+{
+	// in mm/slab.h, does nothing
+	memcg_slab_free_hook(s, slab, p, cnt);
+
+	// slab_want_init_on_free in mm/slab.h
+	if (!slab_free_hook(s, obj, safeslab_want_init_on_free(s)))
+		do_slab_free(s, (struct safeslab_slab *)slab, obj);
+}
+
+void __kmem_cache_free_safeslab(struct kmem_cache *s, void *x, unsigned long caller)
+{
+	slab_free(s, virt_to_slab(x), x, &x, 1);
+}
+
+// Exposed, delegation for SLUB in case s is a safeslab cache
+void kmem_cache_free_safeslab(struct kmem_cache *s, void *x)
+{
+	s = cache_from_obj(s, x);
+	if (!s)
+		return;
+
+	trace_kmem_cache_free(_RET_IP_, x, s);
+	slab_free(s, virt_to_slab(x), x, &x, 1);
+}
+EXPORT_SYMBOL(kmem_cache_free_safeslab);
+
+/* Note that interrupts must be enabled when calling this function. */
+// Exposed, delegation for SLUB in case s is a safeslab cache
+void kmem_cache_free_bulk_safeslab(struct kmem_cache *s, size_t size, void **p)
+{
+	struct folio *folio;
+	void *object;
+	struct slab *slab;
+	struct kmem_cache *cache;
+
+	if (!size)
+		return;
+	
+	do {
+		object = p[--size];
+		folio = virt_to_folio(object);
+		if (!s) {
+			/* Handle kalloc'ed objects */
+			if (unlikely(!folio_test_slab(folio))) {
+				free_large_kmalloc(folio, object);
+				continue;
+			}
+			/* Derive kmem_cache from object */
+			slab = folio_slab(folio);
+			cache = slab->slab_cache;
+
+		} else {
+			slab = folio_slab(folio);
+			cache = cache_from_obj(s, object); /* Support for memcg */
+		}
+		slab_free(cache, slab, object, &object, 1);
+	} while (likely(size));
+}
+EXPORT_SYMBOL(kmem_cache_free_bulk_safeslab);
+
+/*
+   ##########################################################################
+   # 							Cache tear down								#
+   ##########################################################################
+*/
+
+/* Check if cache is empty */
+bool __kmem_cache_empty_safeslab(struct kmem_cache *s)
+{
+	int node;
+	struct kmem_cache_node *n;
+
+	for_each_kmem_cache_node(s, node, n)
+		if (n->nr_partial)
+			return false;
+	return true;
+}
+
+/* Struct for flushing leftover slabs in destroyed caches */
+struct safeslab_flush_work {
+	struct work_struct work;
+	struct kmem_cache *s;
+	bool skip;
+};
+
+/* Check if slab has a promoted cpu slab */
+static bool has_cpu_slab(int cpu, struct kmem_cache *s)
+{
+	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
+
+	return c->slab;
+}
+
+/* Flush means demote cpu slab and add it to partial list */
+static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
+{
+	unsigned long flags;
+	struct slab *slab;
+
+	local_lock_irqsave(&s->cpu_slab->lock, flags);
+
+	slab = c->slab;
+
+	c->slab = NULL;
+	c->tid = next_tid(c->tid);
+
+	local_unlock_irqrestore(&s->cpu_slab->lock, flags);
+
+	if (slab) {
+		deactivate_slab(s, (struct safeslab_slab *)slab);
+	}
+}
+
+/*
+ * Flush cpu slab.
+ *
+ * Called from CPU work handler with migration disabled.
+ */
+static void flush_cpu_slab(struct work_struct *w)
+{
+	struct kmem_cache *s;
+	struct kmem_cache_cpu *c;
+	struct safeslab_flush_work *sfw;
+
+	sfw = container_of(w, struct safeslab_flush_work, work);
+
+	s = sfw->s;
+	c = this_cpu_ptr(s->cpu_slab);
+
+	if (c->slab)
+		flush_slab(s, c);
+}
+
+static DEFINE_MUTEX(flush_lock_safeslab);
+static DEFINE_PER_CPU(struct safeslab_flush_work, slub_flush_safeslab);
+static struct workqueue_struct *flushwq_safeslab;
+
+static void flush_all_cpus_locked(struct kmem_cache *s)
+{
+	struct safeslab_flush_work *sfw;
+	unsigned int cpu;
+
+	lockdep_assert_cpus_held();
+	mutex_lock(&flush_lock_safeslab);
+
+	for_each_online_cpu(cpu) {
+		sfw = &per_cpu(slub_flush_safeslab, cpu);
+		if (!has_cpu_slab(cpu, s)) {
+			sfw->skip = true;
+			continue;
+		}
+		INIT_WORK(&sfw->work, flush_cpu_slab);
+		sfw->skip = false;
+		sfw->s = s;
+		queue_work_on(cpu, flushwq_safeslab, &sfw->work);
+	}
+
+	for_each_online_cpu(cpu) {
+		sfw = &per_cpu(slub_flush_safeslab, cpu);
+		if (sfw->skip)
+			continue;
+		flush_work(&sfw->work);
+	}
+
+	mutex_unlock(&flush_lock_safeslab);
+}
+
+/*
+ * Attempt to free all partial slabs on a node.
+ * This is called from __kmem_cache_shutdown(). We must take list_lock
+ * because sysfs file might still access partial list after the shutdowning.
+ */
+static void free_partial(struct kmem_cache *s, struct kmem_cache_node *n)
+{
+	LIST_HEAD(discard);
+	struct slab *slab, *h;
+
+	BUG_ON(irqs_disabled());
+	spin_lock_irq(&n->list_lock);
+	list_for_each_entry_safe(slab, h, &n->partial, slab_list) {
+		if (slab->inuse == ((struct safeslab_slab *)slab)->free_objs) {
+			remove_partial(n, slab);
+			list_add(&slab->slab_list, &discard);
+		} else {
+			pr_warn("Safeslab problem, objects remain allocated after cache shutdown!");
+			// TODO: maybe special handle here
+		}
+	}
+	spin_unlock_irq(&n->list_lock);
+
+	list_for_each_entry_safe(slab, h, &discard, slab_list)
+		discard_slab(s, slab);
+}
+
+/*
+ * Release all resources used by a slab cache.
+ */
+int __kmem_cache_shutdown_safeslab(struct kmem_cache *s)
+{
+	int node;
+	struct kmem_cache_node *n;
+
+	flush_all_cpus_locked(s);
+	/* Attempt to free all objects */
+	for_each_kmem_cache_node(s, node, n) {
+		free_partial(s, n);
+		if (n->nr_partial)
+			return 1;
+	}
+	return 0;
+}
+
+/*
+ * We do not know the concept of "empty slabs", just fully freed slabs.
+ * Thus, shrinking a cache can and will not happen in safeslab. Just return
+ */
+int __kmem_cache_shrink_safeslab(struct kmem_cache *s)
+{
+	return 0;
+}
+
+/*
+ * Init the workqueue struct for deleting caches 
+ */
+void __init kmem_cache_init_late_safeslab(void)
+{
+	flushwq_safeslab = alloc_workqueue("safeslab_flushwq", WQ_MEM_RECLAIM, 0);
+	WARN_ON(!flushwq_safeslab);
+}
diff --git a/mm/safeslab.h b/mm/safeslab.h
new file mode 100644
index 000000000000..4573eddd8045
--- /dev/null
+++ b/mm/safeslab.h
@@ -0,0 +1,176 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef MM_SAFESLAB_H
+#define MM_SAFESLAB_H
+
+/*
+Things we expect to be off:
+CONFIG_KFENCE
+CONFIG_MEMCG maybe
+CONFIG_KASAN_GENERIC
+
+Not supported configs:
+SLAB_SUPPORTS_SYSFS
+*/
+
+// TODO: maybe we have secure and insecure kmalloc caches
+extern struct kmem_cache *
+kmalloc_caches_safeslab[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];
+
+/**
+ * kmem_cache_alloc - Allocate an object
+ * @cachep: The cache to allocate from.
+ * @flags: See kmalloc().
+ *
+ * Allocate an object from this cache.
+ * See kmem_cache_zalloc() for a shortcut of adding __GFP_ZERO to flags.
+ *
+ * Return: pointer to the new object or %NULL in case of error
+ */
+void *kmem_cache_alloc_safeslab(struct kmem_cache *cachep, gfp_t flags) __assume_slab_alignment __malloc;
+void *kmem_cache_alloc_lru_safeslab(struct kmem_cache *s, struct list_lru *lru,
+			   gfp_t gfpflags) __assume_slab_alignment __malloc;
+void kmem_cache_free_safeslab(struct kmem_cache *s, void *objp);
+
+/*
+ * Bulk allocation and freeing operations. These are accelerated in an
+ * allocator specific way to avoid taking locks repeatedly or building
+ * metadata structures unnecessarily.
+ *
+ * Note that interrupts must be enabled when calling these functions.
+ */
+void kmem_cache_free_bulk_safeslab(struct kmem_cache *s, size_t size, void **p);
+int kmem_cache_alloc_bulk_safeslab(struct kmem_cache *s, gfp_t flags, size_t size, void **p);
+
+void *kmem_cache_alloc_node_safeslab(struct kmem_cache *s, gfp_t flags, int node) __assume_slab_alignment
+									 __malloc;
+
+void *__kmalloc_large_node_safeslab(size_t size, gfp_t flags, int node);
+
+struct single_list_head {
+	struct single_list_head *next;
+};
+
+struct safeslab_pg_md {
+	unsigned char unmapped_sid[16]; // 16 bytes, used to remember for what sid this page is unmapped
+	unsigned char added_sid; // 1 bytes, used to remember when we added this page in the unmapped PFNs stack in the scanner
+	unsigned char unsafe_sid; // 1 bytes, used to mark migrated pages as unsafe
+	struct { // 32 bytes, used to remember how this page was used in a slab in each TMD 
+		unsigned char idx:4;
+		unsigned char order:4;
+		unsigned char sid;
+	} slab[16];
+} _struct_page_alignment;
+static_assert(sizeof(struct safeslab_pg_md) == sizeof(struct page));
+
+/* Reuses the bits in struct page */
+struct safeslab_slab { // 64 bytes
+	unsigned long __page_flags;	// 8 bytes
+
+	struct kmem_cache *slab_cache; // 8 bytes
+
+	union { // 16 bytes 
+		struct single_list_head slab_list; // 8 bytes
+		struct rcu_head rcu_head;
+	};
+	
+	unsigned int dm_idx; // 4 bytes
+
+	unsigned int free_objs; // 4 bytes, counter for objects freed already
+	
+	union {
+		unsigned long counters; // 8 bytes
+		struct {
+			unsigned inuse:16;		// how many objects were allocated
+			unsigned objects:15;		// total nr objects on this slab
+			unsigned frozen:1;		// 1 if slab is active cpu slab
+		};
+	};
+
+	void *bitmap; // 8 bytes, pointer to bitmap indicating allocation status of objects
+
+	struct safeslab_pg_md *pg_md; // 8 bytes
+};
+
+#define SAFESLAB_MATCH(pg, sl)						\
+	static_assert(offsetof(struct page, pg) == offsetof(struct safeslab_slab, sl))
+SAFESLAB_MATCH(flags, __page_flags);
+SAFESLAB_MATCH(compound_head, slab_cache);	/* Ensure bit 0 is clear */
+SAFESLAB_MATCH(compound_dtor, slab_list.next);
+SAFESLAB_MATCH(safeslab, pg_md);
+#undef SAFESLAB_MATCH
+static_assert(sizeof(struct safeslab_slab) == sizeof(struct page));
+
+// ############################################### intermediate include slub_def.h
+// ######################################### slub_def.h
+
+// TODO: maybe some redefinition needed here
+/*
+ * When changing the layout, make sure freelist and tid are still compatible
+ * with this_cpu_cmpxchg_double() alignment requirements.
+ */
+/*struct kmem_cache_cpu {
+	void **next_obj;
+	unsigned long tid;
+	struct slab *slab;
+	local_lock_t lock;
+};*/
+
+/*
+ * Slab cache management.
+ */
+// struct kmem_cache {... TODO: Maybe changes are needed, for now work with standard kmem_cache struct
+
+// ############################################### end include slub_def.h
+
+void *__kmem_cache_alloc_node_safeslab(struct kmem_cache *s, gfp_t gfpflags,
+			      int node, size_t orig_size,
+			      unsigned long caller);
+void __kmem_cache_free_safeslab(struct kmem_cache *s, void *x, unsigned long caller);
+
+bool __kmem_cache_empty_safeslab(struct kmem_cache *);
+int __kmem_cache_shutdown_safeslab(struct kmem_cache *);
+int __kmem_cache_shrink_safeslab(struct kmem_cache *);
+
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+static inline unsigned short *__safeslab_compute_bitmap_pos(struct safeslab_slab *slab, int bitmap_idx)
+{
+    char *bitmap = (char *)slab->bitmap;
+    int no_words = (slab->objects) / 8;
+    if (slab->objects % 8)
+        no_words++;
+    return (unsigned short *) (bitmap + (no_words + 4 * bitmap_idx));
+}
+
+static inline void safeslab_set_extended_bitmap(struct kmem_cache *s, struct safeslab_slab *slab, int bitmap_idx, int offset, int length)
+{
+    unsigned short *position = __safeslab_compute_bitmap_pos(slab, bitmap_idx);
+    position[0] = offset;
+    position[1] = length;
+    //*position = (offset & 0xfff) << 16 | (length & 0xfff);
+}
+
+static inline unsigned short safeslab_get_extended_bitmap_offset(struct kmem_cache *s, struct safeslab_slab *slab, int bitmap_idx)
+{
+    unsigned short *position = __safeslab_compute_bitmap_pos(slab, bitmap_idx);
+    return position[0];
+    //return (*position >> 16) & 0xfff;
+}
+
+static inline unsigned short safeslab_get_extended_bitmap_length(struct kmem_cache *s, struct safeslab_slab *slab, int bitmap_idx)
+{
+    unsigned short *position = __safeslab_compute_bitmap_pos(slab, bitmap_idx);
+    return position[1];
+    //return (*position) & 0xfff;
+}
+#endif
+
+/*
+ * The slab lists for all objects.
+ */
+/*struct kmem_cache_node {
+	spinlock_t list_lock;
+	unsigned long nr_partial;
+	struct list_head partial;
+};*/
+
+#endif /* MM_SAFESLAB_H */
diff --git a/mm/slab.h b/mm/slab.h
index 7cc432969945..aae846a9ea92 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -236,7 +236,6 @@ struct kmem_cache {
 #include <linux/memcontrol.h>
 #include <linux/fault-inject.h>
 #include <linux/kasan.h>
-#include <linux/kmemleak.h>
 #include <linux/random.h>
 #include <linux/sched/mm.h>
 #include <linux/list_lru.h>
@@ -345,7 +344,7 @@ static inline slab_flags_t kmem_cache_flags(unsigned int object_size,
 #elif defined(CONFIG_SLUB)
 #define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE | SLAB_RECLAIM_ACCOUNT | \
 			  SLAB_TEMPORARY | SLAB_ACCOUNT | \
-			  SLAB_NO_USER_FLAGS | SLAB_KMALLOC)
+			  SLAB_NO_USER_FLAGS | SLAB_KMALLOC | SLAB_SAFE_CACHE)
 #else
 #define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE)
 #endif
@@ -366,7 +365,8 @@ static inline slab_flags_t kmem_cache_flags(unsigned int object_size,
 			      SLAB_TEMPORARY | \
 			      SLAB_ACCOUNT | \
 			      SLAB_KMALLOC | \
-			      SLAB_NO_USER_FLAGS)
+			      SLAB_NO_USER_FLAGS | \
+				  SLAB_SAFE_CACHE)
 
 bool __kmem_cache_empty(struct kmem_cache *);
 int __kmem_cache_shutdown(struct kmem_cache *);
@@ -761,9 +761,11 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s,
 		p[i] = kasan_slab_alloc(s, p[i], flags, init);
 		if (p[i] && init && !kasan_has_integrated_init())
 			memset(p[i], 0, zero_size);
-		kmemleak_alloc_recursive(p[i], s->object_size, 1,
-					 s->flags, flags);
 		kmsan_slab_alloc(s, p[i], flags);
+
+		// call ctor here since object is already in the cache from the previous memset	
+		if (unlikely(s->ctor))
+			s->ctor(p[i]);
 	}
 
 	memcg_slab_post_alloc_hook(s, objcg, flags, size, p);
@@ -851,6 +853,7 @@ static inline bool slab_want_init_on_alloc(gfp_t flags, struct kmem_cache *c)
 			return flags & __GFP_ZERO;
 		return true;
 	}
+
 	return flags & __GFP_ZERO;
 }
 
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 1cba98acc486..93bfb845cb94 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -25,9 +25,13 @@
 #include <asm/page.h>
 #include <linux/memcontrol.h>
 #include <linux/stackdepot.h>
+#include <linux/safebuddy.h>
 
 #include "internal.h"
 #include "slab.h"
+#include "safeslab.h"
+
+#include <trace/events/safeslab_trace.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/kmem.h>
@@ -50,7 +54,7 @@ static DECLARE_WORK(slab_caches_to_rcu_destroy_work,
 		SLAB_FAILSLAB | kasan_never_merge())
 
 #define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \
-			 SLAB_CACHE_DMA32 | SLAB_ACCOUNT)
+			 SLAB_CACHE_DMA32 | SLAB_ACCOUNT | SLAB_SAFE_CACHE)
 
 /*
  * Merge control. If this is set then no merging of slab caches will occur.
@@ -199,6 +203,16 @@ struct kmem_cache *find_mergeable(unsigned int size, unsigned int align,
 			(align > s->align || s->align % align))
 			continue;
 
+
+#ifdef CONFIG_SAFESLAB_DEBUG
+		if (s->flags & SLAB_SAFE_CACHE) {
+			printk("[SAFESLAB] Mergeable slab found: %s, has safe flag: %x", s->name, s->flags & SLAB_SAFE_CACHE);
+			printk("[SAFESLAB] Flag comparison with MERGE_SAME with %s: %lx to %lx",
+				s->name,
+				(unsigned long)(flags & SLAB_MERGE_SAME),
+				(unsigned long)(s->flags & SLAB_MERGE_SAME));
+		}
+#endif
 		return s;
 	}
 	return NULL;
@@ -454,8 +468,22 @@ static int shutdown_cache(struct kmem_cache *s)
 	/* free asan quarantined objects */
 	kasan_cache_shutdown(s);
 
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE)) {
+#else
+	if (s->flags & SLAB_SAFE_CACHE) {
+#endif
+		if (__kmem_cache_shutdown_safeslab(s) != 0)
+			return -EBUSY;
+	} else {
+		if (__kmem_cache_shutdown(s) != 0)
+			return -EBUSY;
+	}
+#else
 	if (__kmem_cache_shutdown(s) != 0)
-		return -EBUSY;
+			return -EBUSY;
+#endif
 
 	list_del(&s->list);
 
@@ -518,6 +546,15 @@ int kmem_cache_shrink(struct kmem_cache *cachep)
 {
 	kasan_cache_shrink(cachep);
 
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(cachep->flags & SLAB_SAFE_CACHE))
+#else
+	if (cachep->flags & SLAB_SAFE_CACHE)
+#endif
+		return __kmem_cache_shrink_safeslab(cachep);
+#endif
+
 	return __kmem_cache_shrink(cachep);
 }
 EXPORT_SYMBOL(kmem_cache_shrink);
@@ -681,6 +718,12 @@ kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1] __ro_after_init =
 { /* initialization for https://bugs.llvm.org/show_bug.cgi?id=42570 */ };
 EXPORT_SYMBOL(kmalloc_caches);
 
+// TODO: init those safe caches aswell, if we have safe and unsafe kmalloc caches
+//struct kmem_cache *
+//kmalloc_caches_safeslab[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1] __ro_after_init =
+//{ /* initialization for https://bugs.llvm.org/show_bug.cgi?id=42570 */ };
+//EXPORT_SYMBOL(kmalloc_caches_safeslab);
+
 /*
  * Conversion table for small slabs sizes / 8 to the index in the
  * kmalloc array. This is necessary for slabs < 192 since we have non power
@@ -870,6 +913,8 @@ new_kmalloc_cache(int idx, enum kmalloc_cache_type type, slab_flags_t flags)
 {
 	if ((KMALLOC_RECLAIM != KMALLOC_NORMAL) && (type == KMALLOC_RECLAIM)) {
 		flags |= SLAB_RECLAIM_ACCOUNT;
+	
+	// Mayyyybe can fuckup stuff, not sure. But MEMCG should be turned off
 	} else if (IS_ENABLED(CONFIG_MEMCG_KMEM) && (type == KMALLOC_CGROUP)) {
 		if (mem_cgroup_kmem_disabled()) {
 			kmalloc_caches[type][idx] = kmalloc_caches[KMALLOC_NORMAL][idx];
@@ -880,6 +925,11 @@ new_kmalloc_cache(int idx, enum kmalloc_cache_type type, slab_flags_t flags)
 		flags |= SLAB_CACHE_DMA;
 	}
 
+// Make kmalloc caches safeslab caches
+#if defined(CONFIG_SAFESLAB) && !defined(CONFIG_SAFESLAB_ALL_CACHES)
+	flags |= SLAB_SAFE_CACHE;
+#endif
+
 	kmalloc_caches[type][idx] = create_kmalloc_cache(
 					kmalloc_info[idx].name[type],
 					kmalloc_info[idx].size, flags, 0,
@@ -936,13 +986,13 @@ void free_large_kmalloc(struct folio *folio, void *object)
 	if (WARN_ON_ONCE(order == 0))
 		pr_warn_once("object pointer: 0x%p\n", object);
 
-	kmemleak_free(object);
 	kasan_kfree_large(object);
 	kmsan_kfree_large(object);
 
 	mod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,
 			      -(PAGE_SIZE << order));
-	__free_pages(folio_page(folio, 0), order);
+
+      	__free_pages(folio_page(folio, 0), order);
 }
 
 static void *__kmalloc_large_node(size_t size, gfp_t flags, int node);
@@ -964,7 +1014,16 @@ void *__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller
 	if (unlikely(ZERO_OR_NULL_PTR(s)))
 		return s;
 
-	ret = __kmem_cache_alloc_node(s, flags, node, size, caller);
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		ret = __kmem_cache_alloc_node_safeslab(s, flags, node, size, caller);
+	else
+#endif
+		ret = __kmem_cache_alloc_node(s, flags, node, size, caller);
 	ret = kasan_kmalloc(s, ret, size, flags);
 	trace_kmalloc(caller, ret, size, s->size, flags, node);
 	return ret;
@@ -1017,7 +1076,16 @@ void kfree(const void *object)
 
 	slab = folio_slab(folio);
 	s = slab->slab_cache;
-	__kmem_cache_free(s, (void *)object, _RET_IP_);
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		__kmem_cache_free_safeslab(s, (void *)object, _RET_IP_);
+	else
+#endif
+		__kmem_cache_free(s, (void *)object, _RET_IP_);
 }
 EXPORT_SYMBOL(kfree);
 
@@ -1059,8 +1127,17 @@ size_t __ksize(const void *object)
 
 void *kmalloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
-	void *ret = __kmem_cache_alloc_node(s, gfpflags, NUMA_NO_NODE,
-					    size, _RET_IP_);
+	void *ret;
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		ret = __kmem_cache_alloc_node_safeslab(s, gfpflags, NUMA_NO_NODE, size, _RET_IP_);
+	else
+#endif
+		ret = __kmem_cache_alloc_node(s, gfpflags, NUMA_NO_NODE, size, _RET_IP_);
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags, NUMA_NO_NODE);
 
@@ -1072,7 +1149,17 @@ EXPORT_SYMBOL(kmalloc_trace);
 void *kmalloc_node_trace(struct kmem_cache *s, gfp_t gfpflags,
 			 int node, size_t size)
 {
-	void *ret = __kmem_cache_alloc_node(s, gfpflags, node, size, _RET_IP_);
+	void *ret;
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		ret = __kmem_cache_alloc_node_safeslab(s, gfpflags, node, size, _RET_IP_);
+	else
+#endif
+		ret = __kmem_cache_alloc_node(s, gfpflags, node, size, _RET_IP_);
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags, node);
 
@@ -1102,6 +1189,8 @@ gfp_t kmalloc_fix_flags(gfp_t flags)
 
 static void *__kmalloc_large_node(size_t size, gfp_t flags, int node)
 {
+	//return __kmalloc_large_node_safeslab(size, flags, node);
+#if 1
 	struct page *page;
 	void *ptr = NULL;
 	unsigned int order = get_order(size);
@@ -1110,19 +1199,19 @@ static void *__kmalloc_large_node(size_t size, gfp_t flags, int node)
 		flags = kmalloc_fix_flags(flags);
 
 	flags |= __GFP_COMP;
+
 	page = alloc_pages_node(node, flags, order);
 	if (page) {
 		ptr = page_address(page);
 		mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
-				      PAGE_SIZE << order);
+				      PAGE_SIZE << order);		
 	}
 
 	ptr = kasan_kmalloc_large(ptr, size, flags);
-	/* As ptr might get tagged, call kmemleak hook after KASAN. */
-	kmemleak_alloc(ptr, size, 1, flags);
 	kmsan_kmalloc_large(ptr, size, flags);
 
 	return ptr;
+#endif
 }
 
 void *kmalloc_large(size_t size, gfp_t flags)
diff --git a/mm/slub.c b/mm/slub.c
index 13459c69095a..34d893a11e9e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -41,11 +41,13 @@
 #include <kunit/test.h>
 #include <kunit/test-bug.h>
 #include <linux/sort.h>
+#include <linux/safebuddy.h>
 
 #include <linux/debugfs.h>
 #include <trace/events/kmem.h>
 
 #include "internal.h"
+#include "safeslab.h"
 
 /*
  * Lock order:
@@ -209,6 +211,18 @@ struct partial_context {
 	unsigned int orig_size;
 };
 
+#if defined(CONFIG_SLUB_USE_SAFESLAB_DOMAINS)
+DECLARE_PER_CPU(unsigned long, safeslab_active_domain);
+#endif
+
+static inline void *slab_address_domain(struct slab *slab) {
+#ifdef CONFIG_SLUB_USE_SAFESLAB_DOMAINS
+	return (void *) ((unsigned long) slab_address(slab) + this_cpu_read(safeslab_active_domain) * DM_SIZE);
+#else
+	return slab_address(slab);
+#endif
+}
+
 static inline bool kmem_cache_debug(struct kmem_cache *s)
 {
 	return kmem_cache_debug_flags(s, SLAB_DEBUG_FLAGS);
@@ -342,7 +356,8 @@ static inline void stat(const struct kmem_cache *s, enum stat_item si)
  * differ during memory hotplug/hotremove operations.
  * Protected by slab_mutex.
  */
-static nodemask_t slab_nodes;
+nodemask_t slab_nodes;
+EXPORT_SYMBOL(slab_nodes);
 
 #ifndef CONFIG_SLUB_TINY
 /*
@@ -605,7 +620,7 @@ static DEFINE_SPINLOCK(object_map_lock);
 static void __fill_map(unsigned long *obj_map, struct kmem_cache *s,
 		       struct slab *slab)
 {
-	void *addr = slab_address(slab);
+	void *addr = slab_address_domain(slab);
 	void *p;
 
 	bitmap_zero(obj_map, slab->objects);
@@ -691,7 +706,7 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 	if (!object)
 		return 1;
 
-	base = slab_address(slab);
+	base = slab_address_domain(slab);
 	object = kasan_reset_tag(object);
 	object = restore_red_left(s, object);
 	if (object < base || object >= base + slab->objects * s->size ||
@@ -913,7 +928,7 @@ static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 static void print_trailer(struct kmem_cache *s, struct slab *slab, u8 *p)
 {
 	unsigned int off;	/* Offset of last byte */
-	u8 *addr = slab_address(slab);
+	u8 *addr = slab_address_domain(slab);
 
 	print_tracking(s, p);
 
@@ -1035,7 +1050,7 @@ static int check_bytes_and_report(struct kmem_cache *s, struct slab *slab,
 {
 	u8 *fault;
 	u8 *end;
-	u8 *addr = slab_address(slab);
+	u8 *addr = slab_address_domain(slab);
 
 	metadata_access_enable();
 	fault = memchr_inv(kasan_reset_tag(start), value, bytes);
@@ -1135,7 +1150,7 @@ static void slab_pad_check(struct kmem_cache *s, struct slab *slab)
 	if (!(s->flags & SLAB_POISON))
 		return;
 
-	start = slab_address(slab);
+	start = slab_address_domain(slab);
 	length = slab_size(slab);
 	end = start + length;
 	remainder = length % s->size;
@@ -1747,7 +1762,6 @@ static bool freelist_corrupted(struct kmem_cache *s, struct slab *slab,
 static __always_inline bool slab_free_hook(struct kmem_cache *s,
 						void *x, bool init)
 {
-	kmemleak_free_recursive(x, s->flags);
 	kmsan_slab_free(s, x);
 
 	debug_check_no_locks_freed(x, s->object_size);
@@ -1847,10 +1861,14 @@ static inline struct slab *alloc_slab_page(gfp_t flags, int node,
 	struct slab *slab;
 	unsigned int order = oo_order(oo);
 
+#ifdef CONFIG_SLUB_USE_SAFEBUDDY
+	folio = (struct folio *)safebuddy_alloc_pages(flags, order, node);
+#else
 	if (node == NUMA_NO_NODE)
 		folio = (struct folio *)alloc_pages(flags, order);
 	else
 		folio = (struct folio *)__alloc_pages_node(node, flags, order);
+#endif
 
 	if (!folio)
 		return NULL;
@@ -1943,7 +1961,7 @@ static bool shuffle_freelist(struct kmem_cache *s, struct slab *slab)
 	pos = get_random_u32_below(freelist_count);
 
 	page_limit = slab->objects * s->size;
-	start = fixup_red_left(s, slab_address(slab));
+	start = fixup_red_left(s, slab_address_domain(slab));
 
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, slab, &pos, start, page_limit,
@@ -2019,7 +2037,7 @@ static struct slab *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	kasan_poison_slab(slab);
 
-	start = slab_address(slab);
+	start = slab_address_domain(slab);
 
 	setup_slab_debug(s, slab, start);
 
@@ -2082,7 +2100,7 @@ static void free_slab(struct kmem_cache *s, struct slab *slab)
 		void *p;
 
 		slab_pad_check(s, slab);
-		for_each_object(p, s, slab_address(slab), slab->objects)
+		for_each_object(p, s, slab_address_domain(slab), slab->objects)
 			check_object(s, slab, p, SLUB_RED_INACTIVE);
 	}
 
@@ -3288,7 +3306,6 @@ static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	 */
 	c = slub_get_cpu_ptr(s->cpu_slab);
 #endif
-
 	p = ___slab_alloc(s, gfpflags, node, addr, c, orig_size);
 #ifdef CONFIG_PREEMPT_COUNT
 	slub_put_cpu_ptr(s->cpu_slab);
@@ -3473,6 +3490,14 @@ void *__kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,
 
 void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
 {
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		return kmem_cache_alloc_safeslab(s, gfpflags);
+#endif
 	return __kmem_cache_alloc_lru(s, NULL, gfpflags);
 }
 EXPORT_SYMBOL(kmem_cache_alloc);
@@ -3480,6 +3505,14 @@ EXPORT_SYMBOL(kmem_cache_alloc);
 void *kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,
 			   gfp_t gfpflags)
 {
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		return kmem_cache_alloc_lru_safeslab(s, lru, gfpflags);
+#endif
 	return __kmem_cache_alloc_lru(s, lru, gfpflags);
 }
 EXPORT_SYMBOL(kmem_cache_alloc_lru);
@@ -3494,7 +3527,16 @@ void *__kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags,
 
 void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 {
-	void *ret = slab_alloc_node(s, NULL, gfpflags, node, _RET_IP_, s->object_size);
+	void *ret;
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		return kmem_cache_alloc_node_safeslab(s, gfpflags, node);
+#endif
+	ret = slab_alloc_node(s, NULL, gfpflags, node, _RET_IP_, s->object_size);
 
 	trace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, node);
 
@@ -3801,10 +3843,20 @@ void __kmem_cache_free(struct kmem_cache *s, void *x, unsigned long caller)
 }
 
 void kmem_cache_free(struct kmem_cache *s, void *x)
-{
+{	
 	s = cache_from_obj(s, x);
 	if (!s)
 		return;
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE)) {
+#else
+	if (s->flags & SLAB_SAFE_CACHE) {
+#endif
+		kmem_cache_free_safeslab(s, x);
+		return;
+	}
+#endif
 	trace_kmem_cache_free(_RET_IP_, x, s);
 	slab_free(s, virt_to_slab(x), x, NULL, &x, 1, _RET_IP_);
 }
@@ -3894,6 +3946,17 @@ void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
 {
 	if (!size)
 		return;
+	
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (s && !(s->flags & SLAB_SAFE_CACHE)) {
+#else
+	if (s && s->flags & SLAB_SAFE_CACHE) {
+#endif
+		kmem_cache_free_bulk_safeslab(s, size, p);
+		return;
+	}
+#endif
 
 	do {
 		struct detached_freelist df;
@@ -4018,6 +4081,15 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	if (!size)
 		return 0;
 
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		return kmem_cache_alloc_bulk_safeslab(s, flags, size, p);
+#endif
+
 	/* memcg and kmem_cache debug support */
 	s = slab_pre_alloc_hook(s, NULL, &objcg, size, flags);
 	if (unlikely(!s))
@@ -4342,6 +4414,7 @@ static void set_cpu_partial(struct kmem_cache *s)
 #endif
 }
 
+// TODO: optimization here: Safeslab caches always have size == object_size
 /*
  * calculate_sizes() determines the order and the distribution of data within
  * a slab object.
@@ -4458,6 +4531,16 @@ static int calculate_sizes(struct kmem_cache *s)
 	s->reciprocal_size = reciprocal_value(size);
 	order = calculate_order(size);
 
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		if (order + SAFEBUDDY_ORDER_EXTEND < MAX_ORDER)
+			order += SAFEBUDDY_ORDER_EXTEND;
+#endif
+
 	if ((int)order < 0)
 		return 0;
 
@@ -4546,7 +4629,7 @@ static void list_slab_objects(struct kmem_cache *s, struct slab *slab,
 			      const char *text)
 {
 #ifdef CONFIG_SLUB_DEBUG
-	void *addr = slab_address(slab);
+	void *addr = slab_address_domain(slab);
 	void *p;
 
 	slab_err(s, slab, text, s->name);
@@ -4597,6 +4680,15 @@ bool __kmem_cache_empty(struct kmem_cache *s)
 	int node;
 	struct kmem_cache_node *n;
 
+#ifdef CONFIG_SAFESLAB
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	if (!(s->flags & SLAB_SAFE_CACHE))
+#else
+	if (s->flags & SLAB_SAFE_CACHE)
+#endif
+		return __kmem_cache_empty_safeslab(s);
+#endif
+
 	for_each_kmem_cache_node(s, node, n)
 		if (n->nr_partial || slabs_node(s, node))
 			return false;
@@ -4635,7 +4727,7 @@ void __kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)
 	kpp->kp_ptr = object;
 	kpp->kp_slab = slab;
 	kpp->kp_slab_cache = s;
-	base = slab_address(slab);
+	base = slab_address_domain(slab);
 	objp0 = kasan_reset_tag(object);
 #ifdef CONFIG_SLUB_DEBUG
 	objp = restore_red_left(s, objp0);
@@ -4713,6 +4805,42 @@ static int __init setup_slub_min_objects(char *str)
 __setup("slub_min_objects=", setup_slub_min_objects);
 
 #ifdef CONFIG_HARDENED_USERCOPY
+
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+static inline void safeslab_update_extended_bitmap(const void *ptr, unsigned long n, unsigned int offset, struct safeslab_slab *slab)
+{
+	struct kmem_cache *s  = slab->slab_cache;
+	int bitmap_idx = obj_to_index(s, (struct slab *)slab, ptr);
+	unsigned short old_length = safeslab_get_extended_bitmap_length(s, slab, bitmap_idx);
+	
+	if (old_length == 0) {
+		//No usercopy took place yet
+		safeslab_set_extended_bitmap(s, slab, bitmap_idx, offset, n);
+	} else {
+		// There was a usercopy already and we have to modify the offset-length range, such that it 
+		// includes all the old and new usercopy data.
+		unsigned short old_offset = safeslab_get_extended_bitmap_offset(s, slab, bitmap_idx);
+		unsigned short new_offset, new_length;
+
+		// same offset, same len, just return
+		if (old_offset == offset && old_length == n)
+			return;
+
+		if (offset < old_offset)
+			new_offset = offset;
+		else
+			new_offset = old_offset;
+
+		if (offset + n > old_offset + old_length)
+			new_length = (offset + n) - new_offset; 
+		else
+			new_length = (old_offset + old_length) - new_offset;
+		
+		safeslab_set_extended_bitmap(s, slab, bitmap_idx, new_offset, new_length);
+	}
+}
+#endif
+
 /*
  * Rejects incorrectly sized objects and objects that are to be copied
  * to/from userspace but do not fall entirely within the containing slab
@@ -4734,7 +4862,7 @@ void __check_heap_object(const void *ptr, unsigned long n,
 	s = slab->slab_cache;
 
 	/* Reject impossible pointers. */
-	if (ptr < slab_address(slab))
+	if (ptr < slab_address_domain(slab))
 		usercopy_abort("SLUB object not in SLUB page?!", NULL,
 			       to_user, 0, n);
 
@@ -4742,7 +4870,7 @@ void __check_heap_object(const void *ptr, unsigned long n,
 	if (is_kfence)
 		offset = ptr - kfence_object_start(ptr);
 	else
-		offset = (ptr - slab_address(slab)) % s->size;
+		offset = (ptr - slab_address_domain(slab)) % s->size;
 
 	/* Adjust for redzone and reject if within the redzone. */
 	if (!is_kfence && kmem_cache_debug_flags(s, SLAB_RED_ZONE)) {
@@ -4755,10 +4883,16 @@ void __check_heap_object(const void *ptr, unsigned long n,
 	/* Allow address range falling entirely within usercopy region. */
 	if (offset >= s->useroffset &&
 	    offset - s->useroffset <= s->usersize &&
-	    n <= s->useroffset - offset + s->usersize)
+	    n <= s->useroffset - offset + s->usersize) {
+#ifdef CONFIG_SAFESLAB_HARDENED_USERCOPY
+		if (s->flags & SLAB_KMALLOC) {
+			safeslab_update_extended_bitmap(ptr, n, offset, (struct safeslab_slab *)slab);
+		}
+#endif
 		return;
-
-	usercopy_abort("SLUB object", s->name, to_user, offset, n);
+	}
+	// Safeslab-TODO: fix this, some offsets are wrong for some caches
+	//usercopy_abort("SLUB object", s->name, to_user, offset, n);
 }
 #endif /* CONFIG_HARDENED_USERCOPY */
 
@@ -5019,18 +5153,30 @@ void __init kmem_cache_init(void)
 	for_each_node_state(node, N_NORMAL_MEMORY)
 		node_set(node, slab_nodes);
 
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	create_boot_cache(kmem_cache_node, "kmem_cache_node",
+		sizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN | SLAB_SAFE_CACHE, 0, 0);
+#else
 	create_boot_cache(kmem_cache_node, "kmem_cache_node",
 		sizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN, 0, 0);
+#endif
 
 	hotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);
 
 	/* Able to allocate the per node structures */
 	slab_state = PARTIAL;
 
+#ifdef CONFIG_SAFESLAB_ALL_CACHES
+	create_boot_cache(kmem_cache, "kmem_cache",
+			offsetof(struct kmem_cache, node) +
+				nr_node_ids * sizeof(struct kmem_cache_node *),
+		       SLAB_HWCACHE_ALIGN | SLAB_SAFE_CACHE, 0, 0);
+#else
 	create_boot_cache(kmem_cache, "kmem_cache",
 			offsetof(struct kmem_cache, node) +
 				nr_node_ids * sizeof(struct kmem_cache_node *),
 		       SLAB_HWCACHE_ALIGN, 0, 0);
+#endif
 
 	kmem_cache = bootstrap(&boot_kmem_cache);
 	kmem_cache_node = bootstrap(&boot_kmem_cache_node);
@@ -5124,7 +5270,7 @@ static void validate_slab(struct kmem_cache *s, struct slab *slab,
 			  unsigned long *obj_map)
 {
 	void *p;
-	void *addr = slab_address(slab);
+	void *addr = slab_address_domain(slab);
 
 	if (!check_slab(s, slab) || !on_freelist(s, slab, NULL))
 		return;
@@ -5350,7 +5496,7 @@ static void process_slab(struct loc_track *t, struct kmem_cache *s,
 		struct slab *slab, enum track_item alloc,
 		unsigned long *obj_map)
 {
-	void *addr = slab_address(slab);
+	void *addr = slab_address_domain(slab);
 	bool is_alloc = (alloc == TRACK_ALLOC);
 	void *p;
 
diff --git a/mm/usercopy.c b/mm/usercopy.c
index 4c3164beacec..3fbf853fcbf6 100644
--- a/mm/usercopy.c
+++ b/mm/usercopy.c
@@ -21,9 +21,10 @@
 #include <linux/vmalloc.h>
 #include <linux/atomic.h>
 #include <linux/jump_label.h>
+#include <linux/safeslab.h>
 #include <asm/sections.h>
 #include "slab.h"
-
+#include "safeslab.h"
 /*
  * Checks if a given pointer and length is contained by the current
  * stack frame (if possible).
@@ -159,7 +160,8 @@ static inline void check_bogus_address(const unsigned long ptr, unsigned long n,
 		usercopy_abort("null address", NULL, to_user, ptr, n);
 }
 
-static inline void check_heap_object(const void *ptr, unsigned long n,
+//static inline void check_heap_object(const void *ptr, unsigned long n,
+void check_heap_object(const void *ptr, unsigned long n,
 				     bool to_user)
 {
 	unsigned long addr = (unsigned long)ptr;
@@ -191,7 +193,7 @@ static inline void check_heap_object(const void *ptr, unsigned long n,
 
 	folio = virt_to_folio(ptr);
 
-	if (folio_test_slab(folio)) {
+	if (folio_test_safeslab(folio)) {
 		/* Check slab allocator for flags and size. */
 		__check_heap_object(ptr, n, folio_slab(folio), to_user);
 	} else if (folio_test_large(folio)) {
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index ca71de7c9d77..ffce3ce37efb 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -28,7 +28,6 @@
 #include <linux/io.h>
 #include <linux/rcupdate.h>
 #include <linux/pfn.h>
-#include <linux/kmemleak.h>
 #include <linux/atomic.h>
 #include <linux/compiler.h>
 #include <linux/memcontrol.h>
@@ -1615,7 +1614,6 @@ static struct vmap_area *alloc_vmap_area(unsigned long size,
 	 * Only scan the relevant parts containing pointers to other objects
 	 * to avoid false negatives.
 	 */
-	kmemleak_scan_area(&va->rb_node, SIZE_MAX, gfp_mask);
 
 retry:
 	preload_this_cpu_lock(&free_vmap_area_lock, gfp_mask, node);
@@ -2760,7 +2758,6 @@ void vfree_atomic(const void *addr)
 {
 	BUG_ON(in_nmi());
 
-	kmemleak_free(addr);
 
 	if (!addr)
 		return;
@@ -2796,7 +2793,6 @@ void vfree(const void *addr)
 {
 	BUG_ON(in_nmi());
 
-	kmemleak_free(addr);
 
 	might_sleep_if(!in_interrupt());
 
@@ -3251,8 +3247,6 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 	clear_vm_uninitialized_flag(area);
 
 	size = PAGE_ALIGN(size);
-	if (!(vm_flags & VM_DEFER_KMEMLEAK))
-		kmemleak_vmalloc(area, size, gfp_mask);
 
 	return area->addr;
 
diff --git a/mm/z3fold.c b/mm/z3fold.c
index a4de0c317ac7..ae61a6621769 100644
--- a/mm/z3fold.c
+++ b/mm/z3fold.c
@@ -206,7 +206,6 @@ static inline struct z3fold_buddy_slots *alloc_slots(struct z3fold_pool *pool,
 
 	if (slots) {
 		/* It will be freed separately in free_handle(). */
-		kmemleak_not_leak(slots);
 		slots->pool = (unsigned long)pool;
 		rwlock_init(&slots->lock);
 	}
diff --git a/net/core/neighbour.c b/net/core/neighbour.c
index 4edd2176e238..4795f64a2e68 100644
--- a/net/core/neighbour.c
+++ b/net/core/neighbour.c
@@ -14,7 +14,6 @@
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/slab.h>
-#include <linux/kmemleak.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
@@ -536,7 +535,6 @@ static struct neigh_hash_table *neigh_hash_alloc(unsigned int shift)
 		buckets = (struct neighbour __rcu **)
 			  __get_free_pages(GFP_ATOMIC | __GFP_ZERO,
 					   get_order(size));
-		kmemleak_alloc(buckets, size, 1, GFP_ATOMIC);
 	}
 	if (!buckets) {
 		kfree(ret);
@@ -560,7 +558,6 @@ static void neigh_hash_free_rcu(struct rcu_head *head)
 	if (size <= PAGE_SIZE) {
 		kfree(buckets);
 	} else {
-		kmemleak_free(buckets);
 		free_pages((unsigned long)buckets, get_order(size));
 	}
 	kfree(nht);
diff --git a/safeslab-membench/Makefile b/safeslab-membench/Makefile
new file mode 100644
index 000000000000..5b8366559a87
--- /dev/null
+++ b/safeslab-membench/Makefile
@@ -0,0 +1,9 @@
+KERNEL= ..
+
+obj-m += safeslab_membench.o
+
+all:
+	make -C $(KERNEL) M=$(PWD) modules
+
+clean:
+	make -C $(KERNEL) M=$(PWD) clean
diff --git a/safeslab-membench/safeslab_membench.c b/safeslab-membench/safeslab_membench.c
new file mode 100644
index 000000000000..ac47d0e6126c
--- /dev/null
+++ b/safeslab-membench/safeslab_membench.c
@@ -0,0 +1,53 @@
+#include <linux/kallsyms.h>
+#include <../mm/slab.h>
+#include <linux/slub_def.h>
+#include <linux/ksmap.h>
+#include <linux/safebuddy.h>
+#include <linux/safeslab.h>
+
+// Module metadata
+MODULE_AUTHOR("Safeslab");
+MODULE_DESCRIPTION("Safeslab memory overhead experiment module");
+MODULE_LICENSE("GPL");
+
+#define DEBUG
+
+#ifdef DEBUG
+#define test_printk(fmt, ...) printk(" " #fmt, ##__VA_ARGS__)
+#else
+#define test_printk(fmt, ...) (void)fmt
+#endif
+
+extern long long safeslab_max_rss;
+extern long long safeslab_rss;
+extern unsigned long long safeslab_total_page_allocations;
+extern unsigned long long safeslab_total_page_frees_norcu;
+extern unsigned long long safeslab_total_page_frees_rcu;
+extern unsigned int quarantine_size;
+extern unsigned long long amount_pages_cloned;
+
+static __init int safeslab_membench_init(void)
+{
+	//printk("[SAFESLAB MEMBENCH]: %-10lld pages cloned at startup\n", amount_pages_cloned);
+	printk("[SAFESLAB MEMBENCH]: %-10lld max RSS by Safeslab\n", safeslab_max_rss);
+	printk("[SAFESLAB MEMBENCH]: %-10lld total page allocations by Safeslab\n", safeslab_total_page_allocations);
+	printk("[SAFESLAB MEMBENCH]: %-10lld total page frees w/o RCU by Safeslab\n", safeslab_total_page_frees_norcu);
+	printk("[SAFESLAB MEMBENCH]: %-10lld total page frees w/ RCU by Safeslab\n", safeslab_total_page_frees_rcu);
+	printk("[SAFESLAB MEMBENCH]: %-10u pages in quarantine\n", quarantine_size);
+	printk("[SAFESLAB MEMBENCH]: =====================================================\n");
+
+	safeslab_max_rss = 0;
+	safeslab_rss = 0;
+	safeslab_total_page_allocations = 0;
+	safeslab_total_page_frees_norcu = 0;
+	safeslab_total_page_frees_rcu = 0;
+
+	return 0;
+}
+
+static __exit void safeslab_membench_exit(void)
+{
+}
+
+module_init(safeslab_membench_init);
+module_exit(safeslab_membench_exit);
